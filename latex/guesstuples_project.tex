\documentclass[12pt]{article}

\input{guesstuples_preamble}

%opening
\title{GuessTuples Project}
\author{Andrew J. Wren}

\begin{document}

\maketitle

\begin{abstract}
	Notes on GuessTuples project
\end{abstract}


\section{Configuring the nets}

\subsection{Alice}

The input array to guess is $\vec{x}=(x_j)_{j=0,...,N_\text{elements}}.$  There should be $N_\text{code}$ outputs taking values $\vec{y} = (y_j)_{j=0,...,N_\text{code}}.$

Normalise all the rewards so that for each bit $j,$ $\rt{j} + \left(N_\text{code} - 1\right)\rf{j} = 0.$  In other words
\begin{equation}
	r_{jk}
	\gets
	r_{jk} - \frac{ \rt{j} + \left(N_\text{code} - 1\right)\rf{j}}{N_\text{code}}
	.
\end{equation}

The $Q$ estimate is then taken to be
\begin{equation}
	Q(\vec{x})
	=
	\sum_j b_j y_j
	\equiv
	\sum_j \abs{y_j}
	,		
\end{equation}
where
\begin{equation}
	b_j = \operatorname{sgn} (y_j)
\end{equation}
is the prediction for the machine value of the $j$th bit.  The loss function is
\begin{equation}
	L
	=
	\abs{Q(\vec{x}) -r}^2
	. 
\end{equation}

Alternative approaches include:
\begin{enumerate}
	\item Two outputs for each bit showing the reward for each of $0$ and $1.$  {\em May reflect negative rewards better?}
	
	\item Combine the rewards from the bits (with either one or two outputs per bit) by something other than addition - e.g. multiplication or via an NN. {\em The NN option seems quite interesting.  Interesting to use \verb|pytorch|'s gradients for that.}
	
	\item One outputs for each possible code. {\em Might work but $2^{N_\text{code}}$ is quite large... not impossibly so if $N_\text{code}=10.$}
	
	\item Inspired by \rcite{he2015deep}, feed $\x$ into Alice's 'first' net, to get output $\y,$ and all possible codes $\c$ into her 'second' net, both net's having the same target dimensionality (a hyperparameter).  Then the code to use is the one $\c(\x)$ closest to the output of the first net, with the $Q$ being given by the inner product $Q = \left\langle \y, \c(\x) \right\rangle.$  \rcite{dulac2015deep} might provide an alternative, actor--critic, approach on a similar theme.  The main case above is, in effect, an embedding of $\x$ into the target space (of dimensionality $N_\text{code}$) which then compares with the natural embedding of $\c$ by, in effect, the inner product.
	
	\item \rcite{majeed2020exact} suggest sequentialising, which points to a variant of our main approach which does each bit in succession and feeding those results into successive Alice--nets so the $Q$-estimate for later bits takes account of earlier bits / estimates, with the $N_\text{code}$th estimate providing a final code $\c$ and $Q$-estimate for that code.
	
	\item Move away from typical Q-learning.  Instead Alice's output is the code $\vec{c}$ and then when Bob makes his choice $\vec{x}_\text{pred}$ (see below) run that choice through a copy of Alice, to get $\vec{c}_\text{Bob}$ and then the loss function is
	\begin{equation}
		L
		=
		-\,
		r(\vec{c}, \vec{c}_\text{Bob})
		.
	\end{equation}
\end{enumerate}

\subsection{Bob}

Bob receives a matrix, $\mat{X} = (\vec{X}_{i}) = (X_{ij})$ for $0\leq i < N_\text{select},\ 0\leq j < N_\text{elements},$ and a code $\vec{c}=(c_k)_{k=0,...,N_\text{code}}.$  Why not makes his outputs be $Q$-estimates $\vec{z} = (z_i)_{i=0,...,N_\text{select}}.$ Bob's prediction is then
$
	\vec{x}_\text{pred}
=
\vec{X}_{i_\text{pred}}
$
where
\begin{equation}
	i_\text{pred}
	=
	\operatorname{argmax}_{i}  (z_i)
	.
\end{equation}
The loss function is 
\begin{equation}	\label{eq:Bob_loss_fn}
	L
	=
	\abs{\vec{z}_{i_\text{pred}} -r}^2
	. 
\end{equation}

How do we enforce covariance with respect to the order of $(\vec{X}_{i})$?
\begin{enumerate}
	\item Covariance will occur naturally and quickly without any specific intervention.  {\em To be determined.}
	\item Covariance can be enforced through choosing a set $\left\lbrace \sigma \right\rbrace \subseteq S_{n_\text{code}},$ which could be generated element--by--element by composing randomly--selected basis transpositions $(j\ j+1),$ and then adding to the loss a term
	\begin{equation}
		\mu\sum_\sigma\abs{\vec{z} - \sigma^{-1}\left[\vec{z}(\sigma[\mat{X}])\right]}^2
	\end{equation}
	for some fixed hyperparameter $\mu > 0.$  Note this the term is still run backward through the original $\vec{x}\mapsto\vec{z}$ net configuration only.  {\em How effective would that be?  How big does $\left\lbrace \sigma \right\rbrace$ have to be? And how much time would the permutation and the additions forward passes cost?}
	\item Enforce covariance via direct identification of weights in Bob's net.  {\em How?}
	\item Something related to set transformers. {\em ?}
	\item Adopt a different basic set--up where each $(X_i)$ is fed through the net separately, alongside the code $\vec{c},$ resulting in a $Q$-estimate $\vec{z}_i.$  Then find the loss function as in \eref{eq:Bob_loss_fn}.  {\em Seems the most straightforward?} {\bf Adopt this for now.}
\end{enumerate}
None of these quite amount to Bob seeks to reproduce the Alice's code vocabulary.  However Bob could additionally set up a net in the same basic configuration as Alice's (he doesn't know the weights of course) and train {\em that} net jointly with his main net.


\section{Results}

\subsection{Original strategies}

\fref{fig:fig-oneperbitsqrtlosses} is representative of the better results for the original strategies --- in other words, not very good.\footnote{The plot is taken from TensorBoard which gives an \texttt{.svg} file, then converted to \texttt{.pdf} by \texttt{rsvg-convert -f pdf -o <{\em fig-file-name}>.pdf "Sqrt losses.svg"}.}  Increasing from \verb|h.GAMESIZE = 1| to \verb|h.GAMESIZE = 32| gives no better results.
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{fig-one_per_bit_sqrt_losses}
	\caption{The best results --- from \texttt{/runs/Apr27\_23-01-58\_andrew-XPS-15-9570}. The lines show the square root of the mean square losses with (a) \texttt{lr=0.3} Alice (orange), Bob (dark blue); (b) \texttt{lr=0.1} Alice (brick red), Bob (cyan); (c) \texttt{lr=0.01} Alice (pink), Bob (green).  The plot is from TensorFlow and uses smoothing of 0.999.}
	\label{fig:fig-oneperbitsqrtlosses}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{JHEP}
\bibliography{guesstuples_project}

\end{document}

