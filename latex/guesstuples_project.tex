\documentclass[12pt]{article}

\input{guesstuples_preamble}

%opening
\title{GuessTuples Project}
\author{Andrew J. Wren}

\begin{document}

\maketitle

\begin{abstract}
	Notes on \texttt{GuessTuples} project aka \texttt{NLearn}
\end{abstract}


\section{Configuring the nets}

\subsection{Alice}

{\bf One per bit.} The input array to guess is $\vec{x}=(x_j)_{j=0,...,N_\text{elements}}.$  There should be $N_\text{code}$ outputs taking values $\vec{y} = (y_j)_{j=0,...,N_\text{code}}.$

Normalise all the rewards so that for each bit $j,$ $\rt{j} + \left(N_\text{code} - 1\right)\rf{j} = 0.$  In other words
\begin{equation}
	r_{jk}
	\gets
	r_{jk} - \frac{ \rt{j} + \left(N_\text{code} - 1\right)\rf{j}}{N_\text{code}}
	.
\end{equation}

The $Q$ estimate is then taken to be
\begin{equation}
	Q(\vec{x})
	=
	\sum_j b_j y_j
	\equiv
	\sum_j \abs{y_j}
	,		
\end{equation}
where
\begin{equation}
	b_j = \operatorname{sgn} (y_j)
\end{equation}
is the prediction for the machine value of the $j$th bit.  The loss function is
\begin{equation}
	L
	=
	\abs{Q(\vec{x}) -r}^2
	. 
\end{equation}

Alternative approaches include:
\begin{enumerate}
	\item Two outputs for each bit showing the reward for each of $0$ and $1.$  {\em May reflect negative rewards better?}
	
	\item Combine the rewards from the bits (with either one or two outputs per bit) by something other than addition - e.g. multiplication or via an NN. {\em The NN option seems quite interesting.  Interesting to use \verb|pytorch|'s gradients for that.}
	
	\item {\bf One per code.} One output for each possible code. {\em Might work but $2^{N_\text{code}}$ is quite large... not impossibly so if $N_\text{code}=8.$}
	
	\item Inspired by \rcite{he2015deep}, feed $\x$ into Alice's 'first' net, to get output $\y,$ and all possible codes $\c$ into her 'second' net, both net's having the same target dimensionality (a hyperparameter).  Then the code to use is the one $\c(\x)$ closest to the output of the first net, with the $Q$ being given by the inner product $Q = \left\langle \y, \c(\x) \right\rangle.$  \rcite{dulac2015deep} might provide an alternative, actor--critic, approach on a similar theme.  The main case above is, in effect, an embedding of $\x$ into the target space (of dimensionality $N_\text{code}$) which then compares with the natural embedding of $\c$ by, in effect, the inner product.
	
	\item \rcite{majeed2020exact} suggest sequentialising, which points to a variant of our main approach which does each bit in succession and feeding those results into successive Alice--nets so the $Q$-estimate for later bits takes account of earlier bits / estimates, with the $N_\text{code}$th estimate providing a final code $\c$ and $Q$-estimate for that code.
	
	\item Move away from typical Q-learning.  Instead Alice's output is the code $\vec{c}$ and then when Bob makes his choice $\vec{x}_\text{pred}$ (see below) run that choice through a copy of Alice, to get $\vec{c}_\text{Bob}$ and then the loss function is
	\begin{equation}
		L
		=
		-\,
		r(\vec{c}, \vec{c}_\text{Bob})
		.
	\end{equation}
\end{enumerate}

\subsection{Bob}

{\bf One per bit} aka {\bf Simple.}  Bob receives a matrix, $\mat{X} = (\vec{X}_{i}) = (X_{ij})$ for $0\leq i < N_\text{select},\ 0\leq j < N_\text{elements},$ and a code $\vec{c}=(c_k)_{k=0,...,N_\text{code}}.$  Why not makes his outputs be $Q$-estimates $\vec{z} = (z_i)_{i=0,...,N_\text{select}}.$ Bob's prediction is then
$
	\vec{x}_\text{pred}
=
\vec{X}_{i_\text{pred}}
$
where
\begin{equation}
	i_\text{pred}
	=
	\operatorname{argmax}_{i}  (z_i)
	.
\end{equation}
The loss function is 
\begin{equation}	\label{eq:Bob_loss_fn}
	L
	=
	\abs{\vec{z}_{i_\text{pred}} -r}^2
	. 
\end{equation}

How do we enforce covariance with respect to the order of $(\vec{X}_{i})$?
\begin{enumerate}
	\item Covariance will occur naturally and quickly without any specific intervention.  {\em To be determined.}
	\item Covariance can be enforced through choosing a set $\left\lbrace \sigma \right\rbrace \subseteq S_{n_\text{code}},$ which could be generated element--by--element by composing randomly--selected basis transpositions $(j\ j+1),$ and then adding to the loss a term
	\begin{equation}
		\mu\sum_\sigma\abs{\vec{z} - \sigma^{-1}\left[\vec{z}(\sigma[\mat{X}])\right]}^2
	\end{equation}
	for some fixed hyperparameter $\mu > 0.$  Note this the term is still run backward through the original $\vec{x}\mapsto\vec{z}$ net configuration only.  {\em How effective would that be?  How big does $\left\lbrace \sigma \right\rbrace$ have to be? And how much time would the permutation and the additions forward passes cost?}
	\item Enforce covariance via direct identification of weights in Bob's net.  {\em How?}
	\item Something related to set transformers. {\em ?}
	\item Adopt a different basic set--up where each $(X_i)$ is fed through the net separately, alongside the code $\vec{c},$ resulting in a $Q$-estimate $\vec{z}_i.$  Then find the loss function as in \eref{eq:Bob_loss_fn}.  {\em Seems the most straightforward?}
\end{enumerate}
None of these quite amount to Bob seeks to reproduce the Alice's code vocabulary.  However Bob could additionally set up a net in the same basic configuration as Alice's (he doesn't know the weights of course) and train {\em that} net jointly with his main net.


\section{Results}

\subsection{Original strategies}

\fref{fig:fig-oneperbitsqrtlosses} is representative of the better results for the original strategies, {\bf one per bit} --- in other words, not very good.\footnote{The plot is taken from TensorBoard which gives an \texttt{.svg} file, then converted to \texttt{.pdf} by \texttt{rsvg-convert -f pdf -o <{\em fig-file-name}>.pdf "Sqrt losses.svg"}.}  Increasing from \verb|h.GAMESIZE = 1| to \verb|h.GAMESIZE = 32| gives no better results.
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{fig-one_per_bit_sqrt_losses}
	\caption{The best results --- from \texttt{/runs/Apr27\_23-01-58\_andrew-XPS-15-9570}. The lines show the square root of the mean square losses with (a) \texttt{lr=0.3} Alice (orange), Bob (dark blue); (b) \texttt{lr=0.1} Alice (brick red), Bob (cyan); (c) \texttt{lr=0.01} Alice (pink), Bob (green).  The plot is from TensorFlow and uses smoothing of 0.999. Note rewards from random plays are counted.}
	\label{fig:fig-oneperbitsqrtlosses}
\end{figure}

\section{Revised approach --- \texttt{NLearn}}

Key runs:  
\begin{enumerate}
	\item \verb|21-05-01_12:05:16| is the strategy that works 
	\begin{lstlisting}
		'ALICE_STRATEGY': 'from_decisions',
		'BOB_STRATEGY': 'circular_vocab'
	\end{lstlisting}
	up to a point when it levels off.  Gets to $\verb|reward|=0.6$.
	
	\item  \verb|21-05-01_20:04:35| other \verb|lr| choices but same result --- see \fref{fig:figmeanrewards21-05-01200435}
	\begin{figure}
		\centering
		\includegraphics[width=0.7\linewidth]{Fig_Mean_Rewards_21-05-01_20:04:35}
		\caption{Mean Rewards per game for \texttt{21-05-01\_20:04:35}.  By colour, (Alice \texttt{lr}, Bob \texttt{lr}) are: cyan $(0.1, 0.1),$ orange $(0.1, 0.01),$ pink $(0.01, 0.1),$ and blue $(0.01, 0.01).$ Note rewards from random plays are counted.}
		\label{fig:figmeanrewards21-05-01200435}
	\end{figure}

	\item \verb|21-05-02_17:29:40| stops Alice training at some point. Alice $\verb|lr|=0.1$ and Bob $\verb|lr|=0.01$ gets to $0.8$ --- see \fref{fig:figmeanrewards21-05-02172940}.
	\begin{figure}
		\centering
		\includegraphics[width=0.7\linewidth]{Fig_Mean_Rewards_21-05-02_17:29:40}
		\caption{Mean Rewards per game for \texttt{21-05-02\_17:29:40}.  By colour, (Alice \texttt{lr}, Bob \texttt{lr}) are: green $(0.1, 0.1),$ orange $(0.1, 0.01),$ grey $(0.01, 0.1),$ and cyan $(0.01, 0.01).$   Note rewards from random plays are counted.}
		\label{fig:figmeanrewards21-05-02172940}
	\end{figure}
	The \verb|config| includes
	\begin{lstlisting}
		hyperparameters = {
			'N_ITERATIONS': 500000,
			'RANDOM_SEED': 42,
			'TORCH_RANDOM_SEED': 4242,
			'ALICE_LAYERS': 3,
			'ALICE_WIDTH': 50,
			'BOB_LAYERS': 3,
			'BOB_WIDTH': 50,
			'BATCHSIZE': 32,
			'GAMESIZE': 32,
			'BUFFER_CAPACITY': 640000,
			'START_TRAINING': 20000,
			'N_SELECT': 5,
			'EPSILON_ONE_END': 40000,
			'EPSILON_MIN': 0.01,
			'EPSILON_MIN_POINT': 300000,
			'ALICE_STRATEGY': 'from_decisions',
			'BOB_STRATEGY': 'circular_vocab',
			'ALICE_OPTIMIZER': ('SGD', '{"lr": 0.1}'),
			'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
			'ALICE_LOSS_FUNCTION': ('MSE', {}),
			'BOB_LOSS_FUNCTION': 'Same',
			'ALICE_LAST_TRAINING': 200000
	\end{lstlisting}
	Alice here, \verb|21-05-02_17:29:40 hp_run=2| generates codes as follows:
	\begin{lstlisting}
		11101100	[0, 1, 2, 12, 13, 14, 15]
		11101110	[3]
		10101110	[4, 6]
		10100110	[5, 7]
		10100100	[8]
		11100100	[9, 10, 11]
	\end{lstlisting}
	Surprisingly only six distinct codes used!  At least the first and last have sequential runs of numbers.

	\item If increase \verb|N_SELECT| to $16$ (all the numbers shown to Bob), then, in run \verb|21-05-03_10:53:10|, gets to $\verb|reward|=0.8,$ as good as for $\verb|N_SELECT|=5.$ In fact very slightly better (mean at  $\ang{25.0}$ rather than $\ang{32.9}$) Alice's code book is still very small:
	\begin{lstlisting}
		21-05-03_10:53:10BST_NLearn_model_1_Alice_iter500000
		
		01111010	[0, 15]
		01111100	[1, 2, 3, 4, 5, 14]
		01011100	[6, 7]
		01011110	[8, 9, 12, 13]
		01111110	[10, 11]
	\end{lstlisting}
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{Fig_Mean_Rewards_21-05-03_10:53:10}
	\caption{With $\texttt{N\_SELECT}=16,$ at \texttt{21-05-03\_10:53:10}.}
	\label{fig:figmeanrewards21-05-03105310}
\end{figure}
	
\end{enumerate}

\section{From now on exclude random plays from mean reward}

The exclusion is if either Alice or Bob or both is random.

\subsection{Loss includes element to push bits towards $-1$ or $1,$ and simple `proximity bonus'}\label{sec:loss-includes-element-to-push-bits-towards--1-or-1-and-simple-proximity-bonus}

This gets pretty good results --- see \fref{fig:figmeanrewards21-05-04201038} which also (orange, pink, blue) lines adds a `proximity bonus' that --- at least for these seeds --- speeds up training but does not improve the outcome.
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{Fig_Mean_Rewards_21-05-04_20:10:38}
	\caption{The green line shows the best run from \figstamp{21-05-03}{20:36:57}, which introduced \texttt{MSEBits} and had Alice stopping training at iteration $\num{300000}.$  The remaining lines are from \figstamp{21-05-04}{20:10:38} and do not stop Alice training.  They add the simple `proximity bonus' of $1$ when codes or numbers are equal from iteration $\num{100000}$ (orange), $\num{200000}$ (pink) and $\num{300000}$ (blue),  The plot has smoothing set to $0.9.$}
	\label{fig:figmeanrewards21-05-04201038}
\end{figure}

\subsection{Loss includes element to push bits towards $-1$ or $1,$ and simple `proximity bonus'}

At \verb|21-05-05_11:27:12|, changing \sref{sec:loss-includes-element-to-push-bits-towards--1-or-1-and-simple-proximity-bonus} by 
\begin{lstlisting}
	'N_ITERATIONS': 15 * (10 ** 4),
	'ALICE_PROXIMITY_BONUS': 30000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10 ** 4
\end{lstlisting}
get the excellent result shown in \fref{fig:figmeanrewards21-05-05112712}, having a final smoothed value of $0.94.$
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{Fig_Mean_Rewards_21-05-05_11:27:12}
	\caption{The red line shows the mean reward of \figstamp{21-05-05}{11:27:12}, while the just visible cyan line is its standard deviation.  The orange and green lines are as in \fref{fig:figmeanrewards21-05-04201038}, with the blue and grey lines being their respective standard deviations. The plot has smoothing set to $0.9.$}
	\label{fig:figmeanrewards21-05-05112712}
\end{figure}
The final coding and decoding books are
\begin{lstlisting}
	00100111	[0, 1, 2, 3]
	10100111	[4]
	10110111	[5]
	10110011	[6, 7]
	10111011	[8]
	10101010	[9, 10]
	10101101	[11]
	10100101	[12]
	00100101	[13, 14, 15]
	
	
	00100111	2
	10100111	4
	10110111	5
	10110011	6
	10111011	8
	10101010	10
	10101101	11
	10100101	12
	00100101	14
\end{lstlisting}
with Alice using nine codes.

However, another run, \verb|21-05-05_13:13:06|, with the same parameters, except for the three seeds, shows the high random dependence getting a small code book:
\begin{lstlisting}
	10010100	[0, 1, 2, 3, 12, 13, 14, 15]
	10010000	[4, 5, 6, 7]
	00111000	[8, 9, 10]
	10110100	[11]
	
	
	10010100	0
	10010000	5
	00111000	9
	10110100	11
\end{lstlisting}
\fref{fig:figmeanrewards21-05-05131306} compares with previous results.
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{Fig_Mean_Rewards_21-05-05_13:13:06}
	\caption{From \figstamp{21-05-05}{13:13:06} we have the red line. The grey line which is the former run shown in red in \fref{fig:figmeanrewards21-05-05112712} and the that shown in orange in \fref{fig:figmeanrewards21-05-05112712}.  The plot has smoothing set to $0.9.$} 
	\label{fig:figmeanrewards21-05-05131306}
\end{figure}
Perhaps suggests introducing some noise?

\subsection{Noise}

From \verb|21-05-05_21:56:16|, noise doesn't seem to help on this individual run --- see \fref{fig:figmeanrewards21-05-05215616}.  However, does it make the model more robust to changes in random seeds?
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{Fig_Mean_Rewards_21-05-05_21:56:16}
	\caption{From \figstamp{21-05-05}{21:56:16}, with noise of 0.01 (green), 0.03 (orange), 0.1 (grey), starting at iteration $\num{175000}$ (earlier starts were poorer).  The noise is cut off before the end to allow comparison.  Plot smoothing is at 0.9, and lesser smoothing doesn't show more post--noise recovery.}
	\label{fig:figmeanrewards21-05-05215616}
\end{figure}

\subsection{In Alice training, make both sides of the loss function have \texttt{grad}}

As in \verb|21-05-06_09:44:04|, this doesn't work --- reward oscillates around zero.  Is also slower.

\subsection{Phasing in proximity bonus, double deep learning for Alice}

In \verb|21-05-06_20:41:41| find that phasing in (over $\num{10000}$ iterations) helps --- getting to 0.96 --- but adding double deep learning for Alice may not.  See \fref{fig:figmeanrewards21-05-06204141}.
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{Fig_Mean_Rewards_21-05-06_20:41:41}
	\caption{From \figstamp{21-05-06}{20:41:41}, all have phased--in proximity bonus with \texttt{ALICE\_DOUBLE} set to $\text{None}$ (green), $\num{1000}$ (pink), $100$ (blue) and $\num{5000}$ (orange).  Smoothing 0.9.}
	\label{fig:figmeanrewards21-05-06204141}
\end{figure}

\section{After correcting error in the construction of \texttt{MSEBits}}
	
Just before \verb|21-05-07_14:03:47|, corrected an error in the construction of \texttt{MSEBits} which meant it was looking at \verb|closeness| not \verb|alice_outputs_from_target| for the bits!  This meant that trying to push closeness towards $\pm 1$ whatever it should have been!  Surprised it was so successful {\em before}.  However,  \verb|21-05-07_14:03:47| is very unsuccessful, mean reward just oscillating around zero~--- this is because I messed up taking the means in commit \verb|a3038c3| and I think previously.

That previous loss function was
\begin{lstlisting}
self.loss_fn = lambda x, y: torch.mean(
	torch.sum(
		torch.nn.functional.mse_loss(x, y, reduction='none')
		+ (mu / 2) * torch.square(x - torch.sign(x)),
		dim=-1
	)
)
\end{lstlisting}
which had the effect of trying to push the \verb|closeness| to $\pm 1$ with the second term, and~--- taking a sum over batches to be followed by a mean over a scalar~--- multiplying the learning rate by $32.$

The `correct' formulation is
\begin{lstlisting}
self.loss_fn = (
	lambda x, y, z:
	torch.nn.functional.mse_loss(x, y)
	+ (mu / 2)
	* torch.nn.functional.mse_loss(z, torch.sign(z))
)  # for both the mse_loss functions this implies reduction='mean'
\end{lstlisting}
with \verb|z| representing the raw output.

Reverting to the `previous' get a pretty good run at \verb|21-05-07_17:41:29|, but now have code and decode books on view in the run~--- and Alice's code book doesn't change at all!!!


\section{QPerCode}

\verb|QPerCode| is a strategy for Alice with its net outputting $2^{N_\text{code}}$ Q--values.  For SGD learning rates of $0.1$ and above doesn't do well, but with $lr=0.001$ at \verb|21-05-08_14:45:11BST| (and double with period $500$ and Huber \verb|beta=0.5|) get good results, peaking at reward of $0.870.$

The run \verb|21-05-08_15:50:47| for three choices of random seeds shows that this approach is robust.  It's also quick ($\num{50000}$ iteration).  See \fref{fig:figmeanrewards21-05-08155047}.
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{Fig_Mean_Rewards_21-05-08_15:50:47}
	\caption{The rewards for \figstamp{21-05-08}{15:50:47}, for the three hp runs: 1 (pink), 2 (blue) and 3 (green) with different tuples of random seeds.  Smoothing 0.9.}
	\label{fig:figmeanrewards21-05-08155047}
\end{figure}
The config has
\begin{lstlisting}
	hyperparameters = {  
		'N_ITERATIONS': 50 * (10 ** 3),   # 5 * (10 ** 5),
		'RANDOM_SEEDS': [
		(868291, 274344, 358840, 94453),
		(382832, 68444, 754888, 857796),
		(736520, 815195, 305871, 974216)
		],
		'ALICE_NET': 'FFs(3, 50)',
		'BOB_LAYERS': 3,
		'BOB_WIDTH': 50,
		'BATCHSIZE': 32,
		'GAMESIZE': 32,
		'BUFFER_CAPACITY': 32 * 20000,
		'START_TRAINING': 20000,
		'N_SELECT': 16,
		'EPSILON_ONE_END': 2000,
		'EPSILON_MIN': 0.01,
		'EPSILON_MIN_POINT': 40000,
		'ALICE_PLAY': 'QPerCode',
		'ALICE_TRAIN': 'QPerCode',
		'BOB_STRATEGY': 'circular_vocab',
		'ALICE_OPTIMIZER': [
		'SGD(lr=0.01)'
		],
		'BOB_OPTIMIZER': [
		('SGD', '{"lr": 0.01}')
		],
		'ALICE_LOSS_FUNCTION': 'Huber(beta=0.5)',
		'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}), 
		'ALICE_PROXIMITY_BONUS':  10 ** 8, 
		'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
		'ALICE_LAST_TRAINING': 100 * (10 ** 5),
		'NOISE_START': 10 ** 8,
		'NOISE': 0.,
		'ALICE_DOUBLE': 500
	}
	
	TUPLE_SPEC = (
	(16,),
	)
	N_CODE = 8
	
	SMOOTHING_LENGTH = 10000
	SAVE_PERIOD = 10 ** 5
\end{lstlisting}

Run \verb|21-05-08_17:18:16| for the first tuple of random seeds, varying the Huber \verb|beta| as in \fref{fig:figmeanrewards21-05-08171816} suggests \verb|beta=0.1| may be best.
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{Fig_Mean_Rewards_21-05-08_17:18:16}
	\caption{Run \figstamp{21-05-08}{17:18:16} showing rewards for Huber \texttt{beta} being 1 (blue), 0.5 (orange) and 0.1 (pink). Smoothing 0.9.}
	\label{fig:figmeanrewards21-05-08171816}
\end{figure}

\subsection{The first ever 1!}

On run \verb|21-05-08_18:30:55| (with a new tuple of seeds) using noise starting at \verb|30000| gives for the first time a reward of close to 1 with Alice's code book using 16 codes for the 16 numbers for the three with non--zero noise (of 0.1, 0,2 and 0.3).  Gets
\begin{lstlisting}
	 ---- Table of results ----
	
	code  hp_run	noise	result
	00000       1  		0.0	(-0.901, 50000)
	00001       2  	    0.1	(-0.990, 70000)
	00002       3  		0.2	(-0.972, 70000)
	00003       4  		0.3	(-0.967, 70000)
	--------------------------
\end{lstlisting}
marginally favouring noise of $0.1.$
See \fref{fig:figmeanrewards21-05-08183055}.
\begin{figure}
	\centering
	\includegraphics[width=1.\linewidth]{Fig_Mean_Rewards_21-05-08_18:30:55}
	\caption{See narrative on \figstamp{21-05-08}{18:30:55}.  The lines are respectively orange, pink, blue, green.  Smoothing 0.9.}
	\label{fig:figmeanrewards21-05-08183055}
\end{figure}


\section{Things to try}

\begin{enumerate}
	\item What codes does best Alice generate?
	
	\item Try using the loss function to constraint outputs to nearer bit values.  Try increasing the weighting of this.
	
	\item How quickly can \verb|epsilon| be tapered? 
	
	\item Vary learning rates.
	
	\item Vary \verb|modulus|, \verb|N_CODE| and \verb|N_SELECT|.
	
	\item Introduce noise.
	
	\item Alice strategy with a code, as input and the output are values for the numbers.  In each play (or train?) step feed all the codes in and the outputs indicate how well represents each number???
	
	\item Try best strategy but with Alice outputs having dimension \verb|2 ** N_CODE|.
	
	\item Train bits successively. 
	
	\item Look at MARL literature.
	
	\item (At some stage in the training) introduce a `proximity bonus' into Alice's training, which increases (in the same way) both the closeness of codes and the rewards if Bob's decision is right or nearly so.
	
	\item Do a second sweep of \verb|epsilon| going from high to low --- perhaps for one player only?  Definitely should re-\verb|epsilon|--randomise Bob as otherwise Alice will never (or rarely if $\verb|N_SELECT| < \verb|N_CODE|$) get fed choices not in his decoding book.  And I think Alice too, so Bob can learn new codes.
	
	\item Random seeds seem to play a significant role --- at least for short ($\sim \num{12500}$) iteration training.  Test how significant for $\num{500000}$ iterations.
	
	\item Simulate use of a code--decode book pair.
	
\end{enumerate}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{JHEP}
\bibliography{guesstuples_project}

\end{document}

