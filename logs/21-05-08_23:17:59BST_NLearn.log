The closed log for run 21-05-08_23:17:59BST

SMOOTHING_LENGTH = 10000
SAVE_PERIOD = 100000
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # Used: "cuda"
MODEL_FOLDER = 'models'
CONFIGS_FOLDER = 'configs'
LOGS_FOLDER = 'logs'

hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': [(406320, 665309, 640372, 353471)],
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 16,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': ['SGD(lr=0.01)'],
	'BOB_OPTIMIZER': [('SGD', '{"lr": 0.01}')],
	'ALICE_LOSS_FUNCTION': ['Huber(beta=0.1)'],
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': [0.1],
	'ALICE_DOUBLE': [None, 100, 300, 1000, 3000],
	'N_CODE': [8, 16],
	'N_NUMBERS': [16, 256]
}



>>>> hp_run=1 of 20
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (406320, 665309, 640372, 353471),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 16,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 16
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=     20000 training nets give:
alice_loss.item()=0.566226601600647	bob_loss.item()=0.5636007785797119

01110011	[0, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15]
10000010	[1, 2, 3]
10110110	[4]
00001000	[8]

01110011	1	False
10000010	4	False
10110110	5	False
00001000	14	False

Number of codes used=4

Iteration=     30000 training nets give:
alice_loss.item()=0.21709731221199036	bob_loss.item()=0.2885796129703522

01110011	[0, 15]
10000010	[1, 2, 3]
10110110	[4, 5, 6, 7]
01101100	[8, 9, 10, 11, 12]
01111011	[13, 14]

01110011	13	False
10000010	2	True
10110110	4	True
01101100	11	True
01111011	12	False

Number of codes used=5

Iteration=     40000 training nets give:
alice_loss.item()=0.07494567334651947	bob_loss.item()=0.07640045881271362

10000011	[0]
10000010	[1, 2]
10000110	[3]
10110100	[4]
10110110	[5, 6]
01001100	[7, 8, 9]
01111100	[10]
01101100	[11]
01111011	[12]
01101011	[13]
01100011	[14, 15]

10000011	1	False
10000010	1	True
10000110	2	False
10110100	4	True
10110110	4	False
01001100	9	True
01111100	10	True
01101100	9	False
01111011	13	False
01101011	13	True
01100011	14	True

Number of codes used=11

Iteration=     50000 training nets give:
alice_loss.item()=0.06041722744703293	bob_loss.item()=0.08883371949195862

11100011	[0]
11000010	[1]
10000110	[2]
00000110	[3]
10110110	[4]
10100100	[5]
10101110	[6, 7]
00001100	[8]
01001100	[9]
01001000	[10]
01100100	[11]
01101111	[12]
01001011	[13]
01000001	[14]
10001011	[15]

11100011	15	False
11000010	1	True
10000110	2	True
00000110	3	True
10110110	5	False
10100100	5	True
10101110	6	True
00001100	9	False
01001100	10	False
01001000	10	True
01100100	11	True
01101111	12	True
01001011	13	True
01000001	14	True
10001011	15	True

Number of codes used=15

Iteration=     60000 training nets give:
alice_loss.item()=0.012007310055196285	bob_loss.item()=0.06314908713102341

11010011	[0]
10000011	[1]
10000000	[2]
00000110	[3]
10010111	[4]
10100100	[5]
10101110	[6]
00101110	[7]
00011100	[8]
00101100	[9]
01101000	[10]
01100100	[11]
01101111	[12]
01111011	[13]
01100011	[14]
10101011	[15]

11010011	0	True
10000011	1	True
10000000	2	True
00000110	3	True
10010111	4	True
10100100	5	True
10101110	6	True
00101110	7	True
00011100	8	True
00101100	9	True
01101000	10	True
01100100	11	True
01101111	12	True
01111011	13	True
01100011	14	True
10101011	15	True

Number of codes used=16

Iteration=     70000 training nets give:
alice_loss.item()=0.00015749460726510733	bob_loss.item()=0.00010808328806888312

11010011	[0]
10000011	[1]
10001000	[2]
00000110	[3]
10010111	[4]
00110101	[5]
10101110	[6]
00101110	[7]
00011100	[8]
00101100	[9]
01001000	[10]
01100100	[11]
01101111	[12]
00101011	[13]
01000001	[14]
10001011	[15]

11010011	0	True
10000011	1	True
10001000	2	True
00000110	3	True
10010111	4	True
00110101	5	True
10101110	6	True
00101110	7	True
00011100	8	True
00101100	8	False
01001000	10	True
01100100	11	True
01101111	12	True
00101011	13	True
01000001	14	True
10001011	15	True

Number of codes used=16


End of hp run 1.  Result of run:
[(-0.9977864397050018, 70000), ('21-05-08_23:17:59BST_NLearn_model_1_Alice_iter70000', '21-05-08_23:17:59BST_NLearn_model_1_Bob_iter70000')]
(-0.9977864397050018, 70000)


>>>> hp_run=2 of 20, time elapsed 0:20:12 of estimated 6:44:02, 
implying ending at 06:02:02BST on Sunday 9 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (406320, 665309, 640372, 353471),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 16,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7705517503711219

Iteration=     20000 training nets give:
alice_loss.item()=0.694038987159729	bob_loss.item()=0.5482646226882935

11110100	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 249, 250, 251, 252, 253, 254, 255]
01000101	[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]
01001011	[29]
11111001	[30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
10010111	[62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247]
11001111	[248]

11110100	146	False
01000101	110	False
01001011	187	False
11111001	201	False
10010111	120	True
11001111	96	False

Number of codes used=6

Iteration=     30000 training nets give:
alice_loss.item()=0.36007463932037354	bob_loss.item()=0.3835304379463196

01111110	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00011101	[59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86]
10010111	[87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]
00010000	[160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235]

01111110	14	True
00011101	44	False
10010111	143	True
00010000	189	True

Number of codes used=4

Iteration=     40000 training nets give:
alice_loss.item()=0.1571958363056183	bob_loss.item()=0.131921648979187

01011110	[0, 1, 2, 3, 4, 5, 252, 253, 254, 255]
01111110	[6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]
11111110	[27]
01011101	[28, 29, 30, 31, 32, 33, 34, 35, 36, 37]
00001101	[38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93]
00011001	[94, 95, 96, 97, 98, 99, 100]
10011111	[101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]
00010001	[160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174]
10010000	[175, 176, 177]
00000000	[178, 179, 180]
00010000	[181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202]
01010000	[203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227]
01111111	[228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240]
01110110	[241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251]

01011110	12	False
01111110	15	True
11111110	22	False
01011101	42	False
00001101	54	True
00011001	59	False
10011111	121	True
00010001	169	True
10010000	189	False
00000000	180	True
00010000	190	True
01010000	213	True
01111111	14	False
01110110	1	False

Number of codes used=14

Iteration=     50000 training nets give:
alice_loss.item()=0.03810478746891022	bob_loss.item()=0.17305555939674377

01011010	[0, 1, 2, 3, 255]
01111100	[4, 5, 6, 7, 8, 9, 10, 11, 12]
01111110	[13, 14, 15, 16, 17, 18, 19]
00011110	[20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
01011101	[32, 33, 34, 35]
00011100	[36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]
00001100	[47, 48, 49, 50, 51, 52, 53]
00011101	[54, 55, 56, 57, 58, 59, 60]
00000101	[61, 62, 63, 64, 65, 66, 67, 68, 69]
10001101	[70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81]
10101101	[82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106]
10001011	[107, 108]
10001111	[109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120]
10011111	[121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135]
10101011	[136]
10010111	[137, 138, 139, 140, 141, 142, 143, 144]
10010110	[145, 146, 147, 148, 149]
00010111	[150, 151, 152]
00010011	[153, 154]
11010111	[155, 156, 157, 158, 159, 160, 161]
00010001	[162, 163, 164, 165, 166, 167, 168, 169]
00100000	[170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180]
10010000	[181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191]
00010000	[192]
01000000	[193, 194, 195, 196, 197, 198, 199, 200, 201, 202]
01010000	[203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220]
01000010	[221, 222]
01010010	[223, 224, 225, 226, 227, 228, 229, 230, 231]
01110010	[232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250]
01110100	[251, 252, 253, 254]

01011010	254	False
01111100	12	True
01111110	17	True
00011110	12	False
01011101	37	False
00011100	46	True
00001100	52	True
00011101	59	True
00000101	59	False
10001101	77	True
10101101	94	True
10001011	109	False
10001111	116	True
10011111	122	True
10101011	124	False
10010111	131	False
10010110	141	False
00010111	146	False
00010011	156	False
11010111	150	False
00010001	179	False
00100000	182	False
10010000	184	True
00010000	206	False
01000000	204	False
01010000	212	True
01000010	210	False
01010010	228	True
01110010	241	True
01110100	249	False

Number of codes used=30

Iteration=     60000 training nets give:
alice_loss.item()=0.022791648283600807	bob_loss.item()=0.014162572100758553

00111010	[0, 1, 248, 249, 250, 251, 252, 253, 254, 255]
01100100	[2, 3]
01111010	[4, 5]
01101110	[6, 7, 8, 9, 10, 11, 12, 13, 14]
11111110	[15, 16, 17, 18, 19, 20, 21, 22, 23, 24]
01111101	[25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
10111100	[36, 37, 38, 39, 42, 43]
01011101	[40, 41]
00011100	[44, 45, 46, 47, 48, 49, 50]
00001100	[51, 52, 53, 54, 55]
00011101	[56, 57, 58, 59, 60, 61, 62]
00001101	[63, 64, 65]
10001100	[66, 67, 68, 69, 70]
10001101	[71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81]
10011101	[82, 83, 84, 85, 86, 87, 88]
10101101	[89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102]
10001011	[103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113]
10011111	[114, 115, 116, 117, 118, 119, 120, 121, 122]
10011011	[123, 124, 125, 126, 127]
10110111	[128, 129, 130, 131, 132, 133, 134, 135]
10010110	[136, 137]
10010011	[138, 139, 140, 141, 142, 143, 144, 145]
00010111	[146, 147, 148]
10010010	[149, 150, 151, 152, 153, 154]
10110001	[155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168]
00000110	[169, 170]
00000001	[171, 172, 173, 174]
00000010	[175, 176]
00100000	[177, 178]
10010000	[179, 180, 181, 182, 183, 184]
00000100	[185, 186, 187, 188, 189, 190, 191, 192, 193]
00011000	[194, 195, 196, 197, 198, 199]
00010100	[200, 201, 202, 203, 204, 205, 206]
01010000	[207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218]
01000010	[219, 220, 221]
01010010	[222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233]
11110010	[234, 235, 236, 237, 238, 239, 240]
01010110	[241, 242, 243, 244]
01110100	[245, 246, 247]

00111010	247	False
01100100	0	False
01111010	8	False
01101110	15	False
11111110	22	True
01111101	27	True
10111100	42	True
01011101	36	False
00011100	41	False
00001100	53	True
00011101	59	True
00001101	64	True
10001100	73	False
10001101	77	True
10011101	81	False
10101101	94	True
10001011	108	True
10011111	126	False
10011011	123	True
10110111	133	True
10010110	140	False
10010011	144	True
00010111	150	False
10010010	154	True
10110001	154	False
00000110	164	False
00000001	167	False
00000010	172	False
00100000	175	False
10010000	179	True
00000100	188	True
00011000	192	False
00010100	197	False
01010000	213	True
01000010	214	False
01010010	232	True
11110010	241	False
01010110	246	False
01110100	247	True

Number of codes used=39

Iteration=     70000 training nets give:
alice_loss.item()=0.0025960677303373814	bob_loss.item()=0.000736778718419373

01100100	[0, 1, 254, 255]
01110110	[2]
01111010	[3, 4, 5, 6, 7, 8]
01001110	[9]
01101110	[10, 11]
01111110	[12, 13, 14, 15, 16, 17, 18, 19]
01001111	[20, 21, 22, 23]
01001100	[24, 25, 26, 27, 28, 29, 30, 31, 32, 33]
01011101	[34, 35, 36, 37, 38, 39, 40, 41, 42, 43]
00011100	[44, 45, 46, 47, 48, 49, 50, 51]
00011001	[52, 53, 54, 55]
00011101	[56, 57, 58, 59, 60]
00000101	[61]
00001101	[62, 63, 64, 65, 66]
10001100	[67, 68, 69, 70, 71, 72]
10111101	[73, 74]
10001101	[75, 76, 77, 78, 79]
10011101	[80, 81, 82, 83, 84]
10101101	[85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
10100101	[100, 101, 102, 103]
10001011	[104, 105, 106, 107, 108, 109, 110, 111]
10001111	[112, 113, 114, 115, 116, 117, 118]
10011111	[119, 120, 121, 122, 123, 124, 125, 126, 127]
10110111	[128, 129, 130, 131, 132]
10000110	[133, 134, 135, 136, 137, 138, 139, 140]
10011010	[141, 142, 143, 144, 145]
00010111	[146]
10010001	[147, 148, 149, 150, 151]
11010111	[152]
10010010	[153, 154, 155, 156, 157, 158, 159]
10000010	[160, 161, 162]
00000110	[163, 164, 165, 166]
10010100	[167, 168, 169, 170, 171, 172, 173]
00000010	[174, 175, 176, 177, 178]
00001000	[179, 180, 181, 182, 184, 185, 186]
10010000	[183]
00000100	[187, 188]
00110000	[189, 190, 191, 192, 193, 194]
00010000	[195, 196, 197, 198, 199, 200, 201, 202]
01000000	[203, 204, 205, 206]
01010001	[207, 208, 209, 210]
01010000	[211, 212, 213, 214, 215, 216, 217]
01000010	[218, 219, 220, 221, 222]
01010010	[223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233]
01110010	[234, 235, 236, 237, 238, 239, 240, 241, 242, 243]
01010110	[244]
01110100	[245, 246, 247, 248]
00111010	[249, 250, 251, 252, 253]

01100100	254	True
01110110	3	False
01111010	3	True
01001110	11	False
01101110	19	False
01111110	15	True
01001111	22	True
01001100	31	True
01011101	37	True
00011100	44	True
00011001	59	False
00011101	57	True
00000101	66	False
00001101	61	False
10001100	74	False
10111101	75	False
10001101	77	True
10011101	80	True
10101101	93	True
10100101	102	True
10001011	109	True
10001111	116	True
10011111	125	True
10110111	137	False
10000110	140	True
10011010	153	False
00010111	155	False
10010001	148	True
11010111	157	False
10010010	162	False
10000010	155	False
00000110	165	True
10010100	171	True
00000010	176	True
00001000	183	False
10010000	179	False
00000100	190	False
00110000	190	True
00010000	192	False
01000000	206	True
01010001	206	False
01010000	209	False
01000010	220	True
01010010	224	True
01110010	240	True
01010110	248	False
01110100	254	False
00111010	251	True

Number of codes used=48


End of hp run 2.  Result of run:
[(-0.9935805820881335, 70000), ('21-05-08_23:17:59BST_NLearn_model_2_Alice_iter70000', '21-05-08_23:17:59BST_NLearn_model_2_Bob_iter70000')]
(-0.9935805820881335, 70000)


>>>> hp_run=3 of 20, time elapsed 0:40:38 of estimated 6:46:18, 
implying ending at 06:04:17BST on Sunday 9 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (406320, 665309, 640372, 353471),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 16,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 16,
	'N_NUMBERS': 16
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=     20000 training nets give:
alice_loss.item()=0.530038595199585	bob_loss.item()=0.3790414035320282

0010111100101101	[0, 1]
0001110011110010	[2, 3, 4]
1001001000011000	[5]
1101100110011001	[6, 7, 8]
0011011111011100	[9]
0100111000011011	[10]
0110011110111100	[11]
1000110000110100	[12]
1100011010101011	[13, 14, 15]

0010111100101101	9	False
0001110011110010	14	False
1001001000011000	15	False
1101100110011001	14	False
0011011111011100	3	False
0100111000011011	14	False
0110011110111100	14	False
1000110000110100	8	False
1100011010101011	12	False

Number of codes used=9

Iteration=     30000 training nets give:
alice_loss.item()=0.18712164461612701	bob_loss.item()=0.10862968116998672

1010011110000111	[0, 14, 15]
0111000000110000	[1, 2, 3, 4]
0000001100000110	[5, 6]
0111111111010110	[7]
1101001011101001	[8, 9, 10, 11]
1100011010101011	[12, 13]

1010011110000111	15	True
0111000000110000	2	True
0000001100000110	6	True
0111111111010110	8	False
1101001011101001	9	True
1100011010101011	14	False

Number of codes used=6

Iteration=     40000 training nets give:
alice_loss.item()=0.15735691785812378	bob_loss.item()=0.03383482247591019

0110110001001001	[0]
0000101111100110	[1]
0000111110010010	[2]
0001010000111010	[3]
1101001011001001	[4, 5]
1100101100111101	[6]
1101001011101000	[7, 8]
0001011100001011	[9, 10]
1010110101000011	[11]
0100110001000111	[12]
1001000001000101	[13]
1011101101111000	[14]
0000110100100100	[15]

0110110001001001	14	False
0000101111100110	6	False
0000111110010010	7	False
0001010000111010	3	True
1101001011001001	10	False
1100101100111101	4	False
1101001011101000	9	False
0001011100001011	4	False
1010110101000011	15	False
0100110001000111	12	True
1001000001000101	9	False
1011101101111000	4	False
0000110100100100	14	False

Number of codes used=13

Iteration=     50000 training nets give:
alice_loss.item()=0.5427716970443726	bob_loss.item()=0.17001771926879883

1010011100000011	[0]
0011010010000001	[1]
0111100000110000	[2]
1110100000100110	[3, 4]
1111110111110010	[5]
0000001100101110	[6]
0111101111010110	[7]
0111111111010010	[8]
1101001011101001	[9]
1101001011111001	[10]
1001001011101001	[11]
1101011010101011	[12]
1110111101101010	[13]
1110111101100011	[14]
0110111000011000	[15]

1010011100000011	15	False
0011010010000001	1	True
0111100000110000	2	True
1110100000100110	15	False
1111110111110010	5	True
0000001100101110	5	False
0111101111010110	7	True
0111111111010010	7	False
1101001011101001	9	True
1101001011111001	9	False
1001001011101001	9	False
1101011010101011	13	False
1110111101101010	13	True
1110111101100011	14	True
0110111000011000	14	False

Number of codes used=15

Iteration=     60000 training nets give:
alice_loss.item()=0.11214801669120789	bob_loss.item()=0.0301381703466177

1111111101100011	[0]
0011010010000001	[1]
0111100000110000	[2]
0001010010111010	[3]
0011000101010111	[4]
1001000111110010	[5]
0000101100001110	[6]
0111101111010110	[7]
0111110111010110	[8]
1101101011001111	[9]
0110110011011011	[10, 11]
0100101001100010	[12]
1110111101101010	[13]
1110111101100011	[14]
1011011110100111	[15]

1111111101100011	13	False
0011010010000001	1	True
0111100000110000	2	True
0001010010111010	3	True
0011000101010111	4	True
1001000111110010	5	True
0000101100001110	6	True
0111101111010110	7	True
0111110111010110	8	True
1101101011001111	9	True
0110110011011011	11	True
0100101001100010	12	True
1110111101101010	13	True
1110111101100011	14	True
1011011110100111	15	True

Number of codes used=15

Iteration=     70000 training nets give:
alice_loss.item()=0.0018805983709171414	bob_loss.item()=0.0008515139925293624

1010011110000110	[0]
0011010010000001	[1]
0001010000111000	[2]
0001010010111010	[3]
0011000101010111	[4]
1111110111110010	[5]
0000011100000110	[6]
0111111111010110	[7]
0111110111010110	[8]
1101101011001111	[9]
1101001011101000	[10]
0110110011011011	[11]
0100101001100010	[12]
1110111101101010	[13]
1110111101100001	[14]
1010010110000011	[15]

1010011110000110	0	True
0011010010000001	1	True
0001010000111000	2	True
0001010010111010	3	True
0011000101010111	4	True
1111110111110010	5	True
0000011100000110	6	True
0111111111010110	7	True
0111110111010110	8	True
1101101011001111	9	True
1101001011101000	10	True
0110110011011011	11	True
0100101001100010	12	True
1110111101101010	14	False
1110111101100001	14	True
1010010110000011	15	True

Number of codes used=16


End of hp run 3.  Result of run:
[(-0.9787140564231916, 70000), ('21-05-08_23:17:59BST_NLearn_model_3_Alice_iter70000', '21-05-08_23:17:59BST_NLearn_model_3_Bob_iter70000')]
(-0.9787140564231916, 70000)


>>>> hp_run=4 of 20, time elapsed 1:05:46 of estimated 7:18:27, 
implying ending at 06:36:26BST on Sunday 9 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (406320, 665309, 640372, 353471),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 16,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 16,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7705517503711219

Iteration=     20000 training nets give:
alice_loss.item()=0.557136058807373	bob_loss.item()=0.4911712110042572

0100010000111100	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
0000000111100101	[43, 44]
1011000000010000	[45, 46, 47, 48, 49, 50]
0011101010011010	[51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68]
0001000001011010	[69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85]
1100100000010110	[86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105]
0101010101101011	[106, 107, 108, 109, 110, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241]
0111101011000100	[111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130]
1101101010000111	[131, 132, 133, 134, 135, 136, 137, 138]
0011111110000001	[139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161]
1100011100110100	[162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196]
1111100111010000	[197, 198, 199, 200, 201, 202, 203]

0100010000111100	214	False
0000000111100101	126	False
1011000000010000	36	False
0011101010011010	230	False
0001000001011010	207	False
1100100000010110	214	False
0101010101101011	203	False
0111101011000100	253	False
1101101010000111	239	False
0011111110000001	13	False
1100011100110100	255	False
1111100111010000	209	False

Number of codes used=12

Iteration=     30000 training nets give:
alice_loss.item()=0.2844986319541931	bob_loss.item()=0.255831778049469

0001000111111001	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
1011000000010000	[23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]
1111100110000110	[70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
0010001010111001	[144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183]
1111100111010000	[184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219]

0001000111111001	1	True
1011000000010000	43	True
1111100110000110	109	True
0010001010111001	158	True
1111100111010000	202	True

Number of codes used=5

Iteration=     40000 training nets give:
alice_loss.item()=0.2839299738407135	bob_loss.item()=0.20422731339931488

0001000111111001	[0, 1, 250, 251, 252, 253, 254, 255]
1001101010111000	[2, 3, 4, 5]
1011000010010000	[6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]
0000111101000111	[29, 33, 34, 35]
0100111101111101	[30, 31, 32]
1011000000010000	[36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
1111000110000110	[52, 53, 54, 55]
1011010000010000	[56, 57, 58, 59, 60]
1101110010001110	[61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75]
1011000000110000	[76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
1111100100000110	[128, 129, 130, 131]
1100101110110001	[132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145]
1011111000110001	[146]
1011000010111100	[147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169]
1111100111010100	[170, 171, 172, 173, 174, 175, 176, 177, 178]
1010100001000010	[179, 180, 181, 182, 183, 184, 185, 186, 187]
0001000111111000	[188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207]
0000010011001110	[208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228]
0101000111111001	[229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249]

0001000111111001	8	False
1001101010111000	172	False
1011000010010000	54	False
0000111101000111	111	False
0100111101111101	195	False
1011000000010000	45	True
1111000110000110	104	False
1011010000010000	46	False
1101110010001110	92	False
1011000000110000	52	False
1111100100000110	114	False
1100101110110001	179	False
1011111000110001	130	False
1011000010111100	56	False
1111100111010100	186	False
1010100001000010	84	False
0001000111111000	7	False
0000010011001110	230	False
0101000111111001	252	False

Number of codes used=19

Iteration=     50000 training nets give:
alice_loss.item()=0.2823908030986786	bob_loss.item()=0.16016119718551636

0001000111111001	[0, 1, 2, 3, 4, 253, 254, 255]
0001000101111001	[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]
0001000111111000	[18, 19, 20, 21, 22, 23, 24]
1011010000010010	[25, 26, 27, 28]
1011000000011000	[29, 30, 31, 32]
1011000000010000	[33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52]
1011010000010000	[53, 54, 55, 56, 57, 58, 59]
1011000000010100	[60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73]
1011000000010001	[74, 75, 76, 77, 78, 79, 80]
1011110000010000	[81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 128, 129, 130, 131]
1110100111000110	[104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116]
1111110100000110	[117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
1101100010000110	[132, 133, 134, 135, 136, 137]
0010001010110001	[138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149]
0010001010111001	[150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164]
0110111110110001	[165]
0010011010111001	[166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185]
1111100111000000	[186, 187, 188]
1110111011010110	[189]
1100000110110111	[190, 191, 192, 193, 194, 195, 196]
1111100111011000	[197, 198]
1111100111010000	[199, 200, 201, 202, 203]
1001000011110100	[204, 205]
0101010100010101	[206, 207, 208, 209, 210, 211, 212, 213]
0101011101101011	[214, 215, 216]
0100010101101011	[217, 218, 219, 220, 221]
0101010101101011	[222, 223, 224, 225, 226, 227, 228, 229, 230, 231]
0011010100011001	[232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243]
0001010111111001	[244, 245, 246, 247, 248, 249, 250, 251, 252]

0001000111111001	4	True
0001000101111001	10	True
0001000111111000	253	False
1011010000010010	41	False
1011000000011000	54	False
1011000000010000	48	True
1011010000010000	48	False
1011000000010100	49	False
1011000000010001	62	False
1011110000010000	49	False
1110100111000110	104	True
1111110100000110	115	False
1101100010000110	98	False
0010001010110001	158	False
0010001010111001	157	True
0110111110110001	158	False
0010011010111001	159	False
1111100111000000	180	False
1110111011010110	196	False
1100000110110111	199	False
1111100111011000	192	False
1111100111010000	189	False
1001000011110100	9	False
0101010100010101	216	False
0101011101101011	218	False
0100010101101011	228	False
0101010101101011	230	True
0011010100011001	239	True
0001010111111001	251	True

Number of codes used=29

Iteration=     60000 training nets give:
alice_loss.item()=0.26679760217666626	bob_loss.item()=0.2328108251094818

0001000111111001	[0, 1, 2, 3, 4, 254, 255]
0001000101111001	[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]
1011000100010000	[23, 24]
0011110000110010	[25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]
1011000000010000	[43, 44, 45, 46, 47, 48, 49, 50, 51]
1011000000000000	[52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
1011010000010001	[64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75]
1111000000000000	[76, 77, 78, 79, 80, 81]
1011000110000110	[82, 83]
1011100110000110	[84, 85, 86, 87, 88, 89, 90, 91]
1111100010000110	[92, 93, 94, 95, 96]
1101100110000110	[97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117]
1001100110000110	[118, 119]
1111110100000110	[120, 121, 122]
1111100110000111	[123, 124, 125, 126, 127]
0010001010111000	[128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161]
0010001010111001	[162, 163, 164, 165]
0110111110110001	[166, 167, 168, 169, 170, 171]
1100101011100000	[172]
0010000010111001	[173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183]
1110111011010110	[184, 185, 186, 187, 188, 189, 190, 191]
0100101000010111	[192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212]
0101010100010101	[213, 214, 215, 216]
0111010101101011	[217, 218, 219, 220, 221, 222, 223]
0101010101101011	[224, 225, 226, 227, 228, 229, 230, 231]
0011010100011001	[232, 233, 234, 235, 236, 237, 238, 239, 240]
0001000111101001	[241, 242, 243, 244, 245]
0001010111111001	[246, 247, 248, 249, 250]
0001010101111001	[251, 252, 253]

0001000111111001	7	False
0001000101111001	12	True
1011000100010000	46	False
0011110000110010	33	True
1011000000010000	55	False
1011000000000000	54	True
1011010000010001	65	True
1111000000000000	64	False
1011000110000110	89	False
1011100110000110	95	False
1111100010000110	97	False
1101100110000110	111	True
1001100110000110	108	False
1111110100000110	111	False
1111100110000111	108	False
0010001010111000	139	True
0010001010111001	161	False
0110111110110001	162	False
1100101011100000	183	False
0010000010111001	171	False
1110111011010110	193	False
0100101000010111	208	True
0101010100010101	216	True
0111010101101011	220	True
0101010101101011	228	True
0011010100011001	229	False
0001000111101001	251	False
0001010111111001	241	False
0001010101111001	2	False

Number of codes used=29

Iteration=     70000 training nets give:
alice_loss.item()=0.0034604601096361876	bob_loss.item()=0.004271856043487787

0001010101111001	[0, 252, 253, 254, 255]
0001000111111001	[1, 2, 3]
0001000101111001	[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]
0011110000110010	[22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]
1011010000010000	[41, 42]
1011000000010000	[43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57]
1011000000000000	[58, 59, 60, 61]
1011010000010001	[62, 63, 64, 65, 66, 67, 68, 69]
1011001000011000	[70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92]
1111100110000010	[93, 94, 95, 96, 97, 98, 99, 100]
1011100110100110	[101, 102, 103]
1001100110000110	[104, 105, 106, 107]
1101100100000110	[108, 109, 110, 111]
1111110100000110	[112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
1010001010110001	[128, 129]
0010001010111000	[130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148]
0010001010110001	[149, 150, 151, 152, 153, 154]
0010001010111001	[155, 156, 157, 158, 159, 160]
0011001010111001	[161, 162, 163, 164]
0010001000111001	[165, 166, 167, 168]
1100101011100000	[169, 170, 171, 172, 173, 174]
1101101101010010	[175, 176, 177, 178]
1111100111000000	[179, 180, 181]
1111100111010010	[182, 183]
1110111011010110	[184, 185, 186, 187, 188, 189, 190, 191, 192, 193]
1111000111010000	[194, 195, 196, 197, 198, 199]
0100101000010111	[200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210]
0101010100010101	[211, 212, 213, 214]
0111010101101011	[215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225]
0101010101101011	[226, 227, 228, 229, 230, 231]
1101010101101011	[232, 233, 234, 235]
0011010100011001	[236, 237, 238, 239]
1001000111111101	[240, 241, 242, 243]
0001010111111001	[244, 245, 246, 247, 248, 249, 250, 251]

0001010101111001	251	False
0001000111111001	6	False
0001000101111001	9	True
0011110000110010	35	True
1011010000010000	49	False
1011000000010000	51	True
1011000000000000	62	False
1011010000010001	69	True
1011001000011000	85	True
1111100110000010	109	False
1011100110100110	101	True
1001100110000110	108	False
1101100100000110	112	False
1111110100000110	119	True
1010001010110001	139	False
0010001010111000	141	True
0010001010110001	145	False
0010001010111001	158	True
0011001010111001	160	False
0010001000111001	165	True
1100101011100000	172	True
1101101101010010	177	True
1111100111000000	191	False
1111100111010010	182	True
1110111011010110	187	True
1111000111010000	197	True
0100101000010111	207	True
0101010100010101	214	True
0111010101101011	220	True
0101010101101011	226	True
1101010101101011	232	True
0011010100011001	226	False
1001000111111101	248	False
0001010111111001	248	True

Number of codes used=34


End of hp run 4.  Result of run:
[(-0.9843659644532734, 70000), ('21-05-08_23:17:59BST_NLearn_model_4_Alice_iter70000', '21-05-08_23:17:59BST_NLearn_model_4_Bob_iter70000')]
(-0.9843659644532734, 70000)


>>>> hp_run=5 of 20, time elapsed 1:30:54 of estimated 7:34:31, 
implying ending at 06:52:30BST on Sunday 9 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (406320, 665309, 640372, 353471),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 16,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': 100,
	'N_CODE': 8,
	'N_NUMBERS': 16
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=     20000 training nets give:
alice_loss.item()=0.7294525504112244	bob_loss.item()=0.4598318040370941

01101111	[0, 1, 2, 3, 4, 12, 13, 14, 15]
11011100	[5]
11010110	[6, 7, 8, 9]
10000101	[10, 11]

01101111	15	True
11011100	15	False
11010110	0	False
10000101	1	False

Number of codes used=4

Iteration=     30000 training nets give:
alice_loss.item()=0.12846358120441437	bob_loss.item()=0.23390698432922363

01101111	[0, 13, 14, 15]
00001001	[1, 2, 3, 4]
00011110	[5, 6, 7]
00000110	[8, 9, 10, 11, 12]

01101111	0	True
00001001	3	True
00011110	7	True
00000110	9	True

Number of codes used=4

Iteration=     40000 training nets give:
alice_loss.item()=0.036116696894168854	bob_loss.item()=0.08530888706445694

01101111	[0]
11100111	[1, 2]
00001001	[3]
00001000	[4, 5]
00011100	[6]
00011110	[7]
00010110	[8]
00000100	[9]
00000111	[10]
00001110	[11]
10010110	[12, 13]
01100111	[14, 15]

01101111	0	True
11100111	14	False
00001001	4	False
00001000	4	True
00011100	7	False
00011110	7	True
00010110	8	True
00000100	9	True
00000111	10	True
00001110	10	False
10010110	10	False
01100111	15	True

Number of codes used=12

Iteration=     50000 training nets give:
alice_loss.item()=0.1439659595489502	bob_loss.item()=0.10852713882923126

00101101	[0, 15]
10001000	[1]
01001000	[2]
00001001	[3]
00001000	[4]
00000000	[5]
10011100	[6]
00011110	[7]
00001100	[8]
00000101	[9]
00000111	[10]
00100111	[11, 12]
11110111	[13]
11100111	[14]

00101101	0	True
10001000	2	False
01001000	3	False
00001001	4	False
00001000	4	True
00000000	5	True
10011100	6	True
00011110	7	True
00001100	8	True
00000101	9	True
00000111	10	True
00100111	11	True
11110111	13	True
11100111	14	True

Number of codes used=14

Iteration=     60000 training nets give:
alice_loss.item()=0.021052565425634384	bob_loss.item()=0.0473472997546196

01101101	[0]
00001101	[1]
01000000	[2]
00101000	[3]
00011000	[4]
00000000	[5]
10011110	[6]
00011010	[7]
00001100	[8]
00000011	[9]
00001111	[10]
00100110	[11]
10100111	[12]
11110111	[13]
01100110	[14]
01110011	[15]

01101101	0	True
00001101	1	True
01000000	2	True
00101000	3	True
00011000	4	True
00000000	4	False
10011110	6	True
00011010	7	True
00001100	8	True
00000011	9	True
00001111	10	True
00100110	11	True
10100111	12	True
11110111	13	True
01100110	14	True
01110011	15	True

Number of codes used=16

Iteration=     70000 training nets give:
alice_loss.item()=0.005871706176549196	bob_loss.item()=0.018236536532640457

01101101	[0]
00001101	[1]
01000000	[2]
00101000	[3]
00011000	[4]
00000000	[5]
10011110	[6]
00011010	[7]
00001010	[8]
01000100	[9]
00001111	[10]
00100110	[11]
10100111	[12]
11110111	[13]
01110101	[14]
00101011	[15]

01101101	0	True
00001101	1	True
01000000	2	True
00101000	3	True
00011000	4	True
00000000	5	True
10011110	6	True
00011010	7	True
00001010	8	True
01000100	9	True
00001111	10	True
00100110	11	True
10100111	12	True
11110111	13	True
01110101	14	True
00101011	0	False

Number of codes used=16


End of hp run 5.  Result of run:
[(-0.9923415178072391, 70000), ('21-05-08_23:17:59BST_NLearn_model_5_Alice_iter70000', '21-05-08_23:17:59BST_NLearn_model_5_Bob_iter70000')]
(-0.9923415178072391, 70000)


>>>> hp_run=6 of 20, time elapsed 1:51:20 of estimated 7:25:19, 
implying ending at 06:43:18BST on Sunday 9 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (406320, 665309, 640372, 353471),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 16,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': 100,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7705517503711219

Iteration=     20000 training nets give:
alice_loss.item()=0.6840039491653442	bob_loss.item()=0.6044204235076904

00100000	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11110110	[85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
00110000	[128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194]
10100100	[195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219]

00100000	184	False
11110110	154	False
00110000	169	True
10100100	170	False

Number of codes used=4

Iteration=     30000 training nets give:
alice_loss.item()=0.2395346611738205	bob_loss.item()=0.13730359077453613

10101000	[0, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00101111	[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]
11100110	[65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121]
11110110	[122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146]
00110000	[147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195]

10101000	225	True
00101111	32	True
11100110	93	True
11110110	125	True
00110000	176	True

Number of codes used=5

Iteration=     40000 training nets give:
alice_loss.item()=0.07930916547775269	bob_loss.item()=0.07650148123502731

10101010	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00101101	[14, 15]
00101111	[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]
00001111	[37, 38, 39, 40]
00101011	[41, 42, 43, 44, 45, 46, 47, 48, 49, 50]
10101111	[51]
11101110	[52, 53, 54, 55, 56, 57, 58, 59]
11100010	[60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]
11100111	[73, 74, 75, 76, 77, 78, 79, 80]
01101111	[81, 82, 83, 84, 85, 86, 87, 88, 89]
11100110	[90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107]
10110110	[108, 113, 147, 148, 149, 150, 151, 152, 153, 154]
01110110	[109, 110, 111, 112]
11110110	[114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135]
11110100	[136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146]
00110000	[155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184]
00110111	[185, 186, 187, 188, 189, 190, 191]
00111000	[192, 193, 194, 195, 196, 197, 198, 199, 200]
10111000	[201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215]
10101000	[216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239]
10001000	[240, 241, 242]

10101010	239	False
00101101	24	False
00101111	38	False
00001111	39	True
00101011	28	False
10101111	42	False
11101110	80	False
11100010	91	False
11100111	81	False
01101111	58	False
11100110	96	True
10110110	130	False
01110110	113	False
11110110	127	True
11110100	134	False
00110000	180	True
00110111	95	False
00111000	198	True
10111000	211	True
10101000	223	True
10001000	233	False

Number of codes used=21

Iteration=     50000 training nets give:
alice_loss.item()=0.03143380582332611	bob_loss.item()=0.0665886402130127

10001011	[0, 1, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10101011	[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]
00001101	[18, 19, 20, 21, 22, 23]
10001111	[24, 25, 26, 27, 28]
00011111	[29, 30, 31, 32, 33, 34, 35, 36]
11000011	[37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55]
01100111	[56, 57, 58, 59, 60, 61, 62, 63]
11100011	[64, 65, 66, 67, 68, 69, 70, 71, 72]
11100101	[73]
01100110	[74, 75, 76, 77, 78]
11010110	[79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109]
11100110	[93, 94, 95, 96, 97, 98]
01110110	[110, 111, 112, 113, 114, 115]
11110110	[116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130]
11110100	[131, 132, 133, 134, 135, 136, 137, 138]
11010100	[139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153]
00110101	[154]
00010100	[155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169]
00110000	[170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186]
00111000	[187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]
10100100	[200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212]
10111010	[213, 214, 215, 216]
10101000	[217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228]
10000010	[229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243]
10001010	[244, 245]

10001011	2	False
10101011	2	True
00001101	22	True
10001111	32	False
00011111	25	False
11000011	66	False
01100111	90	False
11100011	75	False
11100101	55	False
01100110	85	False
11010110	106	True
11100110	92	False
01110110	120	False
11110110	126	True
11110100	137	True
11010100	144	True
00110101	169	False
00010100	164	True
00110000	169	False
00111000	194	True
10100100	204	True
10111010	216	True
10101000	226	True
10000010	242	True
10001010	248	False

Number of codes used=25

Iteration=     60000 training nets give:
alice_loss.item()=0.04962571710348129	bob_loss.item()=0.06128240004181862

00101010	[0, 1, 2, 3, 4, 5, 6, 7, 8, 252, 253, 254, 255]
00001011	[9, 10, 11, 12, 13]
10101110	[14, 15, 16, 17, 18]
00101101	[19, 20, 21]
00101011	[22, 23, 24, 25, 26]
00001111	[27, 28, 29, 30, 31, 32, 33, 34]
10101111	[35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]
00000101	[47, 48]
00100111	[49, 50]
01101111	[51, 52, 53, 54, 55, 56, 57, 58, 59, 60]
10100111	[61]
11100101	[62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]
11100011	[73, 74]
11101110	[75, 76, 77, 78, 79]
11100111	[80]
11100010	[81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98]
11100110	[99, 100]
11111110	[101]
11010110	[102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113]
01110110	[114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124]
11010010	[125, 126, 127, 128, 129, 130]
11110110	[131]
11110100	[132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144]
11010100	[145, 146]
01110000	[147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157]
00110101	[158, 159, 160, 161, 162, 163, 164, 165, 166, 167]
00110100	[168, 169, 170, 171, 172]
00110000	[173, 174, 175, 176, 177, 178, 179, 180]
00110001	[181, 182, 183, 184]
10110000	[185, 186, 187, 188, 189, 190]
00010000	[191, 192, 193, 194]
00111000	[195, 196, 197, 198, 199]
10100100	[200, 201, 202, 203, 204, 205]
10111000	[206, 207, 208, 209]
10100000	[210, 211, 212]
10001100	[213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 228, 229, 230, 231, 232]
10011010	[226, 227]
00101000	[233, 234, 235]
10101010	[236, 237, 238, 239, 240, 241, 242, 243, 244]
10001101	[245, 246, 247, 248, 249, 250, 251]

00101010	5	True
00001011	13	True
10101110	10	False
00101101	19	True
00101011	22	True
00001111	29	True
10101111	26	False
00000101	44	False
00100111	48	False
01101111	61	False
10100111	59	False
11100101	66	True
11100011	73	True
11101110	74	False
11100111	79	False
11100010	89	True
11100110	95	False
11111110	101	True
11010110	108	True
01110110	116	True
11010010	128	True
11110110	120	False
11110100	137	True
11010100	145	True
01110000	150	True
00110101	173	False
00110100	170	True
00110000	179	True
00110001	184	True
10110000	192	False
00010000	185	False
00111000	197	True
10100100	205	True
10111000	208	True
10100000	221	False
10001100	234	False
10011010	232	False
00101000	227	False
10101010	246	False
10001101	250	True

Number of codes used=40

Iteration=     70000 training nets give:
alice_loss.item()=0.001933857100084424	bob_loss.item()=0.000986176310107112

00101010	[0, 254, 255]
10101011	[1, 2, 3, 4, 5, 6, 7, 8, 9]
00001011	[10, 11, 12, 13, 14, 15, 16]
00101101	[17]
00101011	[18, 19, 20, 21, 22, 23, 24, 25, 26]
00001111	[27, 28, 29]
00101111	[30, 31, 32, 33]
00111111	[34, 35, 36]
00101110	[37, 38, 39, 40, 41, 42]
00100011	[43, 44, 45]
00100111	[46, 47, 48, 49, 50, 51, 52, 53]
10100111	[54, 55, 56, 57, 58, 59, 60]
01101110	[61]
11101111	[62, 63, 64, 65, 66, 67, 68, 69, 70]
11100011	[71, 72, 73, 74, 75, 76]
11101110	[77]
11100111	[78, 79, 80, 81, 82]
01100110	[83, 84, 85, 86]
10100110	[87, 88, 89, 90, 91, 92, 93, 94, 95]
11100110	[96, 97, 98]
10110111	[99, 100]
11111110	[101, 102, 103, 104, 105, 106, 107, 108, 109]
11100100	[110, 111, 112, 113, 114, 115]
01110110	[116]
11110010	[117, 118, 119, 120, 121, 122, 123, 124, 125]
01110010	[126]
10110110	[127, 128, 129, 130, 131, 132]
01110100	[133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
11010100	[144, 145, 146, 147, 148]
01110000	[149, 150, 151]
00110010	[152, 153, 154, 155, 156, 157, 158]
00010100	[159, 160, 161, 162, 163, 164, 165, 166, 167]
00110100	[168, 169]
00110000	[170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180]
00110001	[181, 182]
00010000	[183, 184, 185, 186, 187, 188]
01111000	[189]
10110000	[190, 191, 192, 193, 194, 195, 196]
00111100	[197, 198, 199, 200, 201]
10100100	[202, 203, 204]
00111001	[205, 206, 207, 208, 209, 210, 211]
10100000	[212]
10111010	[213, 214, 215, 216, 217]
11001000	[218]
10000001	[219, 220, 221, 222, 223]
10101000	[224, 225, 226, 227, 228, 229, 230]
10011010	[231, 232, 233]
10001001	[234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247]
00001001	[248]
10001101	[249, 250, 251, 252, 253]

00101010	4	False
10101011	1	True
00001011	13	True
00101101	24	False
00101011	19	True
00001111	31	False
00101111	40	False
00111111	37	False
00101110	37	True
00100011	49	False
00100111	68	False
10100111	58	True
01101110	65	False
11101111	68	True
11100011	70	False
11101110	76	False
11100111	81	True
01100110	78	False
10100110	95	True
11100110	95	False
10110111	104	False
11111110	102	True
11100100	112	True
01110110	114	False
11110010	117	True
01110010	126	True
10110110	131	True
01110100	138	True
11010100	144	True
01110000	150	True
00110010	152	True
00010100	165	True
00110100	181	False
00110000	178	True
00110001	187	False
00010000	187	True
01111000	193	False
10110000	187	False
00111100	195	False
10100100	205	False
00111001	208	True
10100000	205	False
10111010	209	False
11001000	221	False
10000001	219	True
10101000	229	True
10011010	231	True
10001001	230	False
00001001	245	False
10001101	252	True

Number of codes used=50


End of hp run 6.  Result of run:
[(-0.9933323581681125, 70000), ('21-05-08_23:17:59BST_NLearn_model_6_Alice_iter70000', '21-05-08_23:17:59BST_NLearn_model_6_Bob_iter70000')]
(-0.9933323581681125, 70000)


>>>> hp_run=7 of 20, time elapsed 2:11:46 of estimated 7:19:12, 
implying ending at 06:37:11BST on Sunday 9 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (406320, 665309, 640372, 353471),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 16,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': 100,
	'N_CODE': 16,
	'N_NUMBERS': 16
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=     20000 training nets give:
alice_loss.item()=0.6570044755935669	bob_loss.item()=0.6112608909606934

1101101111010101	[0, 9]
0001010000001011	[1]
1110010100110011	[2, 3]
0001111011100000	[4]
0000110001100101	[5, 6]
1101010011110000	[7]
0101100000100110	[8]
1110000101101001	[10]
0001101001000011	[11, 13]
0111011111001111	[12]
0100100011010101	[14]
0101010110001111	[15]

1101101111010101	1	False
0001010000001011	15	False
1110010100110011	0	False
0001111011100000	15	False
0000110001100101	13	False
1101010011110000	0	False
0101100000100110	0	False
1110000101101001	0	False
0001101001000011	0	False
0111011111001111	0	False
0100100011010101	1	False
0101010110001111	15	True

Number of codes used=12

Iteration=     30000 training nets give:
alice_loss.item()=0.13271047174930573	bob_loss.item()=0.158742755651474

0110010011011101	[0, 1, 2, 3]
0101100111100100	[4, 5, 6, 11]
0000001100110011	[7, 8, 9, 10]
0101010110001111	[12, 13, 14, 15]

0110010011011101	1	True
0101100111100100	7	False
0000001100110011	8	True
0101010110001111	14	True

Number of codes used=4

Iteration=     40000 training nets give:
alice_loss.item()=0.27854830026626587	bob_loss.item()=0.19349339604377747

1101111011100000	[0, 1]
1111001000111011	[2]
0100000100111111	[3, 8, 9, 10, 11, 12]
1111000100011010	[4, 5]
0000101100110011	[6, 7]
0010010001110000	[13, 14, 15]

1101111011100000	5	False
1111001000111011	5	False
0100000100111111	10	True
1111000100011010	4	True
0000101100110011	9	False
0010010001110000	4	False

Number of codes used=6

Iteration=     50000 training nets give:
alice_loss.item()=0.43024942278862	bob_loss.item()=0.1692785918712616

0010010011011101	[0]
1110010100110011	[1, 2]
1100000101100100	[3]
1111000100111010	[4]
1111001100011010	[5]
1011011110110111	[6]
0100010100111111	[7]
0100001100110011	[8, 9]
0100000100111111	[10, 11]
0101011110001111	[12]
0101010011001111	[13]
0001011011000100	[14]
0110010111011101	[15]

0010010011011101	1	False
1110010100110011	2	True
1100000101100100	7	False
1111000100111010	4	True
1111001100011010	5	True
1011011110110111	9	False
0100010100111111	9	False
0100001100110011	9	True
0100000100111111	10	True
0101011110001111	13	False
0101010011001111	13	True
0001011011000100	0	False
0110010111011101	1	False

Number of codes used=13

Iteration=     60000 training nets give:
alice_loss.item()=0.20059002935886383	bob_loss.item()=0.14819705486297607

0101100111010011	[0, 15]
1001010111001001	[1]
1110010110110011	[2]
1001110000100101	[3]
1111000100011011	[4]
0111000100011010	[5]
1110000100111010	[6]
0001101101111101	[7]
1010000110000011	[8]
0100000100111101	[9]
0100000100111111	[10]
0000010101111101	[11]
0001010110101111	[12]
0101010011001111	[13]
0101010110011111	[14]

0101100111010011	15	True
1001010111001001	1	True
1110010110110011	2	True
1001110000100101	3	True
1111000100011011	4	True
0111000100011010	5	True
1110000100111010	6	True
0001101101111101	7	True
1010000110000011	8	True
0100000100111101	9	True
0100000100111111	10	True
0000010101111101	8	False
0001010110101111	12	True
0101010011001111	13	True
0101010110011111	14	True

Number of codes used=15

Iteration=     70000 training nets give:
alice_loss.item()=0.0007816650904715061	bob_loss.item()=0.0015069785295054317

0110010011011101	[0]
1001010111001001	[1]
1110010110110011	[2]
1110010101100010	[3]
1111000100011011	[4]
0111000100011010	[5]
1110000100111010	[6]
0001101101111101	[7]
1010000110000011	[8]
0000001100110011	[9]
0100000100111111	[10, 11]
0001010110101111	[12]
0101010011001111	[13]
0101010110011111	[14]
0101100111010011	[15]

0110010011011101	0	True
1001010111001001	1	True
1110010110110011	2	True
1110010101100010	3	True
1111000100011011	4	True
0111000100011010	5	True
1110000100111010	6	True
0001101101111101	7	True
1010000110000011	8	True
0000001100110011	9	True
0100000100111111	10	True
0001010110101111	12	True
0101010011001111	13	True
0101010110011111	14	True
0101100111010011	15	True

Number of codes used=15


End of hp run 7.  Result of run:
[(-0.9626392541003088, 70000), ('21-05-08_23:17:59BST_NLearn_model_7_Alice_iter70000', '21-05-08_23:17:59BST_NLearn_model_7_Bob_iter70000')]
(-0.9626392541003088, 70000)


>>>> hp_run=8 of 20, time elapsed 2:36:54 of estimated 7:28:16, 
implying ending at 06:46:15BST on Sunday 9 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (406320, 665309, 640372, 353471),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 16,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': 100,
	'N_CODE': 16,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7705517503711219

Iteration=     20000 training nets give:
alice_loss.item()=0.7913365364074707	bob_loss.item()=0.5551789402961731

0011100001000110	[0, 1, 2, 3, 4, 5, 6, 220, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
1001111001000100	[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52]
0111111111010100	[53, 54, 55, 56]
1000001110010011	[57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81]
1000011101100010	[82, 83, 84, 85]
1011101100000010	[86, 87, 88, 89, 90, 91, 92]
0100001100110111	[93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198]
0000000100111101	[199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219]
1000111011010000	[221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240]

0011100001000110	223	False
1001111001000100	205	False
0111111111010100	246	False
1000001110010011	252	False
1000011101100010	245	False
1011101100000010	238	False
0100001100110111	30	False
0000000100111101	48	False
1000111011010000	211	False

Number of codes used=9

Iteration=     30000 training nets give:
alice_loss.item()=0.33759593963623047	bob_loss.item()=0.20736297965049744

0011100001000110	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
1011101001111110	[20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67]
1101011111011001	[68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130]
0010101111101010	[131, 132, 133]
1100100111011100	[134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198]
0010011011101110	[199]
1000111011010000	[200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235]

0011100001000110	254	True
1011101001111110	48	True
1101011111011001	98	True
0010101111101010	169	False
1100100111011100	165	True
0010011011101110	186	False
1000111011010000	237	False

Number of codes used=7

Iteration=     40000 training nets give:
alice_loss.item()=0.25499892234802246	bob_loss.item()=0.12803733348846436

1011001001010110	[0, 252, 253, 254, 255]
0110001111011001	[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]
0111011110111000	[22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
0010011111000101	[60, 61, 62, 63]
1110000100111100	[64, 65, 66, 67, 68]
1011110001100000	[69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]
0001100000101101	[90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121]
0111000100101101	[122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139]
1111100110101011	[140, 141, 142, 143, 144, 145, 146]
1110110110000101	[147, 148, 149, 150, 151, 152, 153, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181]
0111110100000010	[154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 182, 183, 184, 185]
0010010011110000	[186, 187, 188, 189, 190, 191, 192, 193]
1111110000011101	[194, 195, 196, 197, 198, 199, 200, 201, 202, 203]
1100100110011100	[204, 205]
1100010100011101	[206]
0110111001100111	[207, 208, 209, 210, 211, 212, 213]
1011101000101001	[214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243]
0111100001000110	[244, 245, 246, 247, 248, 249, 250, 251]

1011001001010110	29	False
0110001111011001	143	False
0111011110111000	125	False
0010011111000101	230	False
1110000100111100	161	False
1011110001100000	21	False
0001100000101101	21	False
0111000100101101	141	False
1111100110101011	137	False
1110110110000101	164	False
0111110100000010	32	False
0010010011110000	223	False
1111110000011101	154	False
1100100110011100	180	False
1100010100011101	141	False
0110111001100111	26	False
1011101000101001	42	False
0111100001000110	1	False

Number of codes used=18

Iteration=     50000 training nets give:
alice_loss.item()=0.4099125564098358	bob_loss.item()=0.11662694811820984

0011100001000110	[0, 1, 2, 3, 4, 5, 6, 249, 250, 251, 252, 253, 254, 255]
0011100001000111	[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
1110011001110111	[24, 25, 26]
1011101001011110	[27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]
1011101001111010	[42, 43]
1011101001111110	[44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56]
1111101001111110	[57, 58, 59, 60]
1101001000100010	[61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]
0101101001011001	[75, 76]
1101011101011001	[77, 78, 79, 80, 81, 82]
1101011111010001	[83, 84]
1101011011011001	[85, 86]
1101011111011001	[87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
1111011111011001	[100, 101]
1101111111011001	[102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113]
1100011100111001	[114, 115, 116, 117, 118, 119]
1100100111000011	[120]
0100110110010010	[121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132]
1111111100001000	[133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146]
1100101100010000	[147]
1100100111111100	[148, 149, 150, 151, 152, 153, 154]
1000100111001100	[155, 156, 157, 158, 159, 160]
1100100111011101	[161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174]
0001100101010011	[175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189]
0010011011101100	[190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211]
1010111011010000	[212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222]
1000111011010100	[223, 224, 225, 226, 227]
1000111011010000	[228, 229, 230, 231, 232, 233, 234, 235]
0011100011001110	[236, 237, 238, 239, 240, 241]
0000100110110010	[242, 243, 244, 245, 246, 247, 248]

0011100001000110	1	True
0011100001000111	10	True
1110011001110111	29	False
1011101001011110	40	True
1011101001111010	38	False
1011101001111110	39	False
1111101001111110	41	False
1101001000100010	54	False
0101101001011001	153	False
1101011101011001	99	False
1101011111010001	96	False
1101011011011001	100	False
1101011111011001	104	False
1111011111011001	95	False
1101111111011001	114	False
1100011100111001	99	False
1100100111000011	199	False
0100110110010010	168	False
1111111100001000	137	True
1100101100010000	164	False
1100100111111100	172	False
1000100111001100	174	False
1100100111011101	159	False
0001100101010011	18	False
0010011011101100	199	True
1010111011010000	217	True
1000111011010100	216	False
1000111011010000	227	False
0011100011001110	7	False
0000100110110010	248	True

Number of codes used=30

Iteration=     60000 training nets give:
alice_loss.item()=0.14408966898918152	bob_loss.item()=0.05530703067779541

0011100001000110	[0, 1, 2, 3, 4, 5, 6, 252, 253, 254, 255]
0011100001000100	[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]
1011011000000110	[22, 23, 24, 25, 26, 27, 28]
1110011001110111	[29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]
0011101001111110	[40]
1011101001111110	[41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
1011001001111110	[52, 53, 54, 55, 56, 57, 58, 59]
1011101001101110	[60, 61, 62, 63]
0001010110011011	[64, 65, 66, 67, 68, 69, 70, 71, 72, 73]
1101111111010011	[74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88]
0100011110110000	[89, 90]
1101011111011001	[91, 92, 93, 94, 95, 96, 97, 98, 99]
1111011011011001	[100]
1100011100111101	[101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111]
1101011110011001	[112, 113, 114, 115, 116]
0100001110100010	[117, 118, 119, 120]
1001111111100011	[121, 122, 123, 124, 125]
0100011110110001	[126, 127]
1100110100111001	[128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142]
1111111100001000	[143, 144, 145, 146, 147]
1100110111111100	[148, 149, 150, 151, 152, 153, 154, 155, 156]
1100100111011101	[157, 158, 159, 160, 161, 162, 163, 164, 165]
1100100111010100	[166, 167, 168, 169]
1100100110011100	[170, 171, 172]
0100001000000111	[173, 174]
0010011111011111	[175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192]
0010011011101100	[193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208]
0010010011101100	[209, 210, 211, 212, 213]
1010111011010000	[214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226]
1000111011010000	[227, 228, 229, 230, 231, 232]
1000111010010000	[233, 234, 235, 236, 237, 238, 239, 240, 241]
1000011011010000	[242, 243, 244]
0000100110110010	[245, 246]
0110100011111111	[247, 248, 249, 250, 251]

0011100001000110	6	True
0011100001000100	17	True
1011011000000110	30	False
1110011001110111	31	True
0011101001111110	45	False
1011101001111110	51	True
1011001001111110	48	False
1011101001101110	53	False
0001010110011011	147	False
1101111111010011	85	True
0100011110110000	120	False
1101011111011001	104	False
1111011011011001	79	False
1100011100111101	107	True
1101011110011001	111	False
0100001110100010	133	False
1001111111100011	124	True
0100011110110001	130	False
1100110100111001	129	True
1111111100001000	140	False
1100110111111100	146	False
1100100111011101	165	True
1100100111010100	166	True
1100100110011100	167	False
0100001000000111	206	False
0010011111011111	186	True
0010011011101100	200	True
0010010011101100	205	False
1010111011010000	231	False
1000111011010000	238	False
1000111010010000	242	False
1000011011010000	237	False
0000100110110010	249	False
0110100011111111	254	False

Number of codes used=34

Iteration=     70000 training nets give:
alice_loss.item()=0.06405657529830933	bob_loss.item()=0.004571830853819847

0011100001000110	[0, 1, 2, 3, 254, 255]
0011100001001110	[4, 5, 8, 9, 10, 11]
0011100101000110	[6, 7]
0011100001000100	[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]
1011011000000110	[25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38]
0011101001111110	[39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]
1011001001111110	[54, 55, 56, 57, 58]
1101001000100010	[59, 60, 61, 62, 63]
1011001101010001	[64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75]
1101111111010011	[76, 86, 87]
1001011111011001	[77, 78, 79, 80, 81, 82, 83, 84, 85]
1101011101011011	[88, 89, 90]
1101011001011001	[91, 92, 93, 94, 95, 96, 97]
1111011011011001	[98, 99]
1101011111011000	[100, 101, 102, 103]
1101111111011001	[104, 105, 106, 107]
1101011110011001	[108, 109, 110, 111, 112]
1100001100111001	[113, 114, 115, 116, 117, 118, 119, 120, 121]
1001111111100011	[122, 123, 124, 125, 126, 127, 128, 129, 130]
1111111100001000	[131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145]
1010011110111101	[146, 147, 148]
1100110111111100	[149, 150, 151, 152, 153, 154, 155, 156, 157]
1100000111011100	[158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169]
1100100111010100	[170, 171, 172, 173, 174, 175]
0010011111011111	[176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192]
0010011011101100	[193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203]
0010010011101100	[204, 205, 206, 207, 208, 209, 210]
0000111011010000	[211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222]
1010111011010000	[223, 224]
1000011011010000	[225, 226, 227, 228, 229, 230, 231, 232, 233, 234]
1000111010010000	[235, 236, 237, 238, 239, 240]
0000100110110010	[241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253]

0011100001000110	4	False
0011100001001110	8	True
0011100101000110	1	False
0011100001000100	18	True
1011011000000110	25	True
0011101001111110	43	True
1011001001111110	53	False
1101001000100010	66	False
1011001101010001	66	True
1101111111010011	80	False
1001011111011001	88	False
1101011101011011	90	True
1101011001011001	96	True
1111011011011001	106	False
1101011111011000	105	False
1101111111011001	100	False
1101011110011001	110	True
1100001100111001	114	True
1001111111100011	124	True
1111111100001000	142	True
1010011110111101	134	False
1100110111111100	143	False
1100000111011100	165	True
1100100111010100	166	False
0010011111011111	180	True
0010011011101100	202	True
0010010011101100	207	True
0000111011010000	216	True
1010111011010000	223	True
1000011011010000	236	False
1000111010010000	243	False
0000100110110010	245	True

Number of codes used=32


End of hp run 8.  Result of run:
[(-0.976940777489037, 70000), ('21-05-08_23:17:59BST_NLearn_model_8_Alice_iter70000', '21-05-08_23:17:59BST_NLearn_model_8_Bob_iter70000')]
(-0.976940777489037, 70000)


>>>> hp_run=9 of 20, time elapsed 3:02:02 of estimated 7:35:05, 
implying ending at 06:53:04BST on Sunday 9 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (406320, 665309, 640372, 353471),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 16,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': 300,
	'N_CODE': 8,
	'N_NUMBERS': 16
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=     20000 training nets give:
alice_loss.item()=0.4460007846355438	bob_loss.item()=0.3926512598991394

10010000	[0, 15]
00001011	[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
10110111	[13]
00101101	[14]

10010000	13	False
00001011	12	True
10110111	15	False
00101101	14	True

Number of codes used=4

Iteration=     30000 training nets give:
alice_loss.item()=0.2725544571876526	bob_loss.item()=0.3191207945346832

11111110	[0, 1, 2, 15]
11001100	[3, 4, 5, 6, 7, 8]
00001011	[9, 10, 11]
00101101	[12, 13, 14]

11111110	15	True
11001100	5	True
00001011	8	False
00101101	14	True

Number of codes used=4

Iteration=     40000 training nets give:
alice_loss.item()=0.20755213499069214	bob_loss.item()=0.12141653895378113

11101110	[0, 1]
11001110	[2, 3]
11011100	[4]
11001100	[5]
01001100	[6]
00011011	[7, 8]
00000011	[9]
00001010	[10, 11]
10101101	[12]
01101101	[13]
00101101	[14]
00111101	[15]

11101110	1	True
11001110	2	True
11011100	3	False
11001100	6	False
01001100	6	True
00011011	9	False
00000011	9	True
00001010	9	False
10101101	12	True
01101101	13	True
00101101	12	False
00111101	13	False

Number of codes used=12

Iteration=     50000 training nets give:
alice_loss.item()=0.07464241981506348	bob_loss.item()=0.13390731811523438

11110110	[0]
11001111	[1]
11001110	[2]
11011010	[3]
11011000	[4]
11001101	[5]
01001100	[6]
01001000	[7]
00001110	[8]
00010011	[9]
00101010	[10]
00101001	[11]
10101101	[12]
01101101	[13]
00111100	[14]
11110111	[15]

11110110	0	True
11001111	2	False
11001110	2	True
11011010	3	True
11011000	4	True
11001101	5	True
01001100	6	True
01001000	7	True
00001110	8	True
00010011	9	True
00101010	9	False
00101001	12	False
10101101	13	False
01101101	13	True
00111100	14	True
11110111	15	True

Number of codes used=16

Iteration=     60000 training nets give:
alice_loss.item()=0.027030380442738533	bob_loss.item()=0.06102149933576584

11101111	[0]
01111111	[1]
10001110	[2]
11011100	[3]
11101100	[4]
11001101	[5]
01001110	[6]
01001000	[7]
00001110	[8]
10001010	[9]
01011001	[10]
10101001	[11]
01100101	[12]
01101111	[13]
00111100	[14]
11110111	[15]

11101111	1	False
01111111	1	True
10001110	2	True
11011100	3	True
11101100	4	True
11001101	5	True
01001110	6	True
01001000	7	True
00001110	8	True
10001010	9	True
01011001	9	False
10101001	11	True
01100101	12	True
01101111	13	True
00111100	14	True
11110111	15	True

Number of codes used=16

Iteration=     70000 training nets give:
alice_loss.item()=0.0006935722194612026	bob_loss.item()=0.00010966048284899443

11110110	[0]
01111111	[1]
11001110	[2]
11011100	[3]
11101100	[4]
11001000	[5]
01001100	[6]
01001000	[7]
00001110	[8]
10001010	[9]
00101011	[10]
10101001	[11]
01100101	[12]
01101111	[13]
11111101	[14]
11110111	[15]

11110110	0	True
01111111	1	True
11001110	2	True
11011100	3	True
11101100	4	True
11001000	5	True
01001100	6	True
01001000	7	True
00001110	8	True
10001010	9	True
00101011	10	True
10101001	11	True
01100101	12	True
01101111	13	True
11111101	14	True
11110111	15	True

Number of codes used=16


End of hp run 9.  Result of run:
[(-0.9938725373577013, 70000), ('21-05-08_23:17:59BST_NLearn_model_9_Alice_iter70000', '21-05-08_23:17:59BST_NLearn_model_9_Bob_iter70000')]
(-0.9938725373577013, 70000)


>>>> hp_run=10 of 20, time elapsed 3:22:23 of estimated 7:29:44, 
implying ending at 06:47:43BST on Sunday 9 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (406320, 665309, 640372, 353471),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 16,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': 300,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7705517503711219

Iteration=     20000 training nets give:
alice_loss.item()=0.5987159013748169	bob_loss.item()=0.739158034324646

01011101	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11110001	[30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155]

01011101	148	False
11110001	78	True

Number of codes used=2

Iteration=     30000 training nets give:
alice_loss.item()=0.2001934051513672	bob_loss.item()=0.31236380338668823

11001001	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11110001	[44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110]
10110111	[111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176]
00100100	[177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231]

11001001	9	True
11110001	74	True
10110111	151	True
00100100	202	True

Number of codes used=4

Iteration=     40000 training nets give:
alice_loss.item()=0.08981509506702423	bob_loss.item()=0.1640123724937439

11001001	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 252, 253, 254, 255]
11001011	[21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]
11111000	[34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
11110000	[52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66]
11111001	[67]
11110001	[68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101]
11110011	[102, 103, 104, 105]
10110011	[106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133]
10110111	[134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157]
10110110	[158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172]
00100101	[173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184]
00101100	[185, 186, 187, 188, 189, 190, 191, 192]
00100100	[193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218]
00000100	[219, 220, 221, 222, 223]
11001000	[224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244]
11000001	[245, 246, 247, 248, 249, 250, 251]

11001001	15	True
11001011	23	True
11111000	57	False
11110000	61	True
11111001	69	False
11110001	75	True
11110011	79	False
10110011	127	True
10110111	144	True
10110110	155	False
00100101	186	False
00101100	196	False
00100100	203	True
00000100	204	False
11001000	9	False
11000001	24	False

Number of codes used=16

Iteration=     50000 training nets give:
alice_loss.item()=0.09137152135372162	bob_loss.item()=0.06999380141496658

00001001	[0, 1, 2, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00000010	[3, 4]
11001001	[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]
11000101	[23, 24]
11001011	[25, 26, 27, 28, 29, 30, 31, 32]
01111000	[33, 34, 35, 36]
10100001	[37]
11100001	[38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55]
11010001	[56, 57, 58, 59, 60, 61, 62, 63]
11110000	[64, 65, 66, 67, 68]
11111001	[69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81]
11110011	[82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103]
11110010	[104, 105, 106, 107]
10110011	[108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122]
11110111	[123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137]
10111111	[138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149]
10011111	[150, 151, 152, 153, 154, 155]
00110111	[156, 157]
10100111	[158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169]
10100101	[170, 171, 172, 173, 174, 175]
10101100	[176]
00100101	[177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188]
00000100	[189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]
00000000	[210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224]
00001000	[225, 226, 227, 228, 229, 230, 231, 232, 233]

00001001	249	True
00000010	194	False
11001001	16	True
11000101	28	False
11001011	22	False
01111000	29	False
10100001	49	False
11100001	57	False
11010001	68	False
11110000	73	False
11111001	66	False
11110011	87	True
11110010	93	False
10110011	123	False
11110111	133	True
10111111	141	True
10011111	152	True
00110111	165	False
10100111	156	False
10100101	166	False
10101100	186	False
00100101	187	True
00000100	206	True
00000000	213	True
00001000	240	False

Number of codes used=25

Iteration=     60000 training nets give:
alice_loss.item()=0.03339187055826187	bob_loss.item()=0.04062880948185921

01001101	[0, 1, 254, 255]
10001001	[2, 3, 4, 5, 6, 7]
11001001	[8, 9]
10000001	[10, 11, 12, 13, 14, 15, 16]
11001011	[17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
10011001	[30, 31, 32, 33, 34, 35]
01111000	[36, 37]
01100001	[38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
01010001	[48, 49, 50, 51, 52]
00110001	[53]
01110000	[54, 55, 56, 57, 58, 59, 60, 61]
11111101	[62, 63, 64]
11111001	[65, 66, 67, 68, 69, 70, 71, 72, 73, 74]
11110001	[75, 76, 77, 78, 79, 80, 81, 82]
11110101	[83, 84, 85, 86, 87]
11110010	[88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108]
10110011	[109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122]
10111011	[123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136]
10110111	[137, 138, 139, 140, 141, 142, 143]
10111111	[144]
10110101	[145, 146, 147, 148, 149, 150, 151, 152, 153]
01110111	[154]
11110110	[155, 156, 157]
10010111	[158, 159]
10100111	[160]
10111101	[161, 162, 163]
10110100	[164, 165, 166, 167, 168]
10100101	[169, 170, 171]
01110100	[172, 173, 174, 175, 176, 177, 178, 179]
01110110	[180, 181, 182, 183]
10100100	[184, 185, 186, 187, 188, 189, 190]
01100100	[191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202]
00000100	[203, 204, 205, 206, 207, 208, 209, 210, 211]
00000000	[212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]
00001000	[230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242]
00001001	[243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253]

01001101	1	True
10001001	11	False
11001001	17	False
10000001	16	True
11001011	22	True
10011001	32	True
01111000	34	False
01100001	38	True
01010001	53	False
00110001	52	False
01110000	52	False
11111101	60	False
11111001	68	True
11110001	80	True
11110101	83	True
11110010	95	True
10110011	117	True
10111011	132	True
10110111	142	True
10111111	140	False
10110101	147	True
01110111	150	False
11110110	152	False
10010111	143	False
10100111	157	False
10111101	163	True
10110100	163	False
10100101	168	False
01110100	182	False
01110110	170	False
10100100	182	False
01100100	197	True
00000100	202	False
00000000	214	True
00001000	237	True
00001001	248	True

Number of codes used=36

Iteration=     70000 training nets give:
alice_loss.item()=0.01836484670639038	bob_loss.item()=0.022465365007519722

11001000	[0, 1, 2, 3, 4, 5, 6, 7]
10001001	[8]
11001001	[9, 10, 11, 12, 13, 14, 15]
11000001	[16, 17, 18, 19]
10101001	[20, 21, 22, 23, 24]
11101001	[25, 26]
11000101	[27, 28, 29]
10011001	[30, 31, 32]
11011000	[33, 34, 35]
11011001	[36, 37, 38, 39, 40, 41, 42]
10111000	[43, 44]
11010000	[45, 46, 47, 48]
01010001	[49, 50, 51, 52, 53]
01110000	[54, 55, 56, 57, 58, 59]
11111101	[60, 61, 62, 63]
10110000	[64, 65, 66, 67, 68]
11111001	[69]
11110000	[70, 71, 72, 73, 74, 75, 76, 77]
10110001	[78]
11111011	[79, 80, 81, 82, 83, 84]
11110101	[85, 86, 87, 88]
11110011	[89, 90]
11110010	[91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106]
10110011	[107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124]
11110111	[125, 126, 136, 137]
10111011	[127, 128, 129, 130, 131, 132, 133, 134, 135]
10110111	[138, 139, 140, 142, 143, 144]
10111111	[141, 145]
10110101	[146, 147, 148, 149, 150, 151, 152, 153]
11110110	[154, 155, 156, 157]
10111110	[158, 159, 160]
10111010	[161, 162, 163, 164, 165, 166]
10100101	[167, 168, 169]
00010111	[170, 171, 172, 173, 174, 175]
01110100	[176, 177, 178, 179, 180]
11100110	[181]
10101100	[182, 183, 184, 185, 186]
00100101	[187, 188, 189, 190, 191, 192]
00010110	[193, 200, 201, 202, 203, 204]
00000110	[194, 195, 196, 197, 198, 199]
00010000	[205, 206]
00000000	[207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]
00001000	[230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242]
00001001	[243, 244, 245, 246, 247, 248, 249, 250, 251]
01001101	[252, 253, 254, 255]

11001000	7	True
10001001	10	False
11001001	18	False
11000001	17	True
10101001	20	True
11101001	31	False
11000101	23	False
10011001	33	False
11011000	36	False
11011001	35	False
10111000	43	True
11010000	51	False
01010001	45	False
01110000	51	False
11111101	63	True
10110000	71	False
11111001	71	False
11110000	73	True
10110001	73	False
11111011	80	True
11110101	82	False
11110011	90	True
11110010	94	True
10110011	115	True
11110111	132	False
10111011	130	True
10110111	144	True
10111111	142	False
10110101	150	True
11110110	153	False
10111110	160	True
10111010	163	True
10100101	170	False
00010111	169	False
01110100	178	True
11100110	181	True
10101100	181	False
00100101	185	False
00010110	198	False
00000110	199	True
00010000	216	False
00000000	222	True
00001000	239	True
00001001	254	False
01001101	253	True

Number of codes used=45


End of hp run 10.  Result of run:
[(-0.9937126031343254, 70000), ('21-05-08_23:17:59BST_NLearn_model_10_Alice_iter70000', '21-05-08_23:17:59BST_NLearn_model_10_Bob_iter70000')]
(-0.9937126031343254, 70000)


>>>> hp_run=11 of 20, time elapsed 3:42:50 of estimated 7:25:41, 
implying ending at 06:43:40BST on Sunday 9 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (406320, 665309, 640372, 353471),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 16,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': 300,
	'N_CODE': 16,
	'N_NUMBERS': 16
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=     20000 training nets give:
alice_loss.item()=0.6234208345413208	bob_loss.item()=0.5196805000305176

0000010010111100	[0, 12, 13, 14, 15]
1110011001011011	[1]
0001011010100101	[2, 3, 4]
1101000000101101	[5, 8]
0101010000010101	[6, 7, 9]
1001100101100011	[10]
1001110101100100	[11]

0000010010111100	8	False
1110011001011011	9	False
0001011010100101	11	False
1101000000101101	8	True
0101010000010101	9	True
1001100101100011	2	False
1001110101100100	1	False

Number of codes used=7

Iteration=     30000 training nets give:
alice_loss.item()=0.09833785146474838	bob_loss.item()=0.24827340245246887

1010100001001010	[0, 14, 15]
0001010010011001	[1, 2, 3]
0110001000010101	[4, 5]
1101000000101101	[6, 7, 8]
0101010000010101	[9, 10]
0111110111001010	[11, 12, 13]

1010100001001010	14	True
0001010010011001	2	True
0110001000010101	4	True
1101000000101101	8	True
0101010000010101	8	False
0111110111001010	13	True

Number of codes used=6

Iteration=     40000 training nets give:
alice_loss.item()=0.2648324966430664	bob_loss.item()=0.08639860153198242

0101010010011001	[0]
0001010010001001	[1]
1010101110110111	[2, 3, 6]
0110001000010101	[4, 5]
1101000000101101	[7, 8]
0101010000010101	[9]
0011110111001010	[10, 11]
0111110111001010	[12]
0110110111001010	[13, 14, 15]

0101010010011001	2	False
0001010010001001	1	True
1010101110110111	12	False
0110001000010101	5	True
1101000000101101	8	True
0101010000010101	8	False
0011110111001010	11	True
0111110111001010	11	False
0110110111001010	12	False

Number of codes used=9

Iteration=     50000 training nets give:
alice_loss.item()=0.41810911893844604	bob_loss.item()=0.1656641960144043

0000101011101111	[0]
0001000010011001	[1]
0001001010010000	[2]
1010101010111100	[3]
0110001000011101	[4]
0110001000010101	[5]
0010001000010101	[6]
1111000000101101	[7]
0110100111001010	[8]
1011100101001101	[9]
0101011111110000	[10, 11]
1111000111101011	[12]
0100011001101110	[13]
1100111100011010	[14]
0110100001011000	[15]

0000101011101111	0	True
0001000010011001	1	True
0001001010010000	2	True
1010101010111100	3	True
0110001000011101	4	True
0110001000010101	5	True
0010001000010101	5	False
1111000000101101	8	False
0110100111001010	12	False
1011100101001101	14	False
0101011111110000	11	True
1111000111101011	12	True
0100011001101110	15	False
1100111100011010	14	True
0110100001011000	15	True

Number of codes used=15

Iteration=     60000 training nets give:
alice_loss.item()=0.1723828911781311	bob_loss.item()=0.0452115461230278

0101000011011110	[0]
0001000010011001	[1]
0001001010010000	[2]
1110001000010101	[3]
0110000100010101	[4]
0110001000110101	[5]
0010110110000100	[6]
1111000000101101	[7]
1101000001101101	[8]
1001000000101101	[9]
1101000000101001	[10]
1000011100010111	[11]
1111000111101011	[12]
1001100100011011	[13]
1010100101001010	[14]
0110100001011000	[15]

0101000011011110	0	True
0001000010011001	1	True
0001001010010000	2	True
1110001000010101	5	False
0110000100010101	4	True
0110001000110101	5	True
0010110110000100	0	False
1111000000101101	7	True
1101000001101101	8	True
1001000000101101	8	False
1101000000101001	8	False
1000011100010111	4	False
1111000111101011	12	True
1001100100011011	13	True
1010100101001010	14	True
0110100001011000	15	True

Number of codes used=16

Iteration=     70000 training nets give:
alice_loss.item()=0.018880682066082954	bob_loss.item()=0.010501353070139885

0101000011011110	[0]
0001000010011001	[1]
0001001010010000	[2]
1000001110100001	[3]
0000000110011011	[4]
0110001000110101	[5, 6]
1111000000101101	[7]
0101010000010101	[8]
0101110000010101	[9]
1101110001111001	[10]
1101011111110000	[11]
0111110011001010	[12]
1001001100110010	[13]
0001010001111011	[14]
0010101011111111	[15]

0101000011011110	0	True
0001000010011001	1	True
0001001010010000	2	True
1000001110100001	3	True
0000000110011011	2	False
0110001000110101	5	True
1111000000101101	7	True
0101010000010101	8	True
0101110000010101	9	True
1101110001111001	10	True
1101011111110000	11	True
0111110011001010	12	True
1001001100110010	13	True
0001010001111011	14	True
0010101011111111	15	True

Number of codes used=15


End of hp run 11.  Result of run:
[(-0.9067826966714765, 70000), ('21-05-08_23:17:59BST_NLearn_model_11_Alice_iter70000', '21-05-08_23:17:59BST_NLearn_model_11_Bob_iter70000')]
(-0.9067826966714765, 70000)


>>>> hp_run=12 of 20, time elapsed 4:07:58 of estimated 7:30:51, 
implying ending at 06:48:50BST on Sunday 9 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (406320, 665309, 640372, 353471),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 16,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': 300,
	'N_CODE': 16,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7705517503711219

Iteration=     20000 training nets give:
alice_loss.item()=0.6440346240997314	bob_loss.item()=0.6440566778182983

1101111001001111	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
0111100110111111	[40, 41, 42, 43]
0101011101100101	[44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57]
0111100110000010	[58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68]
1001110110000111	[69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110]
0101101111011110	[111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
1011111110000100	[128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140]
1111110110111101	[141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189]

1101111001001111	76	False
0111100110111111	51	False
0101011101100101	89	False
0111100110000010	78	False
1001110110000111	63	False
0101101111011110	99	False
1011111110000100	88	False
1111110110111101	68	False

Number of codes used=8

Iteration=     30000 training nets give:
alice_loss.item()=0.1700591742992401	bob_loss.item()=0.1924542933702469

0110000111111011	[0, 1, 2, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
0101111101011011	[3, 4, 5, 6, 7, 8, 9]
0111100110111111	[10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67]
0111100110000010	[68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94]
0101101111011110	[95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129]
1001011011100100	[130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186]

0110000111111011	220	True
0101111101011011	21	False
0111100110111111	42	True
0111100110000010	71	True
0101101111011110	110	True
1001011011100100	159	True

Number of codes used=6

Iteration=     40000 training nets give:
alice_loss.item()=0.30663585662841797	bob_loss.item()=0.20253148674964905

0110000011111011	[0, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
0101111101011011	[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
0111100110111111	[26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]
0110110101000010	[45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75]
0100111111011001	[76, 77, 78, 79]
1000000000011000	[80, 81, 82, 83, 84]
1001011011011101	[85, 86, 87, 88, 89]
0011010110001101	[90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106]
1101010110000100	[107, 108, 109, 110, 111, 112, 113]
0000000000000110	[114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139]
1111010101111101	[140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154]
1001010011100100	[155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165]
0011111001111100	[166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176]
0101101111001110	[177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211]
1101010111001100	[212, 213, 214, 215, 216, 217, 218, 219, 220, 221]
0110000111101011	[222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234]

0110000011111011	224	False
0101111101011011	4	True
0111100110111111	41	True
0110110101000010	40	False
0100111111011001	235	False
1000000000011000	155	False
1001011011011101	154	False
0011010110001101	104	True
1101010110000100	128	False
0000000000000110	77	False
1111010101111101	168	False
1001010011100100	153	False
0011111001111100	160	False
0101101111001110	109	False
1101010111001100	150	False
0110000111101011	226	True

Number of codes used=16

Iteration=     50000 training nets give:
alice_loss.item()=0.4331819713115692	bob_loss.item()=0.3029833734035492

0101111001011011	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 250, 251, 252, 253, 254, 255]
0111100010111111	[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
0111100110111110	[24, 25, 26, 27]
0111100110111111	[28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]
0111100110101111	[43, 44, 45, 46, 47, 48, 49, 50, 51]
0011100110111111	[52, 53, 54, 55, 56]
1010100100010100	[57]
0111100110000000	[58]
1001110110000111	[59, 60, 61, 62, 63, 64, 65, 66, 67]
0111100110000010	[68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]
1101010001100100	[80, 81, 82]
1101101101110110	[83]
0101101111010110	[84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96]
0010011010010100	[97, 98, 99, 100]
0101101111011110	[101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111]
0101101111011100	[112, 113, 114, 115, 116]
0001101111011110	[117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129]
1111101101110110	[130, 131, 132]
1001011010100000	[133, 134, 135, 136, 137, 138, 139, 140, 141]
1001011011100110	[142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152]
0001101000011010	[153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173]
1110000011111011	[174, 175, 176, 177, 178, 179, 180, 181]
1001111011101100	[182, 183, 184, 185]
1100010011111000	[186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208]
1110000111111011	[209, 210, 211, 212, 213, 214, 215]
0110000111111010	[216]
0101010001111001	[217, 218]
0110000111111011	[219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232]
0110000111111001	[233]
0110000111011011	[234, 235, 236, 237, 238, 239, 240, 241, 242, 243]
1111111000110110	[244, 245, 246, 249]
0001100111001111	[247, 248]

0101111001011011	251	True
0111100010111111	18	True
0111100110111110	37	False
0111100110111111	40	True
0111100110101111	30	False
0011100110111111	38	False
1010100100010100	60	False
0111100110000000	76	False
1001110110000111	60	True
0111100110000010	75	True
1101010001100100	162	False
1101101101110110	95	False
0101101111010110	98	False
0010011010010100	111	False
0101101111011110	109	True
0101101111011100	113	True
0001101111011110	114	False
1111101101110110	53	False
1001011010100000	155	False
1001011011100110	160	False
0001101000011010	84	False
1110000011111011	223	False
1001111011101100	159	False
1100010011111000	201	True
1110000111111011	215	True
0110000111111010	217	False
0101010001111001	214	False
0110000111111011	220	True
0110000111111001	219	False
0110000111011011	223	False
1111111000110110	34	False
0001100111001111	23	False

Number of codes used=32

Iteration=     60000 training nets give:
alice_loss.item()=0.1777442991733551	bob_loss.item()=0.12123901396989822

1100000011111000	[0, 1, 2, 3, 4, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 253, 254, 255]
0101111101010011	[5, 6, 7, 8, 9]
0101111111011011	[10, 11]
0101111101001011	[12, 13, 14, 15, 16, 17, 18]
0111100010111111	[19, 20, 21, 22, 23]
0111100110111011	[24, 25, 26, 27, 28, 29, 30, 31, 32]
0111100110111111	[33, 34, 35, 36, 37, 38, 39, 40, 41]
0111101110111111	[42, 43, 44]
1001111101000001	[45, 46]
1001110100000111	[47, 48, 49, 50, 51, 52, 53, 54, 55, 56]
0011100110000010	[57, 58, 59, 60]
1001110110000111	[61, 62, 63, 64, 65, 66, 67, 68]
0111100110000010	[69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82]
0101101111010110	[83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103]
0010011010010100	[104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125]
1111011000011001	[126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137]
1101011010100100	[138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148]
1001011000100100	[149]
1001011011110100	[150, 151, 152, 153, 154, 155, 156, 157, 158, 159]
0011100011010110	[160, 161, 162, 163, 164]
1001011011100100	[165, 166, 167, 168, 169, 170]
0001011011100000	[171, 172, 173, 174]
1111010011010001	[175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193]
1100010011111000	[194, 195, 196, 197, 198, 199, 200, 201, 202, 203]
1001001011000100	[204, 205, 206, 207, 208, 209]
1110000111111011	[210, 211, 212, 213, 214, 215, 216]
0110000111111010	[217, 218, 219, 220, 221, 222, 223]
0110000111111011	[224, 225, 226]
0110010111111011	[227, 228, 229, 230, 231]
0101111001011011	[246, 247, 248, 249, 250, 251, 252]

1100000011111000	202	False
0101111101010011	9	True
0101111111011011	20	False
0101111101001011	10	False
0111100010111111	24	False
0111100110111011	25	True
0111100110111111	32	False
0111101110111111	48	False
1001111101000001	58	False
1001110100000111	57	False
0011100110000010	58	True
1001110110000111	67	True
0111100110000010	71	True
0101101111010110	91	True
0010011010010100	109	True
1111011000011001	135	True
1101011010100100	142	True
1001011000100100	143	False
1001011011110100	156	True
0011100011010110	46	False
1001011011100100	166	True
0001011011100000	168	False
1111010011010001	181	True
1100010011111000	200	True
1001001011000100	160	False
1110000111111011	216	True
0110000111111010	223	True
0110000111111011	220	False
0110010111111011	232	False
0101111001011011	3	False

Number of codes used=30

Iteration=     70000 training nets give:
alice_loss.item()=0.005008631385862827	bob_loss.item()=0.00025678391102701426

0101111001011011	[0, 1, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
0101111101010011	[2, 3, 4, 5, 6, 7, 8, 9, 10]
0101111101001011	[11, 12, 13, 14, 15]
0111100010111111	[16, 17, 18, 19, 20]
0101111111011011	[21, 22, 23, 24, 25, 26, 27]
0111100110111011	[28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]
0111101110111111	[41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
1001110100000111	[52, 53, 54, 55]
0011100110000010	[56, 57, 58, 59, 60, 61, 62]
1001110110000111	[63, 64, 65, 66, 67]
1110110110010100	[68]
0111100110000010	[69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81]
0101101111010110	[82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101]
0101101111011100	[102, 103, 104, 105, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123]
0101101111011110	[106, 107, 108, 109, 110]
1111011000011001	[124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136]
1101011010100100	[137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149]
1011011011100100	[150, 151, 152, 153, 154, 155, 156]
1001010010100100	[157, 158, 159, 160, 161, 162]
1000011011100100	[163, 164]
1001011011100100	[165, 166, 167, 168, 169, 170, 171]
1001001011100000	[172]
0001011011100000	[173]
1111010011010001	[174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192]
1100010011111000	[193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207]
1110000111111011	[208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219]
0110000111111010	[220, 227, 228, 229]
0110000111111011	[221, 222, 223, 224, 225, 226]
0110100111111011	[230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245]

0101111001011011	254	True
0101111101010011	2	True
0101111101001011	8	False
0111100010111111	27	False
0101111111011011	8	False
0111100110111011	33	True
0111101110111111	49	True
1001110100000111	53	True
0011100110000010	60	True
1001110110000111	63	True
1110110110010100	69	False
0111100110000010	74	True
0101101111010110	92	True
0101101111011100	115	True
0101101111011110	103	False
1111011000011001	132	True
1101011010100100	143	True
1011011011100100	152	True
1001010010100100	159	True
1000011011100100	162	False
1001011011100100	161	False
1001001011100000	171	False
0001011011100000	159	False
1111010011010001	183	True
1100010011111000	197	True
1110000111111011	217	True
0110000111111010	224	False
0110000111111011	223	True
0110100111111011	236	True

Number of codes used=29


End of hp run 12.  Result of run:
[(-0.9721060858184872, 70000), ('21-05-08_23:17:59BST_NLearn_model_12_Alice_iter70000', '21-05-08_23:17:59BST_NLearn_model_12_Bob_iter70000')]
(-0.9721060858184872, 70000)


>>>> hp_run=13 of 20, time elapsed 4:33:13 of estimated 7:35:22, 
implying ending at 06:53:21BST on Sunday 9 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (406320, 665309, 640372, 353471),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 16,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': 1000,
	'N_CODE': 8,
	'N_NUMBERS': 16
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=     20000 training nets give:
alice_loss.item()=0.6032907962799072	bob_loss.item()=0.5151190757751465

10111000	[0, 1, 2, 3, 4, 15]
10000000	[5, 6, 7, 8, 9]
10100100	[10, 11, 12, 13, 14]

10111000	13	False
10000000	1	False
10100100	14	True

Number of codes used=3

Iteration=     30000 training nets give:
alice_loss.item()=0.2599981129169464	bob_loss.item()=0.2572340965270996

11100000	[0, 1, 11]
11101100	[2]
11010101	[3, 4, 5, 6]
11100011	[7, 8, 9]
10101010	[10]
10100100	[12, 13, 14, 15]

11100000	14	False
11101100	13	False
11010101	6	True
11100011	7	True
10101010	6	False
10100100	13	True

Number of codes used=6

Iteration=     40000 training nets give:
alice_loss.item()=0.15015746653079987	bob_loss.item()=0.17774267494678497

11101000	[0, 1, 2]
10000000	[3]
11011101	[4]
11011001	[5, 6, 7]
10100011	[8]
11100111	[9]
00100011	[10]
00100100	[11, 12]
10100100	[13]
01100000	[14]
11100000	[15]

11101000	1	True
10000000	8	False
11011101	5	False
11011001	5	True
10100011	8	True
11100111	8	False
00100011	8	False
00100100	13	False
10100100	13	True
01100000	15	False
11100000	0	False

Number of codes used=11

Iteration=     50000 training nets give:
alice_loss.item()=0.23045921325683594	bob_loss.item()=0.39402222633361816

11001101	[0, 1]
11111000	[2]
11010100	[3]
01011101	[4]
11011001	[5]
11011111	[6]
11110111	[7]
11100111	[8]
10000011	[9]
10100001	[10]
00000100	[11, 12]
10100100	[13]
01100101	[14]
10110100	[15]

11001101	4	False
11111000	1	False
11010100	3	True
01011101	4	True
11011001	5	True
11011111	5	False
11110111	7	True
11100111	8	True
10000011	9	True
10100001	10	True
00000100	12	True
10100100	13	True
01100101	11	False
10110100	14	False

Number of codes used=14

Iteration=     60000 training nets give:
alice_loss.item()=0.017576158046722412	bob_loss.item()=0.032635293900966644

01101100	[0]
11101100	[1]
11111000	[2]
11010100	[3]
11011100	[4]
11011011	[5]
11101011	[6]
10101011	[7]
00101011	[8]
10000011	[9]
10100001	[10]
00100101	[11]
10000100	[12]
10100100	[13]
10101101	[14]
01101001	[15]

01101100	15	False
11101100	1	True
11111000	1	False
11010100	3	True
11011100	4	True
11011011	5	True
11101011	6	True
10101011	7	True
00101011	8	True
10000011	8	False
10100001	10	True
00100101	11	True
10000100	12	True
10100100	12	False
10101101	14	True
01101001	15	True

Number of codes used=16

Iteration=     70000 training nets give:
alice_loss.item()=0.00872056558728218	bob_loss.item()=0.01232275553047657

11101010	[0]
11001000	[1]
01010100	[2]
11010100	[3]
01011101	[4]
11011011	[5]
10010000	[6]
11110111	[7]
00101011	[8]
00110111	[9]
10100001	[10]
00100101	[11]
10100101	[12]
10000110	[13]
00101000	[14]
01101001	[15]

11101010	0	True
11001000	1	True
01010100	3	False
11010100	3	True
01011101	4	True
11011011	5	True
10010000	6	True
11110111	7	True
00101011	8	True
00110111	9	True
10100001	10	True
00100101	11	True
10100101	13	False
10000110	13	True
00101000	14	True
01101001	15	True

Number of codes used=16


End of hp run 13.  Result of run:
[(-0.9019955123351093, 60000), ('21-05-08_23:17:59BST_NLearn_model_13_Alice_iter70000', '21-05-08_23:17:59BST_NLearn_model_13_Bob_iter70000')]
(-0.9019955123351093, 60000)


>>>> hp_run=14 of 20, time elapsed 4:53:37 of estimated 7:31:43, 
implying ending at 06:49:42BST on Sunday 9 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (406320, 665309, 640372, 353471),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 16,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': 1000,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7705517503711219

Iteration=     20000 training nets give:
alice_loss.item()=0.6400509476661682	bob_loss.item()=0.6341392397880554

01100111	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 251, 252, 253, 254, 255]
01010000	[13, 14, 15, 16, 17]
10101001	[28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]
01010100	[65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210]
10011100	[211, 212, 213, 214, 215, 216, 217, 218]
01111000	[219, 220, 221, 222, 245, 246, 247, 248, 249, 250]

01100111	249	False
01010000	224	False
10101001	4	False
01010100	68	True
10011100	9	False
01111000	147	False

Number of codes used=6

Iteration=     30000 training nets give:
alice_loss.item()=0.328612744808197	bob_loss.item()=0.271243155002594

01100111	[0, 1, 2, 3, 4, 5, 6, 7, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10101001	[8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58]
10010001	[59, 60]
00000000	[61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]
01010100	[73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96]
10110110	[97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121]
01000101	[122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168]
10010010	[169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205]

01100111	231	True
10101001	40	True
10010001	55	False
00000000	91	False
01010100	109	False
10110110	121	True
01000101	145	True
10010010	192	True

Number of codes used=8

Iteration=     40000 training nets give:
alice_loss.item()=0.21925026178359985	bob_loss.item()=0.1985090970993042

00000001	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 59, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10111001	[53]
00001000	[60, 61, 62, 63, 64, 65, 66, 67, 68, 69]
00000000	[70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85]
10000000	[86, 87, 88, 89, 90]
00100000	[91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103]
11110110	[104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
10110110	[114, 115]
11000101	[128, 129, 130, 131]
01001101	[132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161]
01010101	[162]
01000000	[163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174]
10010010	[175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202]
01100110	[203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219]
01100111	[220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238]

00000001	90	False
10111001	43	False
00001000	69	True
00000000	67	False
10000000	85	False
00100000	72	False
11110110	112	True
10110110	120	False
11000101	146	False
01001101	153	True
01010101	144	False
01000000	91	False
10010010	190	True
01100110	215	True
01100111	227	True

Number of codes used=15

Iteration=     50000 training nets give:
alice_loss.item()=0.2283470630645752	bob_loss.item()=0.22716718912124634

01100001	[0, 1, 2, 249, 250, 251, 252, 253, 254, 255]
00101011	[3, 4, 5, 6, 7, 8]
01101001	[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
10101011	[24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
10101001	[36, 37, 38, 39, 40]
10111001	[41, 42, 43, 44, 45]
10010001	[46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
00001000	[62, 63, 64, 65]
10101101	[66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84]
10000000	[85, 86, 87, 88]
01000000	[89, 90, 91, 92, 93]
10000101	[94, 95, 96, 97, 98]
01010100	[99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109]
00000100	[110, 111, 112]
00110110	[113, 114, 115, 116, 117]
01110110	[118, 119, 120, 121]
01001010	[122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136]
11111101	[137]
01010101	[138, 139, 140, 141, 142, 143, 144, 145, 146, 147]
01000101	[148, 149, 157]
01001101	[150, 151, 152, 153, 154, 155, 156]
10010011	[158, 159, 160, 161, 162]
10010110	[163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176]
10010010	[177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192]
10011010	[193]
10010000	[194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206]
01100110	[207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218]
01100111	[219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235]
00100111	[236, 237, 238, 239]
01100011	[240, 241, 242, 243, 244, 245, 246, 247, 248]

01100001	249	True
00101011	17	False
01101001	14	True
10101011	19	False
10101001	33	False
10111001	42	True
10010001	54	True
00001000	65	True
10101101	43	False
10000000	81	False
01000000	81	False
10000101	108	False
01010100	99	True
00000100	101	False
00110110	120	False
01110110	122	False
01001010	125	True
11111101	115	False
01010101	142	True
01000101	151	False
01001101	151	True
10010011	188	False
10010110	169	True
10010010	187	True
10011010	189	False
10010000	206	True
01100110	214	True
01100111	235	True
00100111	243	False
01100011	247	True

Number of codes used=30

Iteration=     60000 training nets give:
alice_loss.item()=0.08519172668457031	bob_loss.item()=0.08518459647893906

00111011	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 253, 254, 255]
01101001	[18, 19, 20]
10101011	[21, 22, 23, 24, 25, 26, 27]
10101001	[28, 29, 30, 31, 32, 33, 34, 35, 36, 37]
10111001	[38, 39, 40, 42, 43, 44]
10011001	[41, 45, 46, 47, 48]
10100001	[49, 50]
10010001	[51]
10001000	[52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]
00001000	[65, 66, 67, 68]
00000000	[69, 70, 71, 72, 73, 74, 75, 76, 77]
10000000	[78, 79, 80, 81, 82, 83, 84, 85, 86]
01000000	[87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97]
00000100	[98]
00110010	[99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112]
11110110	[113, 114, 115]
10100110	[116, 117, 118, 119]
10110110	[120]
11110101	[121, 122, 123, 124, 125]
10100010	[126, 127, 128, 129]
01111110	[130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140]
01010101	[141, 142, 143, 144, 145, 146, 147]
01001101	[148, 149, 150, 151, 152]
01000101	[153, 154, 155, 156, 157, 158, 159, 160, 161]
10010110	[162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174]
00011010	[175]
11010010	[176, 177, 178]
10000010	[179, 180, 181, 182, 183]
10010010	[184, 185, 186, 187, 188, 189, 190]
10011010	[191, 192, 193, 194, 195, 196, 197, 198, 199]
01010010	[200, 201, 202, 204, 205, 206, 207]
10010000	[203]
10011011	[208, 209, 210, 211, 212, 213]
01100110	[214, 215, 216, 217, 218]
10011000	[219, 220, 221, 222, 223, 224, 225]
01100111	[226, 227, 228, 229, 230, 231]
01101111	[232, 233, 234, 235, 236, 237]
01110011	[238, 239, 240, 241, 242, 243]
01110001	[244, 245, 246, 247, 248, 249, 250, 251, 252]

00111011	20	False
01101001	13	False
10101011	27	True
10101001	33	True
10111001	41	False
10011001	42	False
10100001	49	True
10010001	54	False
10001000	64	True
00001000	65	True
00000000	83	False
10000000	84	True
01000000	91	True
00000100	100	False
00110010	118	False
11110110	118	False
10100110	122	False
10110110	120	True
11110101	118	False
10100010	115	False
01111110	214	False
01010101	139	False
01001101	151	True
01000101	150	False
10010110	170	True
00011010	181	False
11010010	178	True
10000010	183	True
10010010	188	True
10011010	194	True
01010010	199	False
10010000	200	False
10011011	217	False
01100110	210	False
10011000	220	True
01100111	229	True
01101111	242	False
01110011	240	True
01110001	248	True

Number of codes used=39

Iteration=     70000 training nets give:
alice_loss.item()=0.0011446765856817365	bob_loss.item()=0.0006456998526118696

01101101	[0, 254, 255]
10100011	[1, 2, 3, 4, 5]
00100101	[6, 7, 8]
01101001	[9, 10, 11, 12, 13, 14, 15, 16, 17, 18]
10101011	[19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
10101001	[30, 31, 32, 33, 34, 35, 36, 37, 38]
10111001	[39, 40, 41]
10011001	[42, 43, 44, 45]
10101101	[46, 47, 48, 49]
10010001	[50, 51, 52, 53, 54, 55]
00011000	[56, 57, 58, 59, 60, 61, 62]
00001000	[63, 64, 65, 66]
10111101	[67, 68]
10000001	[69, 70, 71, 72]
00000000	[73, 74, 75, 76, 77, 78, 79, 80, 81]
00010000	[82, 83]
10000000	[84, 85, 86]
01000000	[87, 88, 89, 90, 91, 92, 93, 94, 95]
00010100	[96, 97, 98, 99, 100]
00000100	[101, 102, 103, 104]
11100100	[105, 106]
00110100	[107, 108]
11110000	[109, 110, 111, 112]
11110110	[113, 114, 115, 116, 117, 118, 119, 120, 121]
01000100	[122, 123]
10100110	[124, 125, 126]
01001010	[127, 128, 129, 130, 131, 132, 133, 134]
10110010	[135, 136, 137, 138, 139]
01010101	[140, 141, 142, 143, 144, 145]
11000010	[146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157]
11001010	[158, 159, 160, 161]
10010110	[162, 163, 164, 165, 166, 167, 168]
10111010	[169, 170, 171, 172, 173, 174]
00011010	[175, 176, 177, 178, 179, 180, 181]
10000010	[182, 183]
10010010	[184, 185, 186, 187, 188, 189, 190, 191, 192]
10011010	[193, 194, 195, 196]
01010010	[197, 198, 199]
10010000	[200, 201, 202, 203, 204, 205]
01000111	[206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217]
10011011	[218, 219, 220]
10011000	[221]
01101110	[222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232]
11100111	[233, 234, 235, 236, 237, 238, 239, 240, 241, 242]
01110011	[243, 244]
01110001	[245, 246, 247]
01100101	[248, 249, 250, 251]
01100001	[252, 253]

01101101	255	True
10100011	253	False
00100101	5	False
01101001	14	True
10101011	23	True
10101001	42	False
10111001	30	False
10011001	38	False
10101101	55	False
10010001	51	True
00011000	58	True
00001000	62	False
10111101	67	True
10000001	67	False
00000000	74	True
00010000	78	False
10000000	80	False
01000000	91	True
00010100	97	True
00000100	98	False
11100100	105	True
00110100	110	False
11110000	109	True
11110110	120	True
01000100	115	False
10100110	122	False
01001010	131	True
10110010	131	False
01010101	140	True
11000010	134	False
11001010	145	False
10010110	168	True
10111010	170	True
00011010	178	True
10000010	183	True
10010010	181	False
10011010	192	False
01010010	195	False
10010000	202	True
01000111	208	True
10011011	216	False
10011000	223	False
01101110	223	True
11100111	233	True
01110011	237	False
01110001	243	False
01100101	249	True
01100001	248	False

Number of codes used=48


End of hp run 14.  Result of run:
[(-0.962673861260545, 70000), ('21-05-08_23:17:59BST_NLearn_model_14_Alice_iter70000', '21-05-08_23:17:59BST_NLearn_model_14_Bob_iter70000')]
(-0.962673861260545, 70000)


>>>> hp_run=15 of 20, time elapsed 5:14:10 of estimated 7:28:49, 
implying ending at 06:46:48BST on Sunday 9 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (406320, 665309, 640372, 353471),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 16,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': 1000,
	'N_CODE': 16,
	'N_NUMBERS': 16
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=     20000 training nets give:
alice_loss.item()=0.7706488370895386	bob_loss.item()=0.5881568193435669

0000010111100101	[0, 1, 3, 4, 15]
0100111000000101	[2]
0101011110110101	[5, 6]
0001000101011011	[7]
0001010010111111	[8]
1101101001001010	[9]
1001101000010100	[10, 11]
0001010011011011	[12]
0010000100010001	[13, 14]

0000010111100101	2	False
0100111000000101	3	False
0101011110110101	1	False
0001000101011011	1	False
0001010010111111	15	False
1101101001001010	2	False
1001101000010100	3	False
0001010011011011	2	False
0010000100010001	1	False

Number of codes used=9

Iteration=     30000 training nets give:
alice_loss.item()=0.14474910497665405	bob_loss.item()=0.2106977105140686

0000010111100101	[0, 1, 2]
0100111000000101	[3, 4]
1100110000101001	[5, 6, 7, 8, 9, 10, 11, 12]
1010101111110001	[13, 14, 15]

0000010111100101	1	True
0100111000000101	2	False
1100110000101001	8	True
1010101111110001	14	True

Number of codes used=4

Iteration=     40000 training nets give:
alice_loss.item()=0.23749078810214996	bob_loss.item()=0.06696044653654099

1010101110110001	[0]
0101011110110101	[1, 2, 3]
0000110110001010	[4]
0000010101100101	[5]
0000111000000101	[6, 7]
0100110000101001	[8]
1011111101100000	[9]
1011111111100000	[10]
1010111111110001	[11, 12]
1010101111110001	[13, 14]
1010101011110001	[15]

1010101110110001	15	False
0101011110110101	6	False
0000110110001010	9	False
0000010101100101	2	False
0000111000000101	2	False
0100110000101001	7	False
1011111101100000	11	False
1011111111100000	12	False
1010111111110001	13	False
1010101111110001	14	True
1010101011110001	15	True

Number of codes used=11

Iteration=     50000 training nets give:
alice_loss.item()=0.4294828176498413	bob_loss.item()=0.21532991528511047

1010000100100111	[0, 1]
1111100011110111	[2]
0100111000100101	[3]
1101111110110111	[4]
0101010001101110	[5]
1101011011001110	[6, 7]
1100110000101001	[8]
1110111100000100	[9]
1011111111101000	[10]
0011100010110110	[11]
0100101101110010	[12]
1100010111110000	[13]
0000101000000101	[14]
0111100001000111	[15]

1010000100100111	1	True
1111100011110111	12	False
0100111000100101	3	True
1101111110110111	2	False
0101010001101110	9	False
1101011011001110	9	False
1100110000101001	8	True
1110111100000100	2	False
1011111111101000	10	True
0011100010110110	2	False
0100101101110010	10	False
1100010111110000	1	False
0000101000000101	3	False
0111100001000111	3	False

Number of codes used=14

Iteration=     60000 training nets give:
alice_loss.item()=0.17362436652183533	bob_loss.item()=0.07745713740587234

0011100010101101	[0]
1010000101000101	[1]
1000000000011111	[2, 3, 4, 5]
1100110110101001	[6]
1000110000101001	[7]
1011111101100001	[8]
1100110000110001	[9]
0000000000101101	[10]
1110101111110101	[11, 12]
1101010100011100	[13]
0111101100100011	[14]
1010101011110011	[15]

0011100010101101	9	False
1010000101000101	2	False
1000000000011111	3	True
1100110110101001	7	False
1000110000101001	7	True
1011111101100001	8	True
1100110000110001	7	False
0000000000101101	7	False
1110101111110101	13	False
1101010100011100	3	False
0111101100100011	11	False
1010101011110011	12	False

Number of codes used=12

Iteration=     70000 training nets give:
alice_loss.item()=0.053394731134176254	bob_loss.item()=0.008857008069753647

0010001010100011	[0, 1]
1101101111101101	[2]
0000010111111101	[3]
1000000000011111	[4]
0000100101011100	[5]
1100110110101001	[6]
1000110000101001	[7]
1100110000101000	[8]
0111101100101000	[9]
1011111111101000	[10]
1000101001100100	[11]
1110111011110010	[12]
1011101111110001	[13]
1010001110110001	[14]
1110101111110011	[15]

0010001010100011	14	False
1101101111101101	1	False
0000010111111101	0	False
1000000000011111	3	False
0000100101011100	1	False
1100110110101001	8	False
1000110000101001	7	True
1100110000101000	8	True
0111101100101000	11	False
1011111111101000	11	False
1000101001100100	14	False
1110111011110010	11	False
1011101111110001	13	True
1010001110110001	14	True
1110101111110011	13	False

Number of codes used=15


End of hp run 15.  Result of run:
[(-0.6544203066003292, 40000), ('21-05-08_23:17:59BST_NLearn_model_15_Alice_iter70000', '21-05-08_23:17:59BST_NLearn_model_15_Bob_iter70000')]
(-0.6544203066003292, 40000)


>>>> hp_run=16 of 20, time elapsed 5:39:24 of estimated 7:32:32, 
implying ending at 06:50:31BST on Sunday 9 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (406320, 665309, 640372, 353471),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 16,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': 1000,
	'N_CODE': 16,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7705517503711219

Iteration=     20000 training nets give:
alice_loss.item()=0.7327718734741211	bob_loss.item()=0.6564868092536926

0111100000010000	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
1111001001111001	[47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75]
1000001010111101	[104, 105, 106, 107, 108, 109, 110, 111, 112]
0000111010111000	[113, 114, 115, 116, 117]
0111101110110111	[118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130]
1111001010101001	[131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160]
0010000010001011	[161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203]
0101001000100010	[204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214]
1110011010110100	[215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227]

0111100000010000	244	True
1111001001111001	7	False
1000001010111101	112	True
0000111010111000	33	False
0111101110110111	9	False
1111001010101001	174	False
0010000010001011	56	False
0101001000100010	210	True
1110011010110100	12	False

Number of codes used=9

Iteration=     30000 training nets give:
alice_loss.item()=0.27449798583984375	bob_loss.item()=0.24348463118076324

0111100000010000	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
0100110001000000	[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]
1000001010111101	[73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134]
1111001010101001	[135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181]
0101001000100010	[182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232]

0111100000010000	15	False
0100110001000000	45	True
1000001010111101	99	True
1111001010101001	157	True
0101001000100010	216	True

Number of codes used=5

Iteration=     40000 training nets give:
alice_loss.item()=0.29030683636665344	bob_loss.item()=0.05207047611474991

1110000101001111	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
1100110001000000	[15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]
1000001000011111	[34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]
1101010000010101	[50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73]
0110010000010001	[74, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174]
1000100011111110	[75, 76, 77, 78, 79, 80, 81, 82, 83]
0110011010100011	[84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184]
0001011011000101	[110, 111, 112, 113, 114, 115, 116, 117, 118]
1101011001011111	[145, 146, 147, 148]
1000001011111100	[185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223]
1111011110010011	[224, 225, 226, 227, 228, 229, 230, 231, 232, 233]

1110000101001111	44	False
1100110001000000	55	False
1000001000011111	85	False
1101010000010101	36	False
0110010000010001	28	False
1000100011111110	79	True
0110011010100011	184	True
0001011011000101	61	False
1101011001011111	26	False
1000001011111100	109	False
1111011110010011	192	False

Number of codes used=11

Iteration=     50000 training nets give:
alice_loss.item()=0.37345871329307556	bob_loss.item()=0.23573267459869385

1111001000101101	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 249, 250, 251, 252, 253, 254, 255]
0111100000010000	[10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
0111000000010000	[20, 21, 22, 23]
0110000011001110	[24, 25, 26, 27, 28, 29, 30]
0010111010100000	[31, 32, 33, 34, 35, 36]
0010001111000010	[37, 38, 39]
0011100111000100	[40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56]
0100011011010000	[57, 58, 59, 60, 61, 62, 63]
0100110001000001	[64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87]
1000111101101000	[88, 89, 90, 91, 92, 93]
0011100000010010	[94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118]
1111100101111100	[119, 120, 121, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146]
0101010010111001	[122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 147, 148, 149]
1111001010101001	[150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164]
1010101111011000	[165, 166, 167, 168, 169, 170, 171, 172]
0001101010100001	[173, 174, 175, 176, 177, 178, 179, 180, 181, 182]
1000001011101101	[183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196]
0111001000100010	[197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207]
0101001000100010	[208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224]
0100011110000111	[225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235]
0101001000101010	[236, 237, 238, 239, 240, 241, 242]
1010101011011111	[243, 244]
0111100000000000	[245, 246, 247, 248]

1111001000101101	126	False
0111100000010000	6	False
0111000000010000	8	False
0110000011001110	45	False
0010111010100000	73	False
0010001111000010	64	False
0011100111000100	58	False
0100011011010000	66	False
0100110001000001	54	False
1000111101101000	58	False
0011100000010010	6	False
1111100101111100	39	False
0101010010111001	80	False
1111001010101001	158	True
1010101111011000	145	False
0001101010100001	174	True
1000001011101101	118	False
0111001000100010	216	False
0101001000100010	212	True
0100011110000111	57	False
0101001000101010	212	False
1010101011011111	102	False
0111100000000000	8	False

Number of codes used=23

Iteration=     60000 training nets give:
alice_loss.item()=0.19439901411533356	bob_loss.item()=0.1545506715774536

0111100010010000	[0, 1, 2, 3, 249, 250, 251, 252, 253, 254, 255]
0111100000010000	[4, 5, 6, 7, 8, 9, 10, 11, 12]
0100010011010000	[13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]
0100011011010000	[42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]
0000110001010000	[54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65]
1111011111100101	[66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88]
1100001010111101	[89, 90]
1000001010111101	[91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107]
1000001010111001	[108, 109, 110, 111]
1000001010101101	[112, 113, 114, 115, 116, 117, 118, 119, 120, 121]
1000000100101111	[122, 123, 124, 125, 126, 127, 128, 129, 130]
1010101010111101	[131, 132]
0010100011110101	[133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148]
1111001010101001	[149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162]
1111001110101001	[163]
1111010000000001	[164, 165, 166, 167]
0101101110101001	[168, 169, 170, 171, 172]
0001001000101000	[173]
1100000010111101	[174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196]
1111111011101110	[197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214]
0101001000100010	[215, 216, 217, 218, 219]
1101111100101100	[220]
0101011000100010	[221]
0101001000000010	[222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248]

0111100010010000	250	True
0111100000010000	14	False
0100010011010000	48	False
0100011011010000	61	False
0000110001010000	63	True
1111011111100101	89	False
1100001010111101	97	False
1000001010111101	92	True
1000001010111001	111	True
1000001010101101	113	True
1000000100101111	173	False
1010101010111101	91	False
0010100011110101	145	True
1111001010101001	151	True
1111001110101001	150	False
1111010000000001	212	False
0101101110101001	190	False
0001001000101000	183	False
1100000010111101	92	False
1111111011101110	153	False
0101001000100010	225	False
1101111100101100	220	True
0101011000100010	217	False
0101001000000010	239	True

Number of codes used=24

Iteration=     70000 training nets give:
alice_loss.item()=0.042080800980329514	bob_loss.item()=0.012167765758931637

0111100010010000	[0, 1, 2, 3, 4, 5, 249, 250, 251, 252, 253, 254, 255]
0111100000010000	[6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
0011100000010000	[17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
0110101010000111	[32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]
0100110001100000	[43, 44, 45]
1100001111001111	[46, 47, 48, 49, 50, 51, 52]
0100110001000000	[53, 54, 55]
0000110001010000	[56, 57, 58, 59, 60, 61, 62]
0011111101001110	[63, 64, 65, 66, 67, 68, 69]
0100100011011111	[70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88]
1111011111100101	[89, 90, 91, 92]
1000001010111101	[93, 94, 95, 96, 97, 98, 99, 100]
1100001010111101	[101, 102, 103]
1000101010111101	[104, 105, 106, 107, 108, 109, 110, 111, 112]
1000001010101101	[113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124]
0010100011110101	[125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135]
1101101010101001	[136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160]
1110100100010011	[161, 162, 163, 164]
0011000111110100	[165, 166, 167, 168, 169]
1011001100010001	[170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181]
0110110110011101	[182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198]
0111011000110110	[199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212]
0101001000100010	[213, 214, 215, 216]
0101011000100010	[217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227]
0101001000000010	[228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248]

0111100010010000	251	True
0111100000010000	13	True
0011100000010000	22	True
0110101010000111	36	True
0100110001100000	54	False
1100001111001111	51	True
0100110001000000	47	False
0000110001010000	59	True
0011111101001110	68	True
0100100011011111	79	True
1111011111100101	89	True
1000001010111101	93	True
1100001010111101	102	True
1000101010111101	116	False
1000001010101101	121	True
0010100011110101	138	False
1101101010101001	178	False
1110100100010011	207	False
0011000111110100	168	True
1011001100010001	175	True
0110110110011101	193	True
0111011000110110	212	True
0101001000100010	217	False
0101011000100010	223	True
0101001000000010	237	True

Number of codes used=25


End of hp run 16.  Result of run:
[(-0.8467898093524909, 70000), ('21-05-08_23:17:59BST_NLearn_model_16_Alice_iter70000', '21-05-08_23:17:59BST_NLearn_model_16_Bob_iter70000')]
(-0.8467898093524909, 70000)


>>>> hp_run=17 of 20, time elapsed 6:04:30 of estimated 7:35:37, 
implying ending at 06:53:36BST on Sunday 9 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (406320, 665309, 640372, 353471),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 16,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': 3000,
	'N_CODE': 8,
	'N_NUMBERS': 16
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=     20000 training nets give:
alice_loss.item()=0.6285269260406494	bob_loss.item()=0.4971826374530792

00100010	[0, 1, 7, 8, 9, 10]
01011011	[2, 3, 4, 5, 6]
10011010	[11, 12, 13]
00111000	[14, 15]

00100010	4	False
01011011	5	True
10011010	5	False
00111000	14	True

Number of codes used=4

Iteration=     30000 training nets give:
alice_loss.item()=0.2190813422203064	bob_loss.item()=0.19373023509979248

00111000	[0, 1, 12, 13, 14, 15]
01011011	[2, 3, 4, 5, 6, 7]
00100010	[8, 9, 10, 11]

00111000	14	True
01011011	4	True
00100010	8	True

Number of codes used=3

Iteration=     40000 training nets give:
alice_loss.item()=0.11681374907493591	bob_loss.item()=0.09069530665874481

01111000	[0, 1]
01011111	[2]
10100010	[3, 5, 10, 11]
11011011	[4]
01100010	[6, 7]
00100110	[8]
00100010	[9]
00011000	[12]
00111010	[13]
00111000	[14, 15]

01111000	15	False
01011111	4	False
10100010	8	False
11011011	4	True
01100010	8	False
00100110	8	True
00100010	8	False
00011000	14	False
00111010	14	False
00111000	15	True

Number of codes used=10

Iteration=     50000 training nets give:
alice_loss.item()=0.05941078066825867	bob_loss.item()=0.20096686482429504

00110001	[0]
11110011	[1]
10010000	[2]
11000011	[3]
01111011	[4]
00000111	[5]
00110011	[6]
00100000	[7]
01000110	[8, 9]
00000011	[10]
10111100	[11]
01111100	[12]
00111001	[13]
10110000	[14]
01101000	[15]

00110001	14	False
11110011	4	False
10010000	13	False
11000011	4	False
01111011	4	True
00000111	7	False
00110011	7	False
00100000	8	False
01000110	8	True
00000011	8	False
10111100	15	False
01111100	15	False
00111001	15	False
10110000	14	True
01101000	15	True

Number of codes used=15

Iteration=     60000 training nets give:
alice_loss.item()=0.058581411838531494	bob_loss.item()=0.11528608947992325

00101000	[0]
10001000	[1, 15]
11101001	[2, 3]
01010011	[4]
11111001	[5]
11001001	[6, 7, 8]
00100110	[9, 10]
00011010	[11, 12, 13]
01101001	[14]

00101000	15	False
10001000	14	False
11101001	3	True
01010011	4	True
11111001	4	False
11001001	4	False
00100110	9	True
00011010	13	True
01101001	0	False

Number of codes used=9

Iteration=     70000 training nets give:
alice_loss.item()=0.030219335108995438	bob_loss.item()=0.0207664854824543

00101000	[0]
01111110	[1]
11101001	[2, 3]
01110011	[4, 5]
01100011	[6, 7]
00100100	[8, 9]
00100110	[10]
10110010	[11]
00011010	[12]
10011000	[13]
01011100	[14, 15]

00101000	15	False
01111110	14	False
11101001	3	True
01110011	4	True
01100011	4	False
00100100	9	True
00100110	9	False
10110010	13	False
00011010	13	False
10011000	13	True
01011100	15	True

Number of codes used=11


End of hp run 17.  Result of run:
[(-0.8104166501959852, 70000), ('21-05-08_23:17:59BST_NLearn_model_17_Alice_iter70000', '21-05-08_23:17:59BST_NLearn_model_17_Bob_iter70000')]
(-0.8104166501959852, 70000)


>>>> hp_run=18 of 20, time elapsed 6:24:44 of estimated 7:32:38, 
implying ending at 06:50:37BST on Sunday 9 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (406320, 665309, 640372, 353471),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 16,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': 3000,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7705517503711219

Iteration=     20000 training nets give:
alice_loss.item()=0.6725000143051147	bob_loss.item()=0.5930298566818237

00110110	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11011101	[122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148]
11111111	[149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164]
00111001	[165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227]
10110000	[181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201]

00110110	205	False
11011101	52	False
11111111	75	False
00111001	104	False
10110000	235	False

Number of codes used=5

Iteration=     30000 training nets give:
alice_loss.item()=0.3718864917755127	bob_loss.item()=0.23555554449558258

10110000	[0, 1, 2, 3, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10111000	[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]
01010001	[29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]
00001001	[41, 42, 43, 44, 45, 46, 47, 48]
11111011	[49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
01110101	[64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]
10011111	[80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113]
10110011	[114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162]
00010111	[163, 164, 165, 166, 167, 168]

10110000	236	True
10111000	14	True
01010001	77	False
00001001	72	False
11111011	157	False
01110101	9	False
10011111	137	False
10110011	111	False
00010111	29	False

Number of codes used=9

Iteration=     40000 training nets give:
alice_loss.item()=0.22968995571136475	bob_loss.item()=0.2985476851463318

10110100	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 253, 254, 255]
10101000	[13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
00001001	[48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
00001000	[62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111]
10100011	[112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147]
10110111	[148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168]
00101001	[169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194]
10110001	[195, 196, 197, 198, 199, 200]
10110000	[201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232]
10110010	[233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252]

10110100	234	False
10101000	22	True
00001001	66	False
00001000	62	True
10100011	125	True
10110111	142	False
00101001	63	False
10110001	198	True
10110000	236	False
10110010	233	True

Number of codes used=10

Iteration=     50000 training nets give:
alice_loss.item()=0.11421143263578415	bob_loss.item()=0.32707107067108154

10101100	[0, 215, 216, 217, 218, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11101000	[1, 2, 3]
00110110	[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]
00111000	[29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43]
01001000	[44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55]
00101000	[56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68]
00101001	[69, 70, 71, 72, 73, 74, 75]
00001101	[76, 77, 78, 79, 80, 81]
00001011	[82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118]
10110011	[119, 120, 121, 122, 123, 124, 125, 126]
10001111	[127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137]
10100111	[138, 139]
10110111	[140, 141, 142, 143, 144]
00111001	[145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163]
10110101	[164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183]
10110001	[184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214]
10110000	[219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239]

10101100	19	False
11101000	17	False
00110110	243	False
00111000	34	True
01001000	56	False
00101000	63	True
00101001	118	False
00001101	78	True
00001011	74	False
10110011	121	True
10001111	94	False
10100111	147	False
10110111	151	False
00111001	137	False
10110101	176	True
10110001	200	True
10110000	221	True

Number of codes used=17

Iteration=     60000 training nets give:
alice_loss.item()=0.07978445291519165	bob_loss.item()=0.14689965546131134

11111000	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 253, 254, 255]
10111010	[10, 11, 12, 13]
10111100	[14, 15, 16, 17, 18]
10101001	[19, 20, 21, 22, 23]
00111100	[24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]
00111000	[35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]
00101000	[50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67]
00001001	[68, 69, 70, 71]
00011001	[72, 73, 74, 75, 76]
11001001	[77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87]
10001011	[88, 89]
11000011	[90]
10111011	[91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
10011011	[115, 116]
10100110	[117, 118, 119, 120, 121, 122]
00110111	[123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133]
10100101	[134, 135, 136, 137, 138, 139, 140, 141, 142]
10100111	[143, 144, 145, 146, 147]
10110111	[148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162]
11110010	[163]
10110101	[164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182]
10100001	[183, 184, 185, 186]
10110001	[187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204]
10010010	[205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216]
10100010	[217, 218]
10110010	[219, 220, 221, 222, 223, 224]
11110000	[225, 226, 227, 228, 229, 230, 231, 232, 233, 234]
10100000	[235, 236, 237, 238, 239, 240, 241, 242]
01110111	[243, 244, 245, 246, 247, 248, 249, 250, 251, 252]

11111000	6	True
10111010	12	True
10111100	20	False
10101001	26	False
00111100	32	True
00111000	41	True
00101000	61	True
00001001	73	False
00011001	79	False
11001001	69	False
10001011	96	False
11000011	98	False
10111011	116	False
10011011	117	False
10100110	226	False
00110111	144	False
10100101	185	False
10100111	143	True
10110111	151	True
11110010	120	False
10110101	174	True
10100001	188	False
10110001	197	True
10010010	207	True
10100010	225	False
10110010	228	False
11110000	242	False
10100000	237	True
01110111	130	False

Number of codes used=29

Iteration=     70000 training nets give:
alice_loss.item()=0.025065477937459946	bob_loss.item()=0.005645913537591696

00010100	[0, 1, 252, 253, 254, 255]
11111000	[2, 3, 4, 5, 6]
10111010	[7, 8, 9, 10, 11]
00111010	[12, 13, 14, 15, 16]
10011100	[17, 18, 19, 20, 21, 22, 23, 24, 25, 26]
10101001	[27]
00111100	[28, 29, 30, 31, 32, 33]
00111000	[34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
00001100	[48]
00101000	[49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
00001001	[62, 63, 64, 65, 66, 67, 68, 69, 70]
11011001	[71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86]
00001101	[80]
01111001	[87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98]
11000011	[99, 100, 101, 102, 103, 104, 105, 106]
10011011	[107, 108, 109, 110]
10101011	[111, 112, 113, 114, 115]
11110011	[116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126]
01111101	[127, 128, 129]
10110011	[130, 131, 132, 133, 134, 135, 136]
00111001	[137, 138, 139, 140, 141, 142, 143]
10100111	[144, 145, 146, 147, 148, 149]
10110111	[150, 151, 152, 153, 154, 155]
10010101	[156, 157, 158, 159, 160, 161, 162, 163, 164, 165]
10010001	[166, 167, 168, 169, 170]
10110101	[171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181]
10100001	[182, 183, 184, 185]
10110001	[186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203]
10010010	[204, 205, 206, 207, 208, 209, 210, 211]
10100010	[212, 213]
10110000	[214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226]
10010000	[227, 228, 229, 230, 231]
11010100	[232, 233, 234, 235, 236, 237, 238, 239, 240, 241]
10100100	[242, 243, 244, 245, 246, 247, 248, 249, 250]
00100000	[251]

00010100	0	True
11111000	8	False
10111010	6	False
00111010	14	True
10011100	17	True
10101001	27	True
00111100	29	True
00111000	37	True
00001100	61	False
00101000	51	True
00001001	77	False
11011001	86	True
00001101	85	False
01111001	129	False
11000011	96	False
10011011	105	False
10101011	112	True
11110011	108	False
01111101	140	False
10110011	115	False
00111001	148	False
10100111	137	False
10110111	144	False
10010101	153	False
10010001	158	False
10110101	153	False
10100001	189	False
10110001	201	True
10010010	202	False
10100010	214	False
10110000	222	True
10010000	230	True
11010100	222	False
10100100	243	True
00100000	5	False

Number of codes used=35


End of hp run 18.  Result of run:
[(-0.9088810975343473, 70000), ('21-05-08_23:17:59BST_NLearn_model_18_Alice_iter70000', '21-05-08_23:17:59BST_NLearn_model_18_Bob_iter70000')]
(-0.9088810975343473, 70000)


>>>> hp_run=19 of 20, time elapsed 6:45:04 of estimated 7:30:04, 
implying ending at 06:48:03BST on Sunday 9 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (406320, 665309, 640372, 353471),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 16,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': 3000,
	'N_CODE': 16,
	'N_NUMBERS': 16
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=     20000 training nets give:
alice_loss.item()=0.6860721707344055	bob_loss.item()=0.5651372671127319

0010001000000010	[0, 13, 14, 15]
0101101101001101	[1]
0010010101001100	[2]
1101111010000100	[3, 4]
0100011111111110	[5]
1100111110011010	[6]
1100000001110001	[7, 8]
1000110111000010	[9]
0001101011110110	[10, 11]
0011110000011111	[12]

0010001000000010	7	False
0101101101001101	8	False
0010010101001100	9	False
1101111010000100	7	False
0100011111111110	7	False
1100111110011010	8	False
1100000001110001	9	False
1000110111000010	8	False
0001101011110110	7	False
0011110000011111	7	False

Number of codes used=10

Iteration=     30000 training nets give:
alice_loss.item()=0.16595864295959473	bob_loss.item()=0.13447663187980652

0000000001001001	[0, 1, 2, 15]
0111110000010110	[3, 12, 13, 14]
0011011110111001	[4]
0100011111111110	[5, 6]
1000110111000010	[7, 8]
1100000001110001	[9, 10, 11]

0000000001001001	0	True
0111110000010110	15	False
0011011110111001	10	False
0100011111111110	5	True
1000110111000010	10	False
1100000001110001	9	True

Number of codes used=6

Iteration=     40000 training nets give:
alice_loss.item()=0.21490664780139923	bob_loss.item()=0.12062250822782516

0111010010110111	[0, 1, 14, 15]
0000000000001001	[2]
0011011110111001	[3, 4, 5]
0100011111111110	[6]
0001101011110110	[7]
0110110110100101	[8, 9]
0010111110101011	[10]
1010011111000110	[11, 12]
0111101110111010	[13]

0111010010110111	13	False
0000000000001001	1	False
0011011110111001	3	True
0100011111111110	5	False
0001101011110110	10	False
0110110110100101	8	True
0010111110101011	2	False
1010011111000110	8	False
0111101110111010	14	False

Number of codes used=9

Iteration=     50000 training nets give:
alice_loss.item()=0.3159201741218567	bob_loss.item()=0.10673430562019348

0011101100100011	[0]
0001000110111000	[1]
1001010101110011	[2]
1111000001010010	[3]
1001111011110111	[4]
0000001011110101	[5, 6]
1111001111010000	[7, 8, 9]
0011110111010111	[10]
0011110000010110	[11, 12]
0000000010010101	[13]
1111111100000110	[14, 15]

0011101100100011	15	False
0001000110111000	2	False
1001010101110011	3	False
1111000001010010	15	False
1001111011110111	7	False
0000001011110101	7	False
1111001111010000	9	True
0011110111010111	13	False
0011110000010110	12	True
0000000010010101	9	False
1111111100000110	13	False

Number of codes used=11

Iteration=     60000 training nets give:
alice_loss.item()=0.21894633769989014	bob_loss.item()=0.04831332340836525

0001001110110000	[0, 15]
1001011001000100	[1]
0000001010011111	[2]
1001011101111100	[3]
1010001111100101	[4]
1101100110101110	[5]
1101001011101101	[6]
1110011110001000	[7, 8]
1100100011000010	[9]
0001100111100011	[10]
1111111100000110	[11]
1110111111100100	[12]
1010100010101011	[13, 14]

0001001110110000	6	False
1001011001000100	9	False
0000001010011111	7	False
1001011101111100	4	False
1010001111100101	9	False
1101100110101110	7	False
1101001011101101	8	False
1110011110001000	3	False
1100100011000010	9	True
0001100111100011	15	False
1111111100000110	13	False
1110111111100100	8	False
1010100010101011	0	False

Number of codes used=13

Iteration=     70000 training nets give:
alice_loss.item()=0.1676853597164154	bob_loss.item()=0.041040875017642975

1010001011110010	[0, 15]
1110101111100010	[1]
0101110000010111	[2]
0101001100110010	[3]
1101111010011000	[4]
1100010011101000	[5]
0011101010101100	[6]
1000001100111100	[7]
1111110010000000	[8]
0001111100000111	[9]
0001111101000101	[10]
1100101001100011	[11, 12]
1111000001000001	[13]
1011100010110100	[14]

1010001011110010	1	False
1110101111100010	15	False
0101110000010111	12	False
0101001100110010	15	False
1101111010011000	6	False
1100010011101000	6	False
0011101010101100	8	False
1000001100111100	3	False
1111110010000000	15	False
0001111100000111	12	False
0001111101000101	13	False
1100101001100011	8	False
1111000001000001	15	False
1011100010110100	11	False

Number of codes used=14


End of hp run 19.  Result of run:
[(-0.6444575008917889, 40000), ('21-05-08_23:17:59BST_NLearn_model_19_Alice_iter70000', '21-05-08_23:17:59BST_NLearn_model_19_Bob_iter70000')]
(-0.6444575008917889, 40000)


>>>> hp_run=20 of 20, time elapsed 7:10:12 of estimated 7:32:51, 
implying ending at 06:50:50BST on Sunday 9 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (406320, 665309, 640372, 353471),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 16,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': 3000,
	'N_CODE': 16,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7705517503711219

Iteration=     20000 training nets give:
alice_loss.item()=0.6536369323730469	bob_loss.item()=0.5931508541107178

1001111011010110	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
1101001000001111	[75, 76, 77, 78, 79, 80, 81, 82]
0110111101001110	[83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97]
0111100100110111	[98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134]
0100111110101010	[135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156]
1011111101101011	[157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174]
0000101010001101	[175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202]
1111000100011000	[203, 204, 205, 206, 207]
0011111100110100	[208, 209, 210, 211, 212, 213, 214, 215]

1001111011010110	220	True
1101001000001111	17	False
0110111101001110	180	False
0111100100110111	37	False
0100111110101010	192	False
1011111101101011	228	False
0000101010001101	30	False
1111000100011000	253	False
0011111100110100	49	False

Number of codes used=9

Iteration=     30000 training nets give:
alice_loss.item()=0.5015881061553955	bob_loss.item()=0.2603810429573059

1001111011010110	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
0100010110010100	[11, 12, 13, 14, 15, 16]
0101110011001001	[17, 18, 19, 20, 21, 22, 23, 24]
1010000101111111	[25, 26, 27, 28, 29]
0111001101001001	[30, 31]
1101101001011010	[32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
1100100001100000	[48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110]
0000111111000100	[111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147]
1001100110011101	[148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190]
0011110001011001	[191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217]

1001111011010110	245	True
0100010110010100	235	False
0101110011001001	131	False
1010000101111111	60	False
0111001101001001	218	False
1101101001011010	68	False
1100100001100000	91	True
0000111111000100	117	True
1001100110011101	166	True
0011110001011001	229	False

Number of codes used=10

Iteration=     40000 training nets give:
alice_loss.item()=0.3219449818134308	bob_loss.item()=0.07100081443786621

1010000101111111	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
1100100000100000	[101, 102, 103, 104, 105, 106, 107]
0000111111000101	[108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120]
1110110110010010	[121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170]
1110110010010111	[171, 172, 173, 174]
1101111101011010	[175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194]
0001000010110101	[195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208]
1011111011010110	[209, 210, 211, 212, 213]
1011001101111100	[214]

1010000101111111	31	True
1100100000100000	79	False
0000111111000101	129	False
1110110110010010	145	True
1110110010010111	102	False
1101111101011010	99	False
0001000010110101	161	False
1011111011010110	240	False
1011001101111100	5	False

Number of codes used=9

Iteration=     50000 training nets give:
alice_loss.item()=0.36677026748657227	bob_loss.item()=0.08561725169420242

0001111111000100	[0, 1, 2, 3, 4, 5, 6, 7, 8, 252, 253, 254, 255]
1100111111001111	[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
1100111101111101	[30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57]
1110110101001001	[58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91]
0001101000000010	[92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122]
0101011100001011	[123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141]
1011101111100010	[142]
0010110011101110	[143, 144, 145, 146, 147, 148, 149, 150, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180]
0100011111100000	[151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162]
0100111110101010	[181, 182, 183, 184, 185, 186, 187, 188, 189]
0101100100010010	[190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200]
1011111101101011	[201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223]
1111011001101011	[224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234]
1101011001001100	[235, 236, 237, 238, 239, 240, 241, 242, 243, 244]
0000001111101000	[245, 246, 247, 248, 249, 250, 251]

0001111111000100	109	False
1100111111001111	123	False
1100111101111101	103	False
1110110101001001	124	False
0001101000000010	81	False
0101011100001011	124	True
1011101111100010	139	False
0010110011101110	223	False
0100011111100000	120	False
0100111110101010	141	False
0101100100010010	235	False
1011111101101011	186	False
1111011001101011	52	False
1101011001001100	78	False
0000001111101000	112	False

Number of codes used=15

Iteration=     60000 training nets give:
alice_loss.item()=0.31568947434425354	bob_loss.item()=0.0331687331199646

0010110011011111	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 249, 250, 251, 252, 253, 254, 255]
1010010011011110	[38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
0110001000111101	[60, 61, 62, 63, 64]
0110101100001100	[65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
1110011100101101	[84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103]
0100001111011010	[104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121]
0011110110000110	[122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151]
1011010000011000	[152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167]
1100010101100000	[168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180]
0111011000000110	[181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194]
0101110000110001	[195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219]
0111111011000011	[220, 221, 222, 223, 224, 225, 226, 227, 228, 229]
1101100101011100	[230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248]

0010110011011111	200	False
1010010011011110	237	False
0110001000111101	27	False
0110101100001100	29	False
1110011100101101	94	True
0100001111011010	239	False
0011110110000110	221	False
1011010000011000	165	True
1100010101100000	122	False
0111011000000110	29	False
0101110000110001	88	False
0111111011000011	203	False
1101100101011100	134	False

Number of codes used=13

Iteration=     70000 training nets give:
alice_loss.item()=0.1850232481956482	bob_loss.item()=0.040538109838962555

1110001010111000	[0, 1, 2, 3, 4, 5, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
1111101010010100	[6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]
0001011001010101	[33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]
1001001100011101	[45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
0010111110011001	[62, 63, 64, 65, 66]
0001101000000010	[67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82]
0111100100110111	[83, 84, 85, 86, 87, 88, 89, 90]
1001001101100110	[91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107]
0001010100001010	[108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131]
1010111101001000	[132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149]
0011011011101100	[150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170]
1100011100101011	[171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185]
1110101001001100	[186]
1011111101101011	[187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197]
1000101000011100	[198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 212, 213, 214, 215, 216, 218, 219]
0001011101000101	[209, 210, 211, 217]
1110100010000000	[220, 221, 222, 223]

1110001010111000	235	True
1111101010010100	221	False
0001011001010101	92	False
1001001100011101	2	False
0010111110011001	141	False
0001101000000010	96	False
0111100100110111	62	False
1001001101100110	112	False
0001010100001010	144	False
1010111101001000	128	False
0011011011101100	192	False
1100011100101011	129	False
1110101001001100	79	False
1011111101101011	198	False
1000101000011100	179	False
0001011101000101	124	False
1110100010000000	125	False

Number of codes used=17


End of hp run 20.  Result of run:
[(-0.7284957653309024, 40000), ('21-05-08_23:17:59BST_NLearn_model_20_Alice_iter70000', '21-05-08_23:17:59BST_NLearn_model_20_Bob_iter70000')]
(-0.7284957653309024, 40000)



Time taken over all 20 given sets of hyperparameters=7:35:26, averaging 0:22:46 per run


 ---- Table of results ----

     code  hp_run  result
 00000000       1  (-0.998, 70000)
 00000001       2  (-0.994, 70000)
 00000010       3  (-0.979, 70000)
 00000011       4  (-0.984, 70000)
 00000100       5  (-0.992, 70000)
 00000101       6  (-0.993, 70000)
 00000110       7  (-0.963, 70000)
 00000111       8  (-0.977, 70000)
 00000200       9  (-0.994, 70000)
 00000201      10  (-0.994, 70000)
 00000210      11  (-0.907, 70000)
 00000211      12  (-0.972, 70000)
 00000300      13  (-0.902, 60000)
 00000301      14  (-0.963, 70000)
 00000310      15  (-0.654, 40000)
 00000311      16  (-0.847, 70000)
 00000400      17  (-0.810, 70000)
 00000401      18  (-0.909, 70000)
 00000410      19  (-0.644, 40000)
 00000411      20  (-0.728, 40000)
 --------------------------

++++ Best result was (-0.998, 70000) on hp_run=1 with
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (406320, 665309, 640372, 353471),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 16,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 16
	'n_rng': Generator(PCG64)
	'ne_rng': Generator(PCG64)
	't_rng': <torch._C.Generator object at 0x7faacad92cf0>
	'te_rng': <torch._C.Generator object at 0x7faacad92970>
}


End closed log for run 21-05-08_23:17:59BST
