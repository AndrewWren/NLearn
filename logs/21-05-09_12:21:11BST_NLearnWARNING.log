****WARNING NOT CLOSED - MAY BE DUE TO ERROR***
SMOOTHING_LENGTH = 10000
SAVE_PERIOD = 100000
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # Used: "cuda"
MODEL_FOLDER = 'models'
CONFIGS_FOLDER = 'configs'
LOGS_FOLDER = 'logs'

hyperparameters = {
	'N_ITERATIONS': 500000,
	'RANDOM_SEEDS': [(406320, 665309, 640372, 353471)],
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': ['SGD(lr=0.01)'],
	'BOB_OPTIMIZER': [('SGD', '{"lr": 0.01}')],
	'ALICE_LOSS_FUNCTION': ['Huber(beta=0.1)'],
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': [0.1],
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}



>>>> hp_run=1 of 1
hyperparameters = {
	'N_ITERATIONS': 500000,
	'RANDOM_SEEDS': (406320, 665309, 640372, 353471),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=     20000 training nets give:
alice_loss.item()=0.633818507194519	bob_loss.item()=0.48711854219436646

00111101	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 254, 255]
01000010	[24, 25, 26, 27, 28, 29]
11000111	[30, 31]
01000100	[32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149]
00001111	[150, 151, 152, 153, 154, 155, 156]
00100011	[157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194]
10010101	[195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206]
01001010	[207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253]

00111101	197	False
01000010	214	False
11000111	207	False
01000100	61	True
00001111	77	False
00100011	0	False
10010101	190	False
01001010	74	False

Number of codes used=8

Iteration=     30000 training nets give:
alice_loss.item()=0.21884684264659882	bob_loss.item()=0.17427468299865723

00010010	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10101100	[20, 21, 22, 23, 24, 25]
00000111	[26, 27]
01000100	[28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96]
01111100	[97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]
10010101	[160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205]

00010010	225	True
10101100	17	False
00000111	107	False
01000100	90	True
01111100	131	True
10010101	203	True

Number of codes used=6

Iteration=     40000 training nets give:
alice_loss.item()=0.1711045801639557	bob_loss.item()=0.3497779965400696

00000010	[0, 1, 2, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10100100	[3, 4, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]
10101100	[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]
11000100	[54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70]
01000100	[71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
01100100	[96, 97, 98, 99, 100, 101]
01110100	[102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]
01111100	[120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135]
01111110	[136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157]
11111100	[158]
01011100	[159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193]
10010101	[194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205]
11010101	[206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216]
00010011	[217, 218, 219]
00010010	[220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233]

00000010	240	True
10100100	25	False
10101100	21	True
11000100	52	False
01000100	88	True
01100100	95	False
01110100	111	True
01111100	140	False
01111110	142	True
11111100	124	False
01011100	165	True
10010101	214	False
11010101	210	True
00010011	215	False
00010010	241	False

Number of codes used=15

Iteration=     50000 training nets give:
alice_loss.item()=0.10294955968856812	bob_loss.item()=0.16150256991386414

10011100	[0, 1, 2, 3, 4, 5, 6, 249, 250, 251, 252, 253, 254, 255]
10110100	[7, 8]
10001100	[9, 10, 11, 12, 13, 14]
10101100	[15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]
00101100	[27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]
01000001	[45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]
01000000	[65, 66, 67, 68, 69, 70, 71, 72]
11000110	[73, 74, 75]
01000101	[76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86]
01100100	[87, 88, 89, 90, 91, 92]
01101100	[93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112]
01110100	[113, 119]
01101110	[114, 115, 116, 117, 118, 120, 121]
01101010	[122, 123]
11111100	[124, 125, 126, 127, 128, 129, 130, 131, 132]
01111100	[133, 134]
01111000	[135, 136]
01111110	[137, 138, 139, 140, 141, 142, 143, 144, 145]
11111010	[146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157]
00111111	[158, 159]
01011100	[160, 161, 162, 163]
01011101	[164]
01011110	[165, 166, 167, 168, 169, 170, 171]
11110110	[172, 173, 174, 175, 176, 177, 178]
11011101	[179, 180, 181, 182, 183, 184, 185]
01011111	[186, 187, 188, 189, 190, 191]
11011000	[192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]
00010011	[210, 211, 212, 213]
10110101	[214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226]
10010011	[227, 228, 229, 230]
00010110	[231, 232, 233, 234, 235]
00000010	[236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248]

10011100	6	True
10110100	8	True
10001100	15	False
10101100	26	True
00101100	35	True
01000001	65	False
01000000	82	False
11000110	66	False
01000101	80	True
01100100	79	False
01101100	100	True
01110100	111	False
01101110	115	True
01101010	128	False
11111100	131	True
01111100	130	False
01111000	143	False
01111110	143	True
11111010	151	True
00111111	157	False
01011100	155	False
01011101	166	False
01011110	161	False
11110110	152	False
11011101	178	False
01011111	177	False
11011000	195	True
00010011	220	False
10110101	201	False
10010011	217	False
00010110	241	False
00000010	238	True

Number of codes used=32

Iteration=     60000 training nets give:
alice_loss.item()=0.04729563742876053	bob_loss.item()=0.1533113569021225

10011100	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 253, 254, 255]
10001000	[11, 12, 13]
10000100	[14, 15, 16, 17, 18, 19, 20]
10100110	[21, 22, 23]
10101101	[24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
00001100	[36, 37, 38, 39, 40, 41]
00101101	[42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54]
11000001	[55, 56, 57, 58, 59, 60]
11000101	[61, 62, 63]
00100100	[64, 65, 66, 67, 68, 69, 70, 71]
01000000	[72, 73, 74, 75, 76, 77]
01001100	[78, 79, 80, 81, 82, 83, 84, 85]
01100000	[86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96]
01010100	[97]
00110100	[98, 99, 100, 101]
01101000	[102, 103, 104, 105, 106, 107, 108, 109]
01110100	[110, 111, 112, 113, 114]
01101110	[115]
01110101	[116, 117, 118, 119, 120, 121, 122]
11111100	[123, 124, 125]
01111100	[126, 127, 128, 129, 130, 131, 132, 133]
01110110	[134, 135, 136, 137, 138, 139, 140, 141]
01111110	[142, 143, 144]
00011000	[145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168]
11111001	[169, 170, 171, 172, 173, 174, 175]
01011000	[176, 177, 178]
01011011	[179, 180, 181, 182, 183, 184]
11011001	[185, 186, 187, 188, 189, 190, 191]
00110101	[192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204]
11010101	[205, 206, 207]
01010011	[208, 209]
10010111	[210, 211, 212, 213, 214, 215]
00011010	[216, 217]
01010010	[218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]
10100010	[230, 231, 232]
10010010	[233, 234]
00010010	[235, 236, 237, 238, 239, 240, 241, 242, 243]
00000010	[244, 245]
10001010	[246, 247, 248, 249, 250, 251, 252]

10011100	2	True
10001000	1	False
10000100	254	False
10100110	21	True
10101101	36	False
00001100	38	True
00101101	43	True
11000001	62	False
11000101	59	False
00100100	61	False
01000000	71	False
01001100	86	False
01100000	93	True
01010100	94	False
00110100	121	False
01101000	108	True
01110100	116	False
01101110	119	False
01110101	114	False
11111100	134	False
01111100	139	False
01110110	147	False
01111110	155	False
00011000	164	True
11111001	178	False
01011000	179	False
01011011	178	False
11011001	185	True
00110101	146	False
11010101	208	False
01010011	197	False
10010111	217	False
00011010	168	False
01010010	226	True
10100010	241	False
10010010	224	False
00010010	232	False
00000010	231	False
10001010	2	False

Number of codes used=39

Iteration=     70000 training nets give:
alice_loss.item()=0.024311766028404236	bob_loss.item()=0.33116424083709717

10111101	[0, 1, 2, 3, 248, 249, 250, 251, 252, 253, 254, 255]
00101010	[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
10111100	[16, 17, 18, 19, 20, 21]
10101001	[22, 23, 24]
10101111	[25, 26, 27, 28, 29, 30, 31, 32]
00100000	[33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]
00101100	[45, 46, 47, 48, 49]
00101000	[50, 51, 52, 53, 54, 55, 56, 57]
11000001	[58]
11000101	[59, 60, 61, 62]
11101000	[63, 64, 65, 66, 67, 68, 69]
00100001	[70, 71, 72, 73]
01000110	[74, 75, 76, 77]
01001100	[78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88]
01100001	[89, 90, 91]
01010100	[92, 93, 94, 95, 96, 97, 98, 99]
01100110	[100, 101, 102, 103, 104, 105]
01010000	[106, 107, 108, 109, 110, 111, 112]
01110100	[113, 114]
01001110	[115, 116, 117, 118, 119, 120, 121, 122, 123, 124]
01100010	[125, 126, 127]
11111100	[128, 129, 130]
01111100	[131, 132]
11111000	[133, 134, 135, 136, 137, 138]
01110110	[139, 140]
11111110	[141, 142, 143, 144, 145, 146, 147, 148, 149, 150]
11111010	[151, 152]
00011100	[153]
01111001	[154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166]
00011110	[167, 168]
10111111	[169, 170]
11110101	[171, 172, 173, 174, 175, 176, 177, 178, 179]
00011111	[180, 181]
11011001	[182, 183, 184, 185, 186, 187, 188]
11011000	[189, 190, 191, 192, 193, 194, 195, 196, 197, 198]
10110001	[199, 200]
10000101	[201, 202, 203, 204, 205]
10010000	[206, 207]
00010111	[208, 209, 210, 211, 212, 213, 214, 215, 216, 217]
10010110	[218, 219, 220, 221, 222, 223, 224, 225, 226, 227]
00010000	[228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 241, 242, 243, 244]
10011000	[240]
00100010	[245, 246, 247]

10111101	4	False
00101010	16	False
10111100	9	False
10101001	18	False
10101111	25	True
00100000	45	False
00101100	57	False
00101000	53	True
11000001	58	True
11000101	54	False
11101000	62	False
00100001	76	False
01000110	83	False
01001100	87	True
01100001	93	False
01010100	97	True
01100110	103	True
01010000	106	True
01110100	104	False
01001110	118	True
01100010	123	False
11111100	132	False
01111100	134	False
11111000	134	True
01110110	135	False
11111110	144	True
11111010	146	False
00011100	164	False
01111001	145	False
00011110	173	False
10111111	176	False
11110101	182	False
00011111	183	False
11011001	185	True
11011000	195	True
10110001	205	False
10000101	235	False
10010000	216	False
00010111	209	True
10010110	226	True
00010000	235	True
10011000	224	False
00100010	7	False

Number of codes used=43

Iteration=     80000 training nets give:
alice_loss.item()=0.01593642495572567	bob_loss.item()=0.2557574212551117

10100111	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
10100000	[11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
00100110	[21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]
00101101	[35, 36, 37]
11001101	[38, 39, 40, 41, 42]
00101000	[43, 44, 45, 46, 47, 48, 49, 50]
11101110	[51, 52, 53, 54, 55, 56, 57, 58, 59, 60]
00000100	[61, 62, 63, 64]
11101000	[65, 66, 67]
11100110	[68, 69, 70, 75]
11100101	[71, 72, 73, 74]
11100000	[76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87]
01001100	[88, 89, 90, 91, 92]
01001000	[93, 94]
01010100	[95, 96, 97, 98, 99, 100, 101]
01010000	[102, 103, 104, 105, 106, 107, 108, 109, 110, 111]
01110000	[112, 113, 114]
01101110	[115, 116, 117, 118]
01100010	[119, 120, 121, 122, 123, 124, 125, 126]
11110000	[127, 128, 129, 130, 131, 132, 133, 134]
00111000	[135, 136, 137, 138, 139, 140, 141]
11111110	[142, 143, 144]
01110010	[145]
00110110	[146, 147]
01110111	[148, 149, 150, 151, 152]
00011100	[153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165]
01011110	[166, 167, 168, 169]
11011110	[170, 171, 172]
11110111	[173, 174]
00011111	[175, 176, 177, 178, 179, 180, 181, 182, 183]
11011101	[184, 185]
11011001	[186, 187]
00010001	[188, 189, 190, 191, 192]
10111010	[193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203]
10110111	[204, 205]
00001111	[206]
11010101	[207, 208, 209]
10010001	[210, 211, 212, 213]
10010111	[214, 215, 216, 217]
11001011	[218]
11010110	[219, 220, 221, 222, 223, 224, 225]
10010010	[226, 227]
10100011	[228, 229, 230]
10000011	[231, 232, 233, 234, 235, 236, 237]
00000010	[238]
10000000	[239, 240, 241, 242, 243, 244, 245, 246, 247]
00001010	[248, 249, 250]
10110100	[251, 252, 253, 254, 255]

10100111	2	True
10100000	16	True
00100110	25	True
00101101	42	False
11001101	48	False
00101000	45	True
11101110	56	True
00000100	57	False
11101000	67	True
11100110	83	False
11100101	70	False
11100000	86	True
01001100	88	True
01001000	99	False
01010100	97	True
01010000	104	True
01110000	111	False
01101110	107	False
01100010	118	False
11110000	131	True
00111000	137	True
11111110	143	True
01110010	140	False
00110110	148	False
01110111	164	False
00011100	166	False
01011110	161	False
11011110	171	True
11110111	181	False
00011111	183	True
11011101	187	False
11011001	192	False
00010001	192	True
10111010	200	True
10110111	198	False
00001111	192	False
11010101	197	False
10010001	203	False
10010111	217	True
11001011	217	False
11010110	216	False
10010010	223	False
10100011	244	False
10000011	221	False
00000010	237	False
10000000	246	True
00001010	246	False
10110100	252	True

Number of codes used=48

Iteration=     90000 training nets give:
alice_loss.item()=0.03151809424161911	bob_loss.item()=0.20672285556793213

10110100	[0, 1, 250, 251, 252, 253, 254, 255]
10100111	[2, 3, 4, 5]
10111000	[6, 7, 8, 9]
10001100	[10, 11, 12, 13]
10101001	[14, 15, 16, 17, 18, 19]
00100110	[20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
00001101	[31, 32, 33, 34, 35]
00001001	[36, 37, 38]
00101001	[39, 40, 41, 42]
00101000	[43]
11101100	[44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62]
00100100	[63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73]
11100010	[74]
11100110	[75, 76, 77, 78, 79, 80, 81, 82]
11100000	[83, 84, 85]
01000000	[86, 87, 88, 89]
11110100	[90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]
01001000	[101]
01010000	[102]
01100110	[103, 104]
01110100	[105, 106, 107, 108, 109]
01001110	[110, 111, 112, 113, 114, 115, 116]
01100010	[117, 118, 119, 120, 121, 122, 123, 124]
11110000	[125, 126, 127, 128, 129]
01110110	[130, 131, 132]
11011100	[133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148]
01110001	[149, 150, 151, 152, 153, 154, 155, 156, 157]
01111111	[158, 159, 160, 161, 162]
11111011	[163]
11011110	[164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174]
11110011	[175, 176]
00011001	[177, 178, 179, 180, 181, 182]
00011111	[183, 184]
10011001	[185, 186, 187, 188]
11011000	[189, 190]
00010001	[191, 192, 193, 194, 195]
00001111	[196, 197, 198, 199, 200]
00000111	[201, 202, 205, 206, 207, 208]
11010011	[203, 204]
10011000	[209, 210, 211, 212, 213, 214, 215]
11010010	[216, 217, 218, 219, 220, 221, 222, 223, 224, 225]
00010000	[226, 227, 228]
10110010	[229, 230, 231, 232, 233]
00010010	[234, 235, 236, 237, 238, 239]
10110000	[240, 241, 242, 243, 244]
11000010	[245, 246]
10100010	[247, 248, 249]

10110100	252	True
10100111	2	True
10111000	8	True
10001100	11	True
10101001	11	False
00100110	37	False
00001101	38	False
00001001	34	False
00101001	53	False
00101000	51	False
11101100	61	True
00100100	65	True
11100010	77	False
11100110	81	True
11100000	85	True
01000000	87	True
11110100	96	True
01001000	100	False
01010000	107	False
01100110	102	False
01110100	109	True
01001110	112	True
01100010	115	False
11110000	133	False
01110110	134	False
11011100	150	False
01110001	116	False
01111111	161	True
11111011	177	False
11011110	173	True
11110011	177	False
00011001	189	False
00011111	187	False
10011001	212	False
11011000	181	False
00010001	198	False
00001111	187	False
00000111	201	True
11010011	206	False
10011000	209	True
11010010	206	False
00010000	234	False
10110010	239	False
00010010	234	True
10110000	240	True
11000010	250	False
10100010	252	False

Number of codes used=47

Iteration=    100000 training nets give:
alice_loss.item()=0.021599913015961647	bob_loss.item()=0.1949356645345688

10001011	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 252, 253, 254, 255]
10001100	[10, 11, 12, 13, 14, 15, 16, 17, 18]
10100100	[19, 20]
00000110	[21, 22, 23, 24, 25, 26, 27, 28]
10100101	[29]
10101100	[30, 31, 32, 33, 34, 35, 36]
00001001	[37, 38, 39, 40, 41]
11001001	[42, 43, 44, 45, 46, 47]
11001101	[48]
11000101	[49, 50, 51, 52, 53, 54]
11001100	[55, 56, 57, 58, 59, 60]
11101110	[61, 62, 63, 64, 65]
11100100	[66, 67]
11100101	[68, 69, 70, 71, 72, 73]
11001110	[74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87]
00110100	[88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108]
01101110	[109, 110]
01001110	[111, 112]
01001010	[113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123]
01100111	[124, 125]
01011100	[126, 127]
01111100	[128, 129, 130, 131]
01110110	[132, 133, 134]
00111100	[135, 136, 137, 138, 139]
11111110	[140, 141, 142]
11011100	[143, 144, 145, 146, 147, 148, 149, 150]
11111010	[151, 152]
01011110	[153, 154, 159, 160, 161]
11111111	[155, 156, 157, 158]
00011110	[162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174]
11110111	[175, 176, 177, 178]
11011000	[179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193]
10011111	[194, 195, 196, 197]
00010111	[198, 199, 200, 201, 202, 203, 204, 205, 206]
00011010	[207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223]
00010000	[224, 225, 226, 227, 228, 229, 230, 231, 232, 233]
10110010	[234, 235]
10000101	[236, 237, 238, 239, 240, 241, 242, 243, 244, 245]
11000010	[246]
10011100	[247, 248, 249, 250, 251]

10001011	248	False
10001100	15	True
10100100	18	False
00000110	21	True
10100101	24	False
10101100	39	False
00001001	35	False
11001001	43	True
11001101	53	False
11000101	59	False
11001100	64	False
11101110	61	True
11100100	76	False
11100101	74	False
11001110	81	True
00110100	109	False
01101110	106	False
01001110	111	True
01001010	115	True
01100111	120	False
01011100	127	True
01111100	132	False
01110110	130	False
00111100	145	False
11111110	138	False
11011100	147	True
11111010	150	False
01011110	152	False
11111111	152	False
00011110	176	False
11110111	169	False
11011000	189	True
10011111	200	False
00010111	205	True
00011010	225	False
00010000	231	True
10110010	231	False
10000101	240	True
11000010	249	False
10011100	2	False

Number of codes used=40

Iteration=    110000 training nets give:
alice_loss.item()=0.010637491941452026	bob_loss.item()=0.36292755603790283

11000011	[0, 1, 2, 3, 4, 5, 6, 7, 8, 252, 253, 254, 255]
00101010	[9, 10, 11]
10001100	[12, 13, 14, 15, 16, 17, 18, 19, 20, 21]
00101011	[22, 23, 24, 25]
00100111	[26, 27, 28, 29, 30, 31]
00001110	[32, 33, 34]
00001001	[35, 36, 37, 38, 39]
00000101	[40, 41, 42]
00001101	[43, 44, 45, 46, 47, 48]
00101100	[49, 50, 51]
11001000	[52, 53, 54, 55, 56, 57, 58, 59]
11101101	[60]
11101011	[61, 62, 63, 64, 65, 66]
00100100	[67, 68]
01000001	[69, 70, 71, 72, 73, 74]
11000000	[75, 76]
01101001	[77]
01100000	[78, 79, 80, 81, 82, 83, 84, 85, 86]
01000110	[87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
01001000	[100, 101]
01101010	[102, 103, 104, 105]
01010000	[106]
01110100	[107, 108, 109, 110, 111, 112, 113, 114, 115]
01100111	[116, 117, 118, 119]
01011100	[120, 121, 122, 123, 124, 125, 126, 127, 128, 129]
01110010	[130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141]
11111000	[142, 143, 144, 145, 146]
00111010	[147, 148, 149, 150, 151, 152, 153, 154, 155, 156]
01111011	[157, 158, 159, 160, 161, 162, 163, 164]
00111111	[165, 166, 167, 168, 169, 170, 171, 172]
11111011	[173, 174, 175, 176, 177, 178, 179, 180, 181, 182]
00011111	[183, 184, 185, 186, 187, 188, 189, 190]
10111111	[191, 192, 193, 194, 195, 196]
00010111	[197, 198, 199, 200, 201, 202]
10111110	[203, 204, 205, 206, 207]
10111011	[208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222]
10010010	[223, 224, 225, 226, 227]
10010000	[228, 229]
10010100	[230, 231, 232, 233, 234, 235, 236, 237, 238]
10111001	[239, 240, 241, 242, 243, 244, 245, 246, 247]
00001010	[248, 249, 250, 251]

11000011	3	True
00101010	12	False
10001100	41	False
00101011	14	False
00100111	27	True
00001110	52	False
00001001	36	True
00000101	45	False
00001101	46	True
00101100	48	False
11001000	57	True
11101101	64	False
11101011	63	True
00100100	70	False
01000001	71	True
11000000	70	False
01101001	79	False
01100000	89	False
01000110	98	True
01001000	105	False
01101010	110	False
01010000	116	False
01110100	113	True
01100111	113	False
01011100	123	True
01110010	144	False
11111000	147	False
00111010	155	True
01111011	160	True
00111111	170	True
11111011	177	True
00011111	186	True
10111111	192	True
00010111	194	False
10111110	206	True
10111011	218	True
10010010	225	True
10010000	225	False
10010100	233	True
10111001	241	True
00001010	0	False

Number of codes used=41

Iteration=    120000 training nets give:
alice_loss.item()=0.024615764617919922	bob_loss.item()=0.43389445543289185

10000000	[0, 250, 251, 252, 253, 254, 255]
10100010	[1, 2, 4, 5, 6]
11000011	[3, 7]
10100111	[8, 9, 10]
10101011	[11]
00100010	[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
00000001	[24, 25, 26, 27, 28, 29, 30, 31, 32]
11001001	[33]
11000001	[34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]
00001100	[45, 46, 47, 48, 49]
11001101	[50, 51, 52, 53, 54, 55]
11101111	[56, 57]
11101100	[58]
11101110	[59, 60, 61, 62, 63, 64, 65]
11100011	[66, 67, 68, 69, 70]
01000001	[71, 72, 73, 74, 75, 76, 77, 78]
11001110	[79]
01000000	[80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92]
01101101	[93]
01000110	[94, 95, 96, 97, 98, 99]
01001000	[100, 101, 102, 103, 104]
11010100	[105, 106, 107, 108, 109, 110]
01100111	[111, 112, 113]
01110101	[114, 115, 116, 117, 118, 119, 120, 121, 122]
01011100	[123, 124, 125, 126, 127, 128, 129]
01110110	[130, 131, 132, 133, 134]
11111110	[135, 136, 137, 138, 139, 140]
01011110	[141]
01001011	[142, 143, 144]
11111000	[145, 146, 147, 148, 149, 150]
00111110	[151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163]
01011010	[164, 165]
11110010	[166, 167, 168, 169, 170]
00011101	[171, 172, 173, 174, 175, 176]
01011011	[177, 178, 179, 180, 181, 182]
11010101	[183, 184, 185, 186, 187]
00010101	[188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202]
10110101	[203, 204, 205, 206, 207, 208, 209, 210]
10010101	[211, 212, 213, 214, 215, 216, 217, 218]
10110001	[219, 220, 221, 222, 223, 224, 225, 226]
11001011	[227, 228, 229]
10010100	[230, 231, 232, 233, 234, 235, 236]
10111001	[237, 238, 239, 240, 241, 242, 243, 244]
10001010	[245, 246]
10111101	[247, 248, 249]

10000000	2	False
10100010	0	False
11000011	1	False
10100111	7	False
10101011	251	False
00100010	21	True
00000001	25	True
11001001	33	True
11000001	54	False
00001100	43	False
11001101	53	True
11101111	52	False
11101100	60	False
11101110	56	False
11100011	61	False
01000001	78	True
11001110	79	True
01000000	88	True
01101101	94	False
01000110	94	True
01001000	101	True
11010100	112	False
01100111	109	False
01110101	126	False
01011100	127	True
01110110	136	False
11111110	138	True
01011110	143	False
01001011	147	False
11111000	148	True
00111110	159	True
01011010	160	False
11110010	170	True
00011101	180	False
01011011	183	False
11010101	180	False
00010101	188	True
10110101	205	True
10010101	217	True
10110001	217	False
11001011	228	True
10010100	229	False
10111001	240	True
10001010	245	True
10111101	250	False

Number of codes used=45

Iteration=    130000 training nets give:
alice_loss.item()=0.025530917569994926	bob_loss.item()=0.22779984772205353

10100010	[0, 1, 2, 3, 4, 5, 6]
10100111	[7, 8]
00001011	[9, 10, 11, 12, 13, 14, 15, 16, 17, 18]
10100101	[19, 20]
10101000	[21, 22]
00000001	[23, 24, 25, 26, 27, 28, 29, 30, 31, 32]
11001111	[33]
00001001	[34]
00100101	[35, 36, 37, 38, 39, 40]
00001100	[41, 42, 43, 44, 45, 46, 47, 48, 49]
00001101	[50]
11001101	[51, 52, 53]
11001000	[54]
01001001	[55, 56]
00100100	[57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]
01101001	[70, 71, 72, 73, 74, 75, 76, 77, 78, 79]
01001101	[80, 81, 82, 83, 84, 85]
01000100	[86, 87]
01000000	[88, 89, 90, 91]
01100110	[92, 93, 94, 95, 96, 100]
01001100	[97, 98, 99]
01000111	[101, 102, 103]
01101011	[104, 105, 106, 107, 108, 109, 110]
01100111	[111, 112, 113, 114]
01110100	[115]
01001111	[116, 117, 118, 119]
01110101	[120, 121, 122, 123, 124, 125, 126]
11111100	[127, 128, 129, 130]
11110101	[131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144]
01111110	[145, 146]
11111010	[147, 148, 149, 150]
01110111	[151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162]
01111011	[163, 164, 165]
11110011	[166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179]
01010111	[180, 181, 182, 183, 184]
00011111	[185, 186]
01010011	[187]
00010001	[188, 189, 190, 191, 192, 193, 194, 195, 196, 197]
00011010	[198, 199, 200, 201, 202, 203, 204, 205]
10011000	[206, 207, 208]
11011001	[209, 210, 211, 212, 213, 214]
10111011	[215, 216, 217, 218, 219, 220, 221, 222, 223]
10110000	[224, 225, 226, 227, 228, 229, 230, 231]
10110010	[232, 233, 234, 235, 236]
00010000	[237]
10000010	[238, 239, 240, 241, 242]
10001010	[243, 244, 245, 246, 247, 248, 249]
10001111	[250, 251, 252, 253, 254]
10001110	[255]

10100010	16	False
10100111	9	False
00001011	21	False
10100101	20	True
10101000	24	False
00000001	27	True
11001111	36	False
00001001	41	False
00100101	62	False
00001100	43	True
00001101	47	False
11001101	46	False
11001000	52	False
01001001	60	False
00100100	66	True
01101001	79	True
01001101	81	True
01000100	88	False
01000000	94	False
01100110	90	False
01001100	93	False
01000111	109	False
01101011	104	True
01100111	109	False
01110100	121	False
01001111	116	True
01110101	128	False
11111100	132	False
11110101	146	False
01111110	140	False
11111010	152	False
01110111	146	False
01111011	157	False
11110011	169	True
01010111	177	False
00011111	183	False
01010011	187	True
00010001	190	True
00011010	206	False
10011000	211	False
11011001	214	True
10111011	221	True
10110000	220	False
10110010	221	False
00010000	227	False
10000010	243	False
10001010	247	True
10001111	251	True
10001110	2	False

Number of codes used=49

Iteration=    140000 training nets give:
alice_loss.item()=0.012883574701845646	bob_loss.item()=0.29128384590148926

10001111	[0, 250, 251, 252, 253, 254, 255]
00001010	[1, 2]
10001000	[3, 4, 5, 6, 7, 8, 9]
10001001	[10, 11]
10101011	[12, 13, 14]
10101001	[15, 16, 17]
10100100	[18, 19, 20, 21]
00100110	[22, 23, 24, 25, 26, 27]
10101000	[28]
00000001	[29, 30]
10101100	[31]
00001001	[32, 33, 34, 35, 36, 37]
00100000	[38, 39, 40, 41]
00000101	[42, 43, 44, 45, 46, 47]
11101111	[48, 49, 50, 51, 52, 53, 54, 55]
11101011	[56]
11000111	[57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68]
00000100	[69, 70, 71, 72]
11001110	[73]
11100101	[74, 75, 76, 77, 78, 79, 80, 81]
11100000	[82, 83, 84, 85, 86, 87, 88]
01101101	[89, 90, 91]
01101010	[92, 93, 94, 95, 96, 97, 98, 99]
01001000	[100, 101, 102, 103, 104, 105]
01001110	[106]
01100010	[107, 108, 109, 110, 111, 112, 113, 114, 115]
01001111	[116, 117, 118, 119, 120, 121, 122, 123]
01110101	[124]
11111100	[125, 126, 127, 128]
11011100	[129, 130, 131, 132, 133, 134, 135, 136, 137]
01000011	[138, 139, 140, 141, 142]
01111101	[143, 144, 145, 146, 147]
11111000	[148, 149, 150, 151, 152, 153, 154, 155]
11011110	[156, 157, 158, 159, 160]
01111111	[161, 162, 163, 164]
11110010	[165]
00011101	[166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182]
11010111	[183, 184, 185, 186, 187, 188]
00110001	[189, 190, 191, 192, 193, 194, 195, 196]
10111111	[197]
10110111	[198, 199, 200, 201, 202, 203]
10111110	[204, 205, 206, 207, 208, 209, 210, 211]
10110001	[212, 213, 214]
10011110	[215, 216, 217, 218, 219, 220, 221, 222]
10111011	[223]
10010000	[224, 225, 226, 227, 228, 229]
10110110	[230, 231, 232, 233, 234, 235, 236]
10111001	[237, 238]
10111100	[239, 240, 241, 242, 243]
10111101	[244]
10001010	[245, 246, 247, 248, 249]

10001111	251	True
00001010	12	False
10001000	3	True
10001001	7	False
10101011	14	True
10101001	21	False
10100100	28	False
00100110	24	True
10101000	30	False
00000001	24	False
10101100	28	False
00001001	35	True
00100000	41	True
00000101	41	False
11101111	52	True
11101011	55	False
11000111	51	False
00000100	72	True
11001110	68	False
11100101	77	True
11100000	85	True
01101101	91	True
01101010	93	True
01001000	101	True
01001110	103	False
01100010	107	True
01001111	117	True
01110101	129	False
11111100	136	False
11011100	143	False
01000011	136	False
01111101	138	False
11111000	149	True
11011110	165	False
01111111	163	True
11110010	168	False
00011101	178	True
11010111	184	True
00110001	191	True
10111111	198	False
10110111	198	True
10111110	209	True
10110001	212	True
10011110	224	False
10111011	223	True
10010000	227	True
10110110	231	True
10111001	230	False
10111100	239	True
10111101	235	False
10001010	0	False

Number of codes used=51

Iteration=    150000 training nets give:
alice_loss.item()=0.00859074667096138	bob_loss.item()=0.08415552228689194

10001110	[0, 1, 2, 3, 4, 5, 6, 254, 255]
10000111	[7]
10100111	[8, 9, 10, 11, 12, 13, 14, 15]
10000100	[16]
00000111	[17, 18, 19, 20, 21, 22, 23]
00000000	[24, 25, 26, 27, 28]
00101111	[29, 30, 31]
00100001	[32, 33, 34, 35, 36, 37, 38, 39, 40, 41]
00101100	[42, 43]
00000101	[44, 45]
11101001	[46, 47, 48, 49, 50, 51]
11101111	[52]
11000111	[53, 54, 55]
11100011	[56, 57, 58, 59, 60]
11100001	[61, 62]
11100010	[63, 64, 65, 66, 67, 68, 69, 70]
11101010	[71, 72]
01100100	[73, 74]
11100101	[75, 76, 77, 78, 79, 80]
11000000	[81]
11000110	[82, 83]
01101100	[84, 85, 86, 87, 88, 89, 90, 91, 92]
01000000	[93, 94, 95, 96, 97, 98]
01001000	[99, 100, 101, 102, 103, 104, 105]
01001110	[106, 107]
01100111	[108, 109, 110, 111, 112, 113, 114]
01001111	[115, 116, 117, 118, 119, 120, 121]
01010100	[122, 123, 124, 125, 126, 127]
11111100	[128, 129, 130, 131, 132]
01010000	[133]
01110110	[134, 135]
11110101	[136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147]
11111000	[148, 149, 150, 151, 152, 153]
00111000	[154, 155]
01011010	[156, 157, 158, 159, 160, 161, 162, 163, 164, 165]
01011011	[166, 167, 168, 169]
11110001	[170, 171, 172, 173, 174, 175, 176, 177]
00011110	[178, 179]
00110111	[180, 181]
00011111	[182, 183, 184, 185, 186, 187, 188]
00010111	[189, 190, 191, 192, 193, 194, 195, 196]
11011010	[197, 198, 199, 200, 201, 202, 203]
10011001	[204, 205, 206, 207]
10111110	[208, 209]
10110001	[210, 211, 212, 213, 214, 215, 216]
10110000	[217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230]
10111001	[231]
10111101	[232, 233, 234, 235, 236, 237, 238, 239]
10000010	[240, 241, 242, 243, 244, 245, 246, 247]
10001010	[248, 249]
10001011	[250]
10001111	[251, 252, 253]

10001110	254	True
10000111	9	False
10100111	11	True
10000100	17	False
00000111	14	False
00000000	27	True
00101111	22	False
00100001	41	True
00101100	38	False
00000101	44	True
11101001	50	True
11101111	50	False
11000111	54	True
11100011	55	False
11100001	62	True
11100010	64	True
11101010	67	False
01100100	70	False
11100101	69	False
11000000	85	False
11000110	79	False
01101100	82	False
01000000	99	False
01001000	107	False
01001110	106	True
01100111	104	False
01001111	118	True
01010100	124	True
11111100	130	True
01010000	134	False
01110110	136	False
11110101	139	True
11111000	150	True
00111000	154	True
01011010	159	True
01011011	172	False
11110001	163	False
00011110	178	True
00110111	179	False
00011111	184	True
00010111	190	True
11011010	202	True
10011001	209	False
10111110	217	False
10110001	223	False
10110000	226	True
10111001	231	True
10111101	233	True
10000010	242	True
10001010	247	False
10001011	251	False
10001111	249	False

Number of codes used=52

Iteration=    160000 training nets give:
alice_loss.item()=0.04270949959754944	bob_loss.item()=0.2950168251991272

10001000	[0, 1, 2, 3, 4, 5, 6, 7, 254, 255]
10100111	[8, 9, 10, 11, 12, 13]
00101011	[14, 15, 16, 17]
00100010	[18, 19, 20, 21, 22, 23]
00100011	[24]
10101100	[25, 26, 27, 28, 29, 30, 31, 32, 33, 34]
00001100	[35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]
11000010	[47, 48, 49, 50]
11001000	[51, 52]
01001001	[53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67]
11001110	[68, 69, 70]
11100101	[71, 72, 73, 74]
11000100	[75, 76, 77, 78]
01000001	[79, 80]
11000000	[81, 82, 83, 84, 85, 86, 87, 88]
01100000	[89, 90, 91, 92]
01101010	[93, 94, 95]
01100011	[96, 97, 98, 99, 100, 101, 102]
01100010	[103, 104, 105, 106, 107, 108, 109]
01101111	[110, 111, 112, 113, 114]
11010100	[115, 116, 117]
01001111	[118]
01000010	[119, 120, 121, 122]
01111100	[123, 124, 125, 126, 127, 128]
11011100	[129, 130, 131, 132, 133, 134, 135, 136, 137, 138]
11110000	[139, 140, 141, 142, 143, 144, 145, 146]
11111010	[147, 148, 149]
01111011	[150, 151, 152, 153, 154]
01011111	[155, 156, 157, 158, 159, 160, 161]
00111101	[162, 163, 164, 165, 166]
11110001	[167, 168, 169]
11111001	[170, 171, 172, 173, 174, 175, 176]
00111010	[177]
00011110	[178]
00011000	[179, 180, 181, 182, 183, 184, 185, 186, 187]
00010101	[188]
00010111	[189, 190, 191, 192]
10110010	[193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205]
10110011	[206, 207, 208, 209, 210, 211, 212, 213]
10110001	[214, 215, 216, 217, 218]
10111010	[219, 220, 221, 222, 223, 224, 225, 226, 227, 228]
10010000	[229, 230, 231, 232, 233]
10111100	[234, 235, 236, 237, 238, 239]
10000001	[240, 241, 242, 243, 244, 245, 246]
10101011	[247, 248, 249, 250, 251, 252, 253]

10001000	1	True
10100111	7	False
00101011	12	False
00100010	26	False
00100011	19	False
10101100	35	False
00001100	44	True
11000010	45	False
11001000	46	False
01001001	68	False
11001110	66	False
11100101	78	False
11000100	89	False
01000001	86	False
11000000	97	False
01100000	89	True
01101010	98	False
01100011	97	True
01100010	105	True
01101111	117	False
11010100	116	True
01001111	113	False
01000010	125	False
01111100	115	False
11011100	134	True
11110000	142	True
11111010	153	False
01111011	156	False
01011111	166	False
00111101	160	False
11110001	176	False
11111001	167	False
00111010	179	False
00011110	183	False
00011000	182	True
00010101	184	False
00010111	192	True
10110010	211	False
10110011	202	False
10110001	203	False
10111010	220	True
10010000	228	False
10111100	236	True
10000001	250	False
10101011	253	True

Number of codes used=45

Iteration=    170000 training nets give:
alice_loss.item()=0.02060001902282238	bob_loss.item()=0.2721686363220215

10001010	[0, 254, 255]
10101111	[1]
10101110	[2, 3, 4, 5, 6]
10000111	[7]
10001100	[8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]
00100011	[19]
10100000	[20, 21, 22, 23, 24, 25, 26]
10101100	[27, 28]
00001001	[29, 30, 31, 32, 33, 34, 35]
00101001	[36, 37, 38, 39, 40, 41]
11000001	[42, 43]
00100101	[44]
00001100	[45]
11001000	[46]
00000100	[47, 48, 49, 50, 51, 52, 53, 54, 55, 56]
11000111	[57, 58, 59, 60, 61, 62, 63, 64, 65, 66]
11100110	[67, 68, 69, 70]
01100100	[71, 72, 73]
01101001	[74, 75, 76, 77, 78, 79, 80, 81]
01101100	[82]
11000000	[83, 84, 85, 86, 87, 88, 89, 90]
01101000	[91, 92, 93, 94, 95, 96, 97, 98, 99]
01000000	[100, 101, 102, 103, 104, 105]
01000111	[106, 107, 108, 109, 110, 111, 112]
01001111	[113, 114, 115, 116, 117, 118, 119, 120]
01000010	[121, 122, 123, 124, 125]
01011100	[126]
11011100	[127, 128, 129, 130]
00110100	[131, 132, 133, 134, 135, 136, 137, 138]
11110101	[139, 140]
11111111	[141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154]
00110110	[155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171]
01010111	[172, 173, 174]
00111010	[175, 176, 177, 178, 179, 180]
00110010	[181]
00011111	[182, 183, 184, 185, 186, 187]
01010011	[188]
00010111	[189, 190, 191, 192]
10111111	[193, 194, 195, 196, 197]
10011010	[198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210]
10010111	[211]
10111110	[212, 213]
10010010	[214, 215, 216, 217, 218, 219]
10111011	[220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231]
10111001	[232, 233]
10111100	[234, 235, 236, 237]
10110100	[238, 239, 240, 241, 242, 243, 244]
10000010	[245]
10100011	[246]
10001111	[247, 248, 249, 250, 251, 252, 253]

10001010	254	True
10101111	4	False
10101110	2	True
10000111	8	False
10001100	19	False
00100011	18	False
10100000	18	False
10101100	18	False
00001001	35	True
00101001	33	False
11000001	42	True
00100101	46	False
00001100	35	False
11001000	44	False
00000100	45	False
11000111	64	True
11100110	64	False
01100100	76	False
01101001	77	True
01101100	79	False
11000000	52	False
01101000	92	True
01000000	109	False
01000111	115	False
01001111	114	True
01000010	125	True
01011100	126	True
11011100	128	True
00110100	140	False
11110101	147	False
11111111	166	False
00110110	156	True
01010111	169	False
00111010	179	True
00110010	181	True
00011111	185	True
01010011	184	False
00010111	192	True
10111111	197	True
10011010	213	False
10010111	208	False
10111110	214	False
10010010	214	True
10111011	224	True
10111001	229	False
10111100	234	True
10110100	239	True
10000010	245	True
10100011	247	False
10001111	251	True

Number of codes used=50

Iteration=    180000 training nets give:
alice_loss.item()=0.004581683315336704	bob_loss.item()=0.12909820675849915

10001110	[0, 1, 2, 3, 252, 253, 254, 255]
10101110	[4, 5, 6]
00001010	[7, 8, 9, 10]
10101101	[11, 12, 13, 14, 15, 16, 17, 18]
10101001	[19, 20]
00100011	[21, 22, 23]
00000001	[24, 25, 26, 27, 28, 29, 30]
00100001	[31, 32, 33, 34, 35, 36, 37]
11001101	[38, 39, 40, 41]
00001101	[42, 43, 44, 45, 46, 47, 48]
00000100	[49, 50, 51, 52]
11001100	[53, 54, 55, 56]
11101000	[57, 58, 59, 60, 61, 62, 63, 64, 65, 66]
11101010	[67, 68, 69, 70]
01100100	[71, 72, 73, 74, 75, 76, 77]
01001101	[78, 79, 80]
01000100	[81, 82, 83, 84]
01000101	[85, 86]
01001100	[87, 88, 89, 90, 91]
01101010	[92, 93, 94, 95, 96, 97, 98]
01001000	[99, 100, 101, 102, 103, 104, 105]
01001010	[106, 107, 108, 109, 110, 111]
01001111	[112, 113, 114, 115, 116, 117, 118]
01000011	[119, 120, 121, 122, 123, 124, 125]
11011100	[126, 127, 128, 129, 130, 131]
01010000	[132]
00110100	[133, 134, 135, 136, 137, 138, 139, 140]
01111110	[141, 142, 143, 144, 145]
01111010	[146, 147]
01011110	[148, 149]
01110111	[150, 151, 152, 153]
11111010	[154]
01011101	[155, 156, 157, 158, 159, 160]
11110011	[161, 162]
01010010	[163, 164, 165, 166, 167, 168]
11111001	[169, 170, 171]
00111110	[172, 173, 174, 175, 176, 177, 178, 179]
00010101	[180, 181, 182]
00010110	[183, 184, 185, 186]
00010001	[187, 188, 189, 190, 191]
00010111	[192, 193]
00011001	[194, 195, 196, 197, 198, 199, 200]
11010011	[201, 202, 203, 204, 205, 206, 207]
00010010	[208, 209, 210]
10110110	[211, 212, 213, 214, 215]
10010001	[216, 217, 218, 219]
10011001	[220, 221, 222, 223, 224, 225, 226, 227, 228]
10010000	[229, 230]
10111001	[231, 232, 233, 234, 235, 236]
10110100	[237, 238, 239, 240, 241, 242]
10000010	[243, 244, 245, 246, 247, 248]
10001000	[249, 250, 251]

10001110	0	True
10101110	4	True
00001010	10	True
10101101	12	True
10101001	18	False
00100011	21	True
00000001	24	True
00100001	34	True
11001101	41	True
00001101	37	False
00000100	49	True
11001100	51	False
11101000	59	True
11101010	63	False
01100100	78	False
01001101	78	True
01000100	82	True
01000101	81	False
01001100	90	True
01101010	97	True
01001000	100	True
01001010	106	True
01001111	113	True
01000011	118	False
11011100	121	False
01010000	132	True
00110100	140	True
01111110	144	True
01111010	147	True
01011110	148	True
01110111	151	True
11111010	145	False
01011101	159	True
11110011	165	False
01010010	158	False
11111001	170	True
00111110	172	True
00010101	181	True
00010110	180	False
00010001	182	False
00010111	192	True
00011001	199	True
11010011	200	False
00010010	200	False
10110110	218	False
10010001	209	False
10011001	224	True
10010000	225	False
10111001	232	True
10110100	242	True
10000010	248	True
10001000	255	False

Number of codes used=52

Iteration=    190000 training nets give:
alice_loss.item()=0.003998111933469772	bob_loss.item()=0.09864726662635803

10001000	[0, 1, 2, 3, 4, 5, 6, 7, 8, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00101011	[9, 10, 11, 12, 13, 14, 15, 16]
10100001	[17, 18, 19]
10101000	[20, 21, 22]
00000001	[23, 24, 25, 26, 27, 28, 29]
00101101	[30, 31, 32, 33, 34]
11000001	[35, 36, 37, 38, 39, 40, 41, 42]
11001101	[43]
00000101	[44]
11001010	[45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58]
11000111	[59, 60, 61, 62, 63, 64]
01001001	[65, 66, 67]
11100100	[68, 69, 70]
01100001	[71, 72]
11100101	[73]
11000100	[74, 75, 76, 77, 78, 79]
01000100	[80, 81, 82, 83, 84, 85, 86]
01100000	[87, 88]
01101000	[89, 90, 91, 92]
01101010	[93]
01000110	[94, 95, 96, 97, 98, 99, 100, 101]
01101111	[102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113]
01001111	[114, 115, 116, 117]
11110100	[118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130]
01011000	[131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141]
11110110	[142, 143, 144, 145, 146]
11010000	[147]
11011000	[148, 149, 150, 151, 152, 153, 154, 155, 156, 157]
11011110	[158, 159, 160]
01111111	[161, 162]
00110101	[163, 164, 165, 166, 167]
01010111	[168, 169]
11110001	[170, 171, 172, 173, 174, 175, 176, 177]
00110010	[178, 179]
00110111	[180, 181, 182, 183]
00010000	[184, 185, 186, 187, 188, 189, 190, 191, 192, 193]
00010111	[194]
10111111	[195, 196, 197, 198, 199, 200, 201]
00010010	[202]
00011010	[203, 204]
10010111	[205]
10010010	[206, 207]
10011111	[208, 209, 210, 211, 212]
10111110	[213, 214, 215, 216, 217, 218]
10010110	[219, 220, 221, 222]
10110000	[223, 224, 225, 226, 227, 228, 229]
10111100	[230, 231, 232, 233, 234, 235, 236, 237]
10110100	[238, 239, 240, 241, 242, 243, 244, 245]

10001000	13	False
00101011	14	True
10100001	14	False
10101000	18	False
00000001	22	False
00101101	33	True
11000001	38	True
11001101	42	False
00000101	41	False
11001010	56	True
11000111	50	False
01001001	68	False
11100100	70	True
01100001	71	True
11100101	69	False
11000100	78	True
01000100	92	False
01100000	85	False
01101000	90	True
01101010	95	False
01000110	95	True
01101111	107	True
01001111	116	True
11110100	131	False
01011000	133	True
11110110	148	False
11010000	148	False
11011000	143	False
11011110	166	False
01111111	157	False
00110101	164	True
01010111	167	False
11110001	177	True
00110010	184	False
00110111	173	False
00010000	186	True
00010111	188	False
10111111	197	True
00010010	199	False
00011010	188	False
10010111	203	False
10010010	206	True
10011111	206	False
10111110	213	True
10010110	220	True
10110000	226	True
10111100	236	True
10110100	245	True

Number of codes used=48

Iteration=    200000 training nets give:
alice_loss.item()=0.004852445796132088	bob_loss.item()=0.33680248260498047

10000000	[0, 1, 2, 3, 4, 5, 250, 251, 252, 253, 254, 255]
10100111	[6]
10001000	[7, 8, 9, 10, 11, 12, 13, 14]
10101101	[15]
10100100	[16, 17]
10000100	[18, 19, 20, 21, 22, 23, 24]
00100010	[25, 26]
00100110	[27, 28, 29, 30]
00101000	[31, 32, 33, 34]
11000001	[35, 36, 37, 38]
00000101	[39, 40, 41, 42, 43, 44, 45, 46, 47]
11001011	[48, 49, 50, 51, 52, 53, 54, 55, 56, 57]
11101011	[58, 59, 60, 61, 62, 63]
11100110	[64, 65]
11100100	[66, 67, 68, 69, 70, 71]
01000001	[72, 73, 74, 75, 76, 77, 78, 79, 80, 81]
01100000	[82, 83, 84, 85, 86, 87, 88]
01100110	[89, 90]
01101010	[91, 92, 93, 94, 95]
01100011	[96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106]
01000000	[107, 108, 109, 110, 111, 112]
01111000	[113, 114, 115]
01000011	[116, 117, 118, 119, 120, 121, 122, 123, 124, 125]
01011100	[126]
11110100	[127, 128, 129, 130, 131, 132]
11111100	[133, 134, 135]
01110110	[136, 137]
01110101	[138, 139, 140, 141]
01111010	[142, 143, 144]
01111001	[145, 146, 147]
01111011	[148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162]
11010110	[163, 164, 165, 166, 167, 168, 169, 170]
11011111	[171, 172, 173, 174, 175, 176]
00111010	[177, 178, 179, 180, 181, 182, 183, 184]
00111001	[185, 186, 187, 188, 189]
00010111	[190, 191, 192, 193]
10111111	[194, 195, 196, 197, 198, 199, 200, 201]
00011010	[202]
10010111	[203]
10011111	[204, 205, 206, 207, 208, 209, 210, 211]
10011011	[212, 213, 214]
10110001	[215, 216, 217, 218]
10110101	[219, 220, 221, 222, 223, 224, 225, 226, 227]
10011101	[228, 229, 230]
10111001	[231, 232, 233, 234, 235, 236, 237, 238]
10010100	[239, 240, 241, 242, 243, 244, 245, 246]
10000010	[247, 248, 249]

10000000	5	True
10100111	1	False
10001000	14	True
10101101	17	False
10100100	16	True
10000100	23	True
00100010	22	False
00100110	32	False
00101000	36	False
11000001	40	False
00000101	41	True
11001011	49	True
11101011	62	True
11100110	59	False
11100100	68	True
01000001	75	True
01100000	81	False
01100110	89	True
01101010	92	True
01100011	96	True
01000000	96	False
01111000	117	False
01000011	120	True
01011100	125	False
11110100	134	False
11111100	137	False
01110110	138	False
01110101	147	False
01111010	146	False
01111001	150	False
01111011	162	True
11010110	171	False
11011111	179	False
00111010	185	False
00111001	191	False
00010111	196	False
10111111	197	True
00011010	202	True
10010111	206	False
10011111	204	True
10011011	214	True
10110001	217	True
10110101	221	True
10011101	233	False
10111001	232	True
10010100	241	True
10000010	246	False

Number of codes used=47

Iteration=    210000 training nets give:
alice_loss.item()=0.014354517683386803	bob_loss.item()=0.2938806414604187

10001001	[0, 1, 2, 3, 4, 5, 6, 7, 252, 253, 254, 255]
10001101	[8, 9]
10100100	[10, 11, 12, 13, 14, 15, 16, 17, 18]
10000100	[19, 20]
10101100	[21, 22, 23, 24, 25, 26, 27]
00101110	[28]
00100001	[29, 30, 31, 32, 33, 34, 35, 36]
00001001	[37, 38, 39, 40, 41, 42, 43, 44, 45]
11000111	[46, 47, 48]
11000110	[49, 50, 51, 52]
11001100	[53, 54, 55, 56, 57, 58]
11101110	[59, 60, 61, 62, 63, 64, 65, 66, 67, 68]
11100100	[69]
01100001	[70, 71, 72]
11000100	[73, 74, 75, 76, 77]
01000101	[78, 79, 80, 81]
01100100	[82, 83, 84, 85, 86, 87, 88, 89, 90]
01101010	[91, 92, 93, 94, 95, 96]
01101110	[97, 98, 99]
01100010	[100, 101]
01101111	[102, 103, 104, 105, 106, 107, 108]
01100111	[109, 110, 111, 112]
01001111	[113, 114]
01111000	[115, 116, 117, 118, 119]
01011100	[120, 121, 122, 123, 124, 125, 126, 127]
01010000	[128, 129, 130, 131, 132]
11111100	[133, 134, 135]
11111110	[136, 137, 138, 139, 140, 141, 142, 143, 144, 145]
01111010	[146]
01110111	[147, 148, 149, 150, 151, 152, 153, 154]
01011101	[155]
01111011	[156, 157, 158, 159, 160, 161, 162, 163, 164]
11011101	[165, 166, 167, 168, 169]
11110011	[170, 171]
01010001	[172, 173]
11010101	[174, 175, 176, 177]
11110001	[178]
00010001	[179, 180, 181, 182, 183, 184, 185, 186]
00111001	[187, 188, 189, 190, 191, 192, 193, 194]
11011010	[195, 196]
11010010	[197, 198]
11011011	[199]
11011001	[200, 201, 202, 203, 204, 205, 206, 207]
10010010	[208, 209, 210, 211, 212, 213, 214, 215, 216]
10110001	[217]
10111011	[218, 219, 220, 221, 222, 223]
10110110	[224, 225, 226]
10011101	[227, 228, 229, 230, 231, 232, 233, 234]
10011100	[235, 236]
10111100	[237, 238, 239, 240, 241, 242, 243, 244]
10000010	[245, 246, 247, 248, 249, 250]
10000110	[251]

10001001	7	True
10001101	7	False
10100100	14	True
10000100	24	False
10101100	18	False
00101110	36	False
00100001	33	True
00001001	34	False
11000111	42	False
11000110	52	True
11001100	60	False
11101110	60	True
11100100	64	False
01100001	70	True
11000100	71	False
01000101	79	True
01100100	87	True
01101010	90	False
01101110	94	False
01100010	97	False
01101111	110	False
01100111	109	True
01001111	111	False
01111000	115	True
01011100	121	True
01010000	124	False
11111100	135	True
11111110	147	False
01111010	147	False
01110111	150	True
01011101	158	False
01111011	165	False
11011101	171	False
11110011	165	False
01010001	172	True
11010101	171	False
11110001	177	False
00010001	185	True
00111001	192	True
11011010	193	False
11010010	193	False
11011011	201	False
11011001	201	True
10010010	208	True
10110001	219	False
10111011	221	True
10110110	225	True
10011101	237	False
10011100	230	False
10111100	240	True
10000010	248	True
10000110	254	False

Number of codes used=52

Iteration=    220000 training nets give:
alice_loss.item()=0.008634038269519806	bob_loss.item()=0.1386353075504303

10001001	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
10100000	[10, 11]
10100101	[12, 13, 14, 15, 16, 17, 18]
00100010	[19, 20, 21, 22, 23, 24, 25, 26, 27, 28]
00101001	[29, 30, 31, 32]
00100000	[33, 34, 35, 36, 37]
00001100	[38, 39, 40, 41, 42, 43, 44]
11000111	[45, 46, 47]
11001101	[48, 49, 50, 51]
11001110	[52, 53, 54, 55, 56]
01001001	[57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]
11100101	[73, 74, 75, 76, 77]
01101001	[78]
01001101	[79, 80]
01101101	[81, 82, 83, 84, 85, 86]
01000110	[87, 88, 89, 90, 91]
01101010	[92, 93, 94, 95]
01100010	[96, 97, 98, 99]
01001010	[100, 101, 102, 103, 104, 105, 106]
01001011	[107, 108, 109, 110, 111, 112]
01100111	[113]
01000011	[114, 115, 116, 117, 118]
01011100	[119]
11010100	[120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132]
11110100	[133, 134]
11111100	[135]
01111110	[136, 137, 138, 139, 140]
00110100	[141, 142, 143, 144, 145, 146]
01111010	[147, 148]
01111001	[149, 150, 151]
01110011	[152, 153, 154, 155, 156, 157, 158, 159]
01011010	[160, 161]
01011001	[162, 163]
01010111	[164, 165, 166, 167, 168, 169]
00011101	[170]
01010001	[171, 172, 173]
00111110	[174, 175, 176, 177, 178, 179]
00011111	[180]
00110111	[181, 182]
00110011	[183, 184, 185]
00010001	[186, 187, 188, 189, 190]
00111001	[191, 192]
10111111	[193, 194]
00010110	[195, 196, 197, 198]
11011011	[199, 200, 201, 202, 203, 204, 205, 206]
10011111	[207, 208, 209]
10110011	[210]
10111010	[211, 212, 213, 214, 215, 216, 217, 218, 219, 220]
10010110	[221, 222, 223, 224, 225, 226, 227, 228, 229, 230]
10111001	[231, 232, 233, 234, 235, 236]
10011100	[237]
10010100	[238, 239, 240, 241]
10101011	[242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253]
10100011	[254, 255]

10001001	2	True
10100000	12	False
10100101	20	False
00100010	24	True
00101001	33	False
00100000	32	False
00001100	38	True
11000111	42	False
11001101	53	False
11001110	49	False
01001001	72	True
11100101	74	True
01101001	84	False
01001101	94	False
01101101	87	False
01000110	93	False
01101010	89	False
01100010	97	True
01001010	101	True
01001011	112	True
01100111	106	False
01000011	115	True
01011100	122	False
11010100	132	True
11110100	133	True
11111100	139	False
01111110	141	False
00110100	145	True
01111010	147	True
01111001	151	True
01110011	156	True
01011010	163	False
01011001	165	False
01010111	166	True
00011101	169	False
01010001	171	True
00111110	177	True
00011111	177	False
00110111	179	False
00110011	174	False
00010001	186	True
00111001	185	False
10111111	194	True
00010110	197	True
11011011	197	False
10011111	206	False
10110011	213	False
10111010	216	True
10010110	231	False
10111001	228	False
10011100	233	False
10010100	244	False
10101011	245	True
10100011	254	True

Number of codes used=54

Iteration=    230000 training nets give:
alice_loss.item()=0.003232107497751713	bob_loss.item()=0.18630735576152802

10000000	[0, 1, 2, 249, 250, 251, 252, 253, 254, 255]
10001001	[3, 4, 5]
10100001	[6, 7, 8, 9, 10, 11, 12, 13]
10100101	[14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
00000110	[26, 27, 28, 29, 30, 31, 32]
00101001	[33, 34, 35, 36]
00001101	[37, 38, 39, 40, 41, 42, 43]
11000111	[44]
11000010	[45]
11100011	[46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56]
11001000	[57, 58, 59, 60, 61, 62, 63]
11100100	[64, 65, 67, 68]
11000000	[66]
01100001	[69, 70, 71]
11101101	[72]
11100101	[73, 74, 75, 76, 77, 78]
01000110	[79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]
01101100	[90, 91, 92, 93]
01100010	[94]
01000000	[95, 96, 97, 98]
01001010	[99, 100, 101, 102, 103, 104, 105]
01001000	[106, 107]
01001011	[108, 109, 110, 111]
01101111	[112, 113]
01001111	[114]
01000111	[115]
01010100	[116, 117]
01011100	[118, 119, 120, 121, 122, 123, 124]
01110100	[125]
01010000	[126, 127, 128, 129]
11110100	[130, 131, 132, 133]
01110110	[134, 135, 136, 137, 138, 139, 140]
11111000	[141]
01110010	[142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152]
11111101	[153, 154, 155, 156]
01010101	[157, 158, 159, 160, 161, 162]
01011001	[163, 164, 165, 166]
11111111	[167, 168]
11111001	[169, 170, 171, 172]
00111111	[173, 174, 175, 176, 177, 178, 179, 180, 181]
01010011	[182, 183, 184, 185, 186]
00111011	[187]
00111001	[188]
11010111	[189, 190, 191]
10111111	[192, 193, 194, 195, 196]
00011010	[197, 198, 199, 200, 201, 202]
10011010	[203, 204, 205, 206]
10010111	[207, 208]
10010010	[209, 210, 211, 212, 213, 214, 215, 216, 217]
10110101	[218, 219, 220, 221, 222, 223, 224]
10011110	[225]
10110110	[226, 227, 228]
10011100	[229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239]
10010100	[240, 241, 242]
10101011	[243, 244, 245, 246, 247, 248]

10000000	248	False
10001001	8	False
10100001	17	False
10100101	22	True
00000110	22	False
00101001	39	False
00001101	35	False
11000111	43	False
11000010	42	False
11100011	49	True
11001000	72	False
11100100	67	True
11000000	66	True
01100001	71	True
11101101	76	False
11100101	76	True
01000110	83	True
01101100	85	False
01100010	92	False
01000000	97	True
01001010	98	False
01001000	105	False
01001011	111	True
01101111	104	False
01001111	109	False
01000111	113	False
01010100	125	False
01011100	120	True
01110100	126	False
01010000	136	False
11110100	132	True
01110110	135	True
11111000	142	False
01110010	141	False
11111101	150	False
01010101	156	False
01011001	167	False
11111111	166	False
11111001	169	True
00111111	174	True
01010011	180	False
00111011	179	False
00111001	193	False
11010111	191	True
10111111	203	False
00011010	194	False
10011010	203	True
10010111	207	True
10010010	211	True
10110101	226	False
10011110	227	False
10110110	228	True
10011100	232	True
10010100	241	True
10101011	248	True

Number of codes used=55

Iteration=    240000 training nets give:
alice_loss.item()=0.0021678274497389793	bob_loss.item()=0.15176507830619812

10001110	[0, 253, 254, 255]
10000101	[1, 2, 3, 4, 5, 6, 7, 8, 9]
10000111	[10, 11]
10100100	[12, 13, 14, 15, 16]
10000100	[17, 18, 19]
11000011	[20, 21, 22, 23, 24, 25, 26]
00101110	[27, 28, 29, 30, 31, 32, 33]
00100111	[34, 35]
00100001	[36, 37, 38, 39]
11000111	[40, 41, 42, 43, 44, 45, 46]
11100011	[47, 48, 49, 50, 51, 52]
11001101	[53, 54, 55, 56]
11100110	[57]
11100001	[58, 59]
11101100	[60, 61, 62, 63]
11000000	[64, 65, 66, 67, 68]
11000100	[69, 70]
01100001	[71, 72]
01001001	[73, 74]
11100101	[75, 76, 77, 78, 79]
01101001	[80, 81]
01000100	[82]
01101100	[83, 84, 85, 86, 87, 88, 89, 90, 91, 92]
01100010	[93, 94, 95]
01000000	[96]
01001010	[97, 98, 99, 100, 101, 102]
01111000	[103, 104, 105, 106, 107, 108, 109, 110, 111]
01110000	[112, 113, 114, 115]
01000011	[116]
01011100	[117, 118, 119, 120, 121, 122]
01110100	[123, 124, 125, 126, 127, 128, 129]
11110100	[130, 131, 132, 133, 134, 135]
01110110	[136, 137]
01111110	[138, 139, 140, 141, 142, 143, 144, 145, 146]
11111101	[147, 148, 149, 150, 151, 152, 153]
01010101	[154, 155, 156, 157, 158, 159, 160, 161, 162]
11111010	[163]
00011101	[164, 165, 166, 167, 168, 169]
11110011	[170, 171, 172, 173]
11010110	[174, 175, 176, 177, 178]
11010101	[179, 180]
11010001	[181, 182, 183, 184, 185]
00110001	[186, 187, 188]
11010010	[189, 190, 191, 192, 193, 194, 195]
10111111	[196, 197, 198, 199, 200, 201, 202, 203, 204, 205]
10010111	[206, 207, 208, 209, 210, 211, 212, 213]
10011011	[214]
10111011	[215, 216]
10110101	[217, 218, 219, 220, 221, 222, 223]
10010110	[224, 225, 226, 227, 228, 229, 230, 231, 232]
10011100	[233, 234]
10111100	[235, 236, 237, 238, 239, 240, 241]
10101011	[242, 243, 244, 245, 246, 247, 248, 249]
10110100	[250, 251]
10100011	[252]

10001110	1	False
10000101	19	False
10000111	8	False
10100100	16	True
10000100	20	False
11000011	36	False
00101110	34	False
00100111	31	False
00100001	37	True
11000111	39	False
11100011	48	True
11001101	58	False
11100110	47	False
11100001	57	False
11101100	68	False
11000000	68	True
11000100	65	False
01100001	70	False
01001001	75	False
11100101	76	True
01101001	76	False
01000100	81	False
01101100	84	True
01100010	92	False
01000000	99	False
01001010	100	True
01111000	106	True
01110000	136	False
01000011	116	True
01011100	123	False
01110100	126	True
11110100	135	True
01110110	136	True
01111110	143	True
11111101	154	False
01010101	156	True
11111010	163	True
00011101	176	False
11110011	170	True
11010110	176	True
11010101	178	False
11010001	188	False
00110001	189	False
11010010	196	False
10111111	200	True
10010111	215	False
10011011	214	True
10111011	216	True
10110101	222	True
10010110	225	True
10011100	230	False
10111100	228	False
10101011	250	False
10110100	250	True
10100011	255	False

Number of codes used=55

Iteration=    250000 training nets give:
alice_loss.item()=0.008645677007734776	bob_loss.item()=0.06942547112703323

10100010	[0, 251, 252, 253, 254, 255]
10101111	[1, 2, 3, 4, 5, 6, 7]
10000111	[8, 9, 10, 11, 12]
10101000	[13]
10101101	[14, 15, 16]
10000100	[17, 18]
10101001	[19, 20, 21, 22]
00000010	[23, 24, 25, 26]
00100110	[27, 28, 29, 30]
00001110	[31, 32, 33, 34, 35, 36, 37, 38, 39, 40]
00001111	[41, 42]
00000111	[43, 44, 45, 46]
11100011	[47, 48, 49, 50, 51, 52]
11001101	[53, 54, 55, 56]
11000101	[57]
11101011	[58, 59, 60, 61, 62, 63]
11101100	[64, 65, 66, 67]
01100001	[68, 69, 70, 71, 72]
01001001	[73, 74, 75, 76]
11100101	[77, 78, 79]
01101101	[80, 81, 82]
01001100	[83, 84, 85, 86, 87, 88, 89]
01100101	[90, 91, 92]
01001110	[93, 94, 95, 96, 97]
01101111	[98, 99, 100, 101, 102, 103]
01111000	[104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
01000011	[115, 116, 117, 118]
01011100	[119, 120, 121, 122]
01110100	[123, 124, 125, 126, 127, 128]
01010110	[129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139]
11110000	[140]
01111110	[141, 142, 143]
11011100	[144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158]
11110010	[159, 160, 161, 162, 163, 164, 165, 166, 167, 168]
00111000	[169, 170, 171, 172]
00010101	[173, 174, 175, 176]
11010101	[177, 178, 179, 180, 181]
01011011	[182, 183, 184, 185, 186, 187]
00010011	[188, 189, 190, 191, 192]
00111001	[193, 194]
00010110	[195, 196, 197, 198, 199, 204]
11011001	[200, 201, 202, 203]
10011010	[205, 206]
10110011	[207, 208, 209, 210, 211]
10010010	[212, 213, 214, 215, 216]
10111011	[217, 218]
10011001	[219, 220, 221, 222, 223, 224, 225, 226, 227]
10011101	[228, 229, 230, 231, 232, 233, 234, 235, 236, 237]
10010100	[238, 239, 240, 241, 242]
10110100	[243, 244, 245, 246, 247, 248, 249, 250]

10100010	250	False
10101111	3	True
10000111	5	False
10101000	252	False
10101101	13	False
10000100	19	False
10101001	19	True
00000010	21	False
00100110	23	False
00001110	27	False
00001111	37	False
00000111	45	True
11100011	50	True
11001101	55	True
11000101	60	False
11101011	60	True
11101100	66	True
01100001	67	False
01001001	76	True
11100101	77	True
01101101	82	True
01001100	89	True
01100101	83	False
01001110	98	False
01101111	99	True
01111000	111	True
01000011	113	False
01011100	120	True
01110100	126	True
01010110	129	True
11110000	137	False
01111110	142	True
11011100	158	True
11110010	162	True
00111000	174	False
00010101	168	False
11010101	176	False
01011011	174	False
00010011	189	True
00111001	193	True
00010110	200	False
11011001	195	False
10011010	207	False
10110011	210	True
10010010	219	False
10111011	214	False
10011001	223	True
10011101	226	False
10010100	241	True
10110100	245	True

Number of codes used=50

Iteration=    260000 training nets give:
alice_loss.item()=0.018344100564718246	bob_loss.item()=0.08724136650562286

10000001	[0, 1, 2, 3, 4, 249, 250, 251, 252, 253, 254, 255]
10000011	[5]
10101110	[6]
10000101	[7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
10100001	[17, 18]
00000110	[19, 20, 21, 22, 23, 24, 25, 26]
00100110	[27, 28]
00001110	[29, 30, 31, 32, 33]
00000011	[34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]
11100010	[45, 46, 47]
11101110	[48, 49, 50, 51, 52, 53, 54, 55]
11000101	[56, 57, 58, 59, 60, 61, 62, 63]
11101100	[64, 65, 66, 67]
01100001	[68, 69]
01101001	[70, 71, 72, 73]
11101101	[74, 75, 76, 77, 78, 79]
01101000	[80, 81, 82]
01100101	[83, 84, 85, 86, 87, 88, 89, 90, 91]
01100011	[92, 93, 94]
01101111	[95, 96, 97, 98, 99, 100, 101, 102, 103, 104]
01111000	[105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116]
01000011	[117]
01011100	[118, 119, 120, 121, 122, 123, 124, 125]
01110100	[126, 127, 128, 129]
01010110	[130, 131, 132, 133, 134, 135, 136, 137]
01110110	[138]
11110101	[139, 140, 141, 142, 143]
11110111	[144, 145, 146, 147, 148]
01110011	[149, 150, 151, 152, 153, 154, 155, 156, 157]
00110101	[158, 159, 160, 161, 162, 163, 164, 165]
01011010	[166, 167]
11110011	[168, 169, 170, 171, 172, 173, 174, 175, 176, 177]
00110011	[178]
11010101	[179]
11011101	[180, 181, 182]
00010001	[183, 184, 185, 186]
00110001	[187, 188, 189, 190, 191, 192]
11011011	[193, 194, 195, 196, 197, 198, 199, 200]
00010010	[201, 202, 203, 204, 205, 206, 207, 208, 209]
10010011	[210, 211, 212, 213, 214, 215, 216, 217, 218, 219]
10010101	[220, 221, 222, 223, 224, 225, 226]
10111100	[227, 228, 229, 230, 231, 232, 233]
10110110	[234]
10011100	[235, 236, 237]
10010100	[238, 239, 240, 241, 242]
10001010	[243, 244, 245, 246, 247, 248]

10000001	255	True
10000011	8	False
10101110	3	False
10000101	5	False
10100001	19	False
00000110	24	True
00100110	25	False
00001110	36	False
00000011	56	False
11100010	53	False
11101110	57	False
11000101	56	True
11101100	66	True
01100001	66	False
01101001	72	True
11101101	77	True
01101000	82	True
01100101	90	True
01100011	91	False
01101111	97	True
01111000	114	True
01000011	114	False
01011100	120	True
01110100	126	True
01010110	137	True
01110110	136	False
11110101	140	True
11110111	147	True
01110011	148	False
00110101	159	True
01011010	168	False
11110011	172	True
00110011	170	False
11010101	172	False
11011101	180	True
00010001	189	False
00110001	192	True
11011011	194	True
00010010	202	True
10010011	213	True
10010101	227	False
10111100	226	False
10110110	233	False
10011100	238	False
10010100	243	False
10001010	243	True

Number of codes used=46

****WARNING NOT CLOSED - MAY BE DUE TO ERROR***
