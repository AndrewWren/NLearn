The closed log for run 21-05-09_17:56:02BST

SMOOTHING_LENGTH = 10000
SAVE_PERIOD = 100000
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # Used: "cuda"
MODEL_FOLDER = 'models'
CONFIGS_FOLDER = 'configs'
LOGS_FOLDER = 'logs'

hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': [(714844, 936892, 888616, 165835), (508585, 487266, 751926, 247136), (843402, 443788, 742412, 270619), (420915, 961830, 723900, 510954)],
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': ['SGD(lr=0.01)'],
	'BOB_OPTIMIZER': [('SGD', '{"lr": 0.01}')],
	'ALICE_LOSS_FUNCTION': ['Huber(beta=0.1)'],
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': [None, 500],
	'N_CODE': 8,
	'N_NUMBERS': 256
}



>>>> hp_run=1 of 8
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (714844, 936892, 888616, 165835),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=     20000 training nets give:
alice_loss.item()=0.7213364243507385	bob_loss.item()=0.5182915329933167

00100111	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11000111	[75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88]
00010001	[89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193]
01000001	[120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154]

00100111	156	False
11000111	4	False
00010001	197	False
01000001	10	False

Number of codes used=4

Iteration=     30000 training nets give:
alice_loss.item()=0.199901282787323	bob_loss.item()=0.20646370947360992

10101010	[0, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01100000	[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54]
01011110	[55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86]
00111101	[87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157]
00010001	[158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189]
00100111	[190, 191, 192, 193, 194, 195, 196]

10101010	238	True
01100000	14	True
01011110	39	False
00111101	123	True
00010001	145	False
00100111	245	False

Number of codes used=6

Iteration=     40000 training nets give:
alice_loss.item()=0.1216757521033287	bob_loss.item()=0.15919843316078186

00101010	[0, 1, 2, 3, 4, 5, 6, 7, 8, 248, 249, 250, 251, 252, 253, 254, 255]
01100000	[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]
11101010	[28, 29, 30]
01111110	[31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55]
01000110	[56, 57, 58, 59]
01010110	[60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86]
00011110	[87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102]
10010000	[103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
00111001	[115, 116, 117, 118, 119]
00111101	[120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136]
00010011	[137, 138, 139, 140, 141, 142]
00010000	[143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170]
10010001	[171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201]
10101011	[202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221]
10101110	[222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241]
10100011	[242, 243, 244, 245, 246]
01100010	[247]

00101010	236	False
01100000	2	False
11101010	250	False
01111110	52	True
01000110	68	False
01010110	72	True
00011110	103	False
10010000	158	False
00111001	122	False
00111101	131	True
00010011	149	False
00010000	153	True
10010001	171	True
10101011	223	False
10101110	223	True
10100011	234	False
01100010	9	False

Number of codes used=17

Iteration=     50000 training nets give:
alice_loss.item()=0.07295537739992142	bob_loss.item()=0.06679512560367584

01100010	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 249, 250, 251, 252, 253, 254, 255]
01100000	[21, 22, 23]
01001100	[24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]
01011110	[49, 50, 51, 52, 53, 54]
01000110	[55, 56, 57, 58]
01010010	[59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]
01010110	[70, 71, 72, 73]
11010110	[74, 75, 76, 77, 78, 79, 80]
00000110	[81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96]
01011111	[97, 98, 99, 100, 101, 102, 103, 104, 105, 106]
00111110	[107, 108, 109, 110, 111, 112, 113, 114, 115]
00111001	[116, 117, 118, 119, 120, 121, 122, 123, 124]
00111101	[125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137]
00010101	[138, 139, 140, 141]
00110000	[142, 143, 144, 145, 146, 147, 148]
00010011	[149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160]
10010001	[161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178]
00110011	[179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190]
10001000	[191, 192, 193, 194, 195]
10011010	[196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]
00101011	[210, 211, 212, 213, 214, 215, 216, 217, 218]
10101110	[219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231]
10100011	[232, 233, 234, 235, 236, 237, 238]
10000010	[239, 240, 241, 242, 243, 244, 245, 246, 247, 248]

01100010	2	True
01100000	5	False
01001100	41	True
01011110	76	False
01000110	64	False
01010010	67	True
01010110	80	False
11010110	77	True
00000110	72	False
01011111	106	True
00111110	112	True
00111001	132	False
00111101	133	True
00010101	140	True
00110000	164	False
00010011	156	True
10010001	165	True
00110011	173	False
10001000	209	False
10011010	208	True
00101011	218	True
10101110	230	True
10100011	229	False
10000010	241	True

Number of codes used=24

Iteration=     60000 training nets give:
alice_loss.item()=0.03084937483072281	bob_loss.item()=0.02553550899028778

01100010	[0, 1, 2, 252, 253, 254, 255]
01100000	[3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
01101010	[13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
01101110	[24]
01001100	[25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
01011010	[52, 53, 54, 55, 56, 57, 58, 59, 60]
01001110	[61, 62, 63, 64, 65]
01010010	[66, 67, 68, 69, 70, 71, 72]
11010110	[73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84]
00011010	[85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96]
01011101	[97, 98, 99, 100, 101, 102, 103]
01111101	[104, 105, 106, 107]
10010110	[108, 109, 110, 111, 112]
00111110	[113, 114]
10011101	[115, 116, 117, 118, 119, 120]
10111100	[121, 122]
00110101	[123, 124, 125, 126, 127]
00111101	[128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138]
10110101	[139, 140, 141]
10111001	[142, 143, 144, 145, 146, 147, 148]
00010011	[149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160]
10010001	[161, 167, 168, 169, 170, 171, 172]
10010011	[162, 163, 164, 165, 166]
00110010	[173, 174, 175]
10110010	[176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191]
10011011	[192, 193, 194, 195]
10111011	[196, 197, 198, 199, 200, 201]
10001111	[202]
10011010	[203, 204, 205, 206, 207, 208, 209, 210]
10001110	[211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222]
00100011	[223, 224]
10101110	[225]
10100110	[226, 227, 228, 229]
10100011	[230, 231]
10100010	[232, 233, 234, 235]
10000010	[236, 237, 238, 239, 240, 241, 242, 243, 244, 245]
11101010	[246, 247, 248, 249, 250, 251]

01100010	0	True
01100000	1	False
01101010	3	False
01101110	41	False
01001100	41	True
01011010	56	True
01001110	29	False
01010010	62	False
11010110	79	True
00011010	87	True
01011101	101	True
01111101	112	False
10010110	107	False
00111110	116	False
10011101	119	True
10111100	133	False
00110101	133	False
00111101	128	True
10110101	134	False
10111001	139	False
00010011	158	True
10010001	160	False
10010011	161	False
00110010	163	False
10110010	184	True
10011011	198	False
10111011	203	False
10001111	208	False
10011010	217	False
10001110	218	True
00100011	219	False
10101110	227	False
10100110	233	False
10100011	228	False
10100010	232	True
10000010	247	False
11101010	246	True

Number of codes used=37

Iteration=     70000 training nets give:
alice_loss.item()=0.00024093552201520652	bob_loss.item()=0.006910472176969051

01100010	[0, 1, 252, 253, 254, 255]
01100000	[2, 3, 4, 5, 6, 7, 8, 9]
01101010	[10]
01000010	[11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]
01001100	[29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 48]
01101110	[39, 40, 41, 42, 43, 44, 45, 46, 47]
01111010	[49, 50, 51, 52, 53, 54, 55]
01011010	[56, 57, 58, 59]
01000110	[60]
01010010	[61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
11010110	[72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82]
00011010	[83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94]
01011101	[95, 96]
00010110	[97, 98, 99, 100, 101, 102, 103, 104, 105]
10010110	[106, 107, 108, 109, 110, 111]
00111110	[112, 113, 114, 115, 116]
10011101	[117, 118, 119, 120]
00101100	[121, 122]
00010111	[123, 124, 125, 126, 127, 128, 129, 130, 131]
10110101	[132, 133, 136, 137, 138]
00111101	[134, 135]
10111001	[139, 140, 141, 142, 143, 144, 145, 146, 147]
10110001	[148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158]
00011000	[159, 160]
10010011	[161, 162, 163]
10011000	[164, 165, 166, 167, 168, 169, 170]
00110011	[171, 172, 173, 174, 175, 176]
10110010	[177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189]
10011011	[190, 191, 192, 193, 194, 195, 196]
00111011	[197, 198, 199]
00001000	[200, 201, 202, 203, 204]
10111011	[205, 206, 207, 208, 209, 210]
10001100	[211, 212, 213, 214, 215]
10001110	[216]
00101011	[217, 218, 219, 220]
10000011	[221, 222, 223, 224, 225, 226]
10100110	[227, 228, 229, 230, 231, 232]
11111010	[233, 234, 235]
00101010	[236]
00001010	[237, 238, 239, 240, 241]
10000010	[242, 243, 244]
11100010	[245, 246, 247, 248, 249, 250, 251]

01100010	255	True
01100000	0	False
01101010	254	False
01000010	18	True
01001100	41	False
01101110	41	True
01111010	54	True
01011010	57	True
01000110	50	False
01010010	67	True
11010110	77	True
00011010	86	True
01011101	104	False
00010110	102	True
10010110	107	True
00111110	117	False
10011101	128	False
00101100	128	False
00010111	122	False
10110101	132	True
00111101	144	False
10111001	140	True
10110001	150	True
00011000	165	False
10010011	158	False
10011000	168	True
00110011	170	False
10110010	181	True
10011011	196	True
00111011	196	False
00001000	201	True
10111011	200	False
10001100	207	False
10001110	219	False
00101011	219	True
10000011	223	True
10100110	225	False
11111010	236	False
00101010	236	True
00001010	236	False
10000010	243	True
11100010	246	True

Number of codes used=42


End of hp run 1.  Result of run:
[(-0.9858883417383024, 70000), ('21-05-09_17:56:02BST_NLearn_model_1_Alice_iter70000', '21-05-09_17:56:02BST_NLearn_model_1_Bob_iter70000')]
(-0.9858883417383024, 70000)


>>>> hp_run=2 of 8, time elapsed 1:23:34 of estimated 11:08:32, 
implying ending at 05:04:33BST on Monday 10 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (714844, 936892, 888616, 165835),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': 500,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=     20000 training nets give:
alice_loss.item()=0.7443985939025879	bob_loss.item()=0.6200053095817566

01111010	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 251, 252, 253, 254, 255]
10111100	[31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 56, 57, 58, 59, 60, 61, 62, 63]
00101100	[53, 54, 55]
10000011	[64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]
01110001	[75, 76]
01111101	[77, 78, 79, 80, 81, 82]
11010010	[83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250]

01111010	129	False
10111100	188	False
00101100	138	False
10000011	8	False
01110001	97	False
01111101	154	False
11010010	227	True

Number of codes used=7

Iteration=     30000 training nets give:
alice_loss.item()=0.17408573627471924	bob_loss.item()=0.34732717275619507

00010110	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01001011	[22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66]
01110001	[67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101]
11100001	[102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137]
11001101	[138]
10010100	[139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184]
11010010	[185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202]

00010110	244	True
01001011	46	True
01110001	85	True
11100001	102	True
11001101	104	False
10010100	139	True
11010010	151	False

Number of codes used=7

Iteration=     40000 training nets give:
alice_loss.item()=0.11889944970607758	bob_loss.item()=0.11714772880077362

00010111	[0, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 253, 254, 255]
00011110	[1, 2, 3, 4, 5, 6, 7, 8, 9]
00001011	[10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]
01101011	[43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
01001001	[60, 61, 62, 63]
11100101	[64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]
11100001	[101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125]
10011000	[126, 127]
10011100	[128, 129, 130, 131]
10010100	[132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153]
11010111	[154, 155, 156, 157, 158, 159, 160]
10010000	[161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175]
00010000	[176]
01010010	[177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202]
01010110	[203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220]
00010110	[231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252]

00010111	228	True
00011110	237	False
00001011	27	True
01101011	41	False
01001001	63	True
11100101	99	True
11100001	93	False
10011000	160	False
10011100	160	False
10010100	163	False
11010111	222	False
10010000	165	True
00010000	171	False
01010010	191	True
01010110	222	False
00010110	221	False

Number of codes used=16

Iteration=     50000 training nets give:
alice_loss.item()=0.09287331998348236	bob_loss.item()=0.13639169931411743

00110010	[0, 1, 2, 250, 251, 252, 253, 254, 255]
01011111	[3, 4, 5, 6, 7]
00001111	[8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]
00001011	[23, 24, 25, 26, 27, 28]
01100011	[29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]
01000011	[45, 46, 47, 48, 49, 50]
11001111	[51, 52, 53]
10001011	[54, 55, 56, 57, 58, 59, 60, 61, 62]
01001001	[63, 64, 65, 66, 67, 68, 69, 70, 71, 72]
01110001	[73]
11111000	[74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]
11100001	[90, 91, 92, 93]
11100000	[94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113]
11010101	[114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
11000100	[128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142]
11010100	[143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156]
10010000	[157, 158, 159, 160, 161, 162, 163, 164, 165, 166]
10011000	[167, 168, 169, 170, 171]
00010100	[172, 173]
00010000	[174, 175, 176]
11011010	[177, 178, 179, 180, 181]
11010010	[182, 183, 184, 185, 186, 187, 188, 189, 190, 191]
01010010	[192, 193, 194, 195, 196, 197, 198, 199]
10010110	[200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211]
00010010	[212, 213, 214, 215, 216, 217]
01010110	[218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228]
00010110	[229, 230, 231, 232, 233, 236, 237]
00010111	[234, 235]
01110110	[238, 239, 240]
00011111	[241, 242, 243, 244, 245, 246, 247, 248, 249]

00110010	246	False
01011111	2	False
00001111	15	True
00001011	38	False
01100011	51	False
01000011	51	False
11001111	60	False
10001011	61	True
01001001	74	False
01110001	80	False
11111000	110	False
11100001	100	False
11100000	103	True
11010101	126	True
11000100	134	True
11010100	144	True
10010000	162	True
10011000	157	False
00010100	189	False
00010000	175	True
11011010	184	False
11010010	189	True
01010010	196	True
10010110	215	False
00010010	217	True
01010110	221	True
00010110	232	True
00010111	221	False
01110110	224	False
00011111	252	False

Number of codes used=30

Iteration=     60000 training nets give:
alice_loss.item()=0.013922072015702724	bob_loss.item()=0.016556696966290474

00111111	[0, 1, 2, 3, 4, 5, 6, 251, 252, 253, 254, 255]
00001111	[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]
01001111	[19, 20, 21, 22, 23, 24, 25, 26, 27, 28]
00000011	[29, 30, 31, 32, 33, 34, 35, 36, 37]
10001010	[38, 39, 40, 41, 42, 43]
01110011	[44]
01000011	[45, 46, 47, 48, 49, 50, 51, 52, 53]
01011001	[54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67]
00110101	[68]
01110001	[69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
01110000	[84, 85, 86, 87, 88, 89]
11101001	[90, 91]
10111001	[92, 93, 94, 95, 96]
11100101	[97, 98, 99, 100]
11100000	[101, 102, 103, 104, 105, 106, 107]
11001101	[108, 109, 110, 111, 112]
11010001	[113, 114, 115, 116, 117, 118]
10110100	[119, 120, 121, 122, 123, 124, 125, 126]
11010101	[127, 128, 129, 130]
11000100	[131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141]
11010100	[142, 143, 144]
10110000	[145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155]
01010000	[156, 157, 158]
10010100	[159, 160, 161, 162]
11010000	[163, 164, 165]
10000000	[166, 167, 168, 169, 170, 171]
00010000	[172, 173, 174, 175, 176, 177, 178, 179]
11011110	[180, 181]
11011010	[182, 183, 184, 185]
11010010	[186, 187, 188, 189, 190, 191]
01010010	[192, 193, 194, 195, 196, 197]
01000110	[198, 199, 200, 201, 202, 203, 204, 205]
10010010	[206, 207, 208]
11000010	[209, 210, 211, 212, 213]
00010010	[214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226]
00010110	[227, 228, 229]
10010011	[230]
00010011	[231, 232, 233, 234, 235]
00011110	[236, 237, 238, 239, 240]
00110110	[241, 242, 243, 244, 245, 246, 247, 248]
00110111	[249]
00110010	[250]

00111111	250	False
00001111	12	True
01001111	32	False
00000011	28	False
10001010	49	False
01110011	52	False
01000011	47	True
01011001	64	True
00110101	76	False
01110001	83	True
01110000	90	False
11101001	100	False
10111001	100	False
11100101	102	False
11100000	104	True
11001101	101	False
11010001	122	False
10110100	132	False
11010101	123	False
11000100	130	False
11010100	143	True
10110000	148	True
01010000	163	False
10010100	166	False
11010000	156	False
10000000	167	True
00010000	183	False
11011110	175	False
11011010	180	False
11010010	183	False
01010010	198	False
01000110	200	True
10010010	197	False
11000010	204	False
00010010	219	True
00010110	238	False
10010011	219	False
00010011	224	False
00011110	245	False
00110110	234	False
00110111	242	False
00110010	238	False

Number of codes used=42

Iteration=     70000 training nets give:
alice_loss.item()=0.0004159252857789397	bob_loss.item()=0.0025835775304585695

00111111	[0, 1, 2, 3, 4, 5, 249, 250, 251, 252, 253, 254, 255]
00011011	[6, 7, 8, 9, 10, 11, 12]
00001111	[13, 14, 15, 16, 17]
00001010	[18, 19, 20]
00110011	[21, 22, 23, 24, 25, 26, 27, 28]
01001111	[29]
01001010	[30, 31, 32, 33, 34, 35]
00101010	[36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]
01110011	[47]
01000011	[48, 49, 50, 51, 52, 53]
10101011	[54, 55, 56]
10001011	[57, 58, 59, 60, 61, 62, 63, 64, 65]
01011001	[66, 67, 68, 69, 70, 71]
00110101	[72, 73, 74, 75, 76, 77, 78, 79, 80]
01100101	[81]
11100010	[82, 83, 84, 85, 86, 87, 88]
01110000	[89, 90, 91, 92]
11110001	[93]
10111001	[94, 95, 96, 97, 98, 99, 100, 101]
11100101	[102, 103]
11100000	[104, 105, 106, 107, 108, 109, 110, 111]
11111000	[112, 113, 114]
11010001	[115, 116]
11010101	[117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128]
11000100	[129, 130, 131, 132, 133, 134, 135, 136, 137, 138]
11010100	[139, 140]
10010001	[141, 142, 143, 144, 145, 146, 147, 148, 149, 150]
10000100	[151, 152, 153, 154, 155]
10010000	[156, 157, 158, 159, 160, 161, 162, 163, 164, 165]
10000000	[166, 167, 168, 169, 170, 171, 172, 173, 174]
00010000	[175, 176, 177]
11011010	[178, 179, 180, 181, 182, 183, 184]
11010010	[185, 186, 187, 188, 189, 190, 191, 192]
01010010	[193, 194, 195, 196]
01000110	[197, 198, 199, 200, 201, 202, 203, 204, 205, 206]
11000010	[207, 208, 209, 210, 211]
10010110	[212, 213, 214, 215]
00010010	[216, 217, 218, 219, 220, 221, 222, 223, 224]
00010011	[225, 226, 227, 228, 229]
00010110	[230, 231, 232, 233, 234, 235, 236, 237]
00011110	[238, 239, 240, 241, 242, 243, 244, 245, 246]
00110111	[247, 248]

00111111	251	True
00011011	11	True
00001111	14	True
00001010	22	False
00110011	20	False
01001111	30	False
01001010	39	False
00101010	41	True
01110011	50	False
01000011	48	True
10101011	53	False
10001011	58	True
01011001	65	False
00110101	78	True
01100101	74	False
11100010	83	True
01110000	88	False
11110001	101	False
10111001	99	True
11100101	98	False
11100000	99	False
11111000	115	False
11010001	123	False
11010101	124	True
11000100	131	True
11010100	144	False
10010001	150	True
10000100	162	False
10010000	162	True
10000000	171	True
00010000	181	False
11011010	183	True
11010010	190	True
01010010	196	True
01000110	196	False
11000010	206	False
10010110	217	False
00010010	217	True
00010011	239	False
00010110	236	True
00011110	243	True
00110111	234	False

Number of codes used=42


End of hp run 2.  Result of run:
[(-0.9870843701567846, 70000), ('21-05-09_17:56:02BST_NLearn_model_2_Alice_iter70000', '21-05-09_17:56:02BST_NLearn_model_2_Bob_iter70000')]
(-0.9870843701567846, 70000)


>>>> hp_run=3 of 8, time elapsed 2:47:21 of estimated 11:09:25, 
implying ending at 05:05:26BST on Monday 10 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (508585, 487266, 751926, 247136),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=     20000 training nets give:
alice_loss.item()=0.5236128568649292	bob_loss.item()=0.4506351351737976

00100101	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 187, 188, 189, 190, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11111101	[13, 14, 15, 16, 17, 18, 19]
00001101	[35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
00100010	[96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135]
01001101	[136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 182, 183, 184, 185, 186]
01111111	[158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181]
01100000	[191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212]
00011100	[213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240]

00100101	86	False
11111101	70	False
00001101	46	True
00100010	74	False
01001101	50	False
01111111	53	False
01100000	89	False
00011100	44	False

Number of codes used=8

Iteration=     30000 training nets give:
alice_loss.item()=0.19904381036758423	bob_loss.item()=0.2728385925292969

00111101	[0, 1, 2, 3, 4, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00011100	[5, 6]
00001101	[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]
00100010	[73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
11100110	[115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182]

00111101	213	True
00011100	223	False
00001101	54	True
00100010	102	True
11100110	154	True

Number of codes used=5

Iteration=     40000 training nets give:
alice_loss.item()=0.15994465351104736	bob_loss.item()=0.12920501828193665

10111101	[0, 1, 252, 253, 254, 255]
00011101	[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
10101101	[24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]
00001101	[51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62]
00001001	[63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]
00110010	[75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91]
00101010	[92]
00100010	[93, 94, 95, 96, 97, 98, 99, 100, 101, 102]
00100000	[103, 104, 105, 106, 107, 108, 109]
00000010	[110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131]
11100111	[132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
11000110	[144, 145, 146, 147]
11100110	[148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161]
01100110	[162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184]
01111101	[185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200]
00111101	[201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217]
00111100	[218, 219, 220, 221, 222, 223, 224]
00111001	[225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251]

10111101	241	False
00011101	0	False
10101101	29	True
00001101	54	True
00001001	73	True
00110010	86	True
00101010	110	False
00100010	97	True
00100000	107	True
00000010	119	True
11100111	150	False
11000110	152	False
11100110	151	True
01100110	164	True
01111101	194	True
00111101	224	False
00111100	223	True
00111001	228	True

Number of codes used=18

Iteration=     50000 training nets give:
alice_loss.item()=0.09160498529672623	bob_loss.item()=0.132929727435112

10110110	[0, 1, 2, 3, 248, 249, 250, 251, 252, 253, 254, 255]
00011101	[4, 5, 6, 7, 8, 9, 10, 11]
00101101	[12]
10011101	[13, 14, 15, 16, 17, 18, 19, 20, 21, 22]
10101101	[23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]
10000101	[38, 39, 40, 41, 42]
10001101	[43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60]
00001001	[61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73]
10000001	[74, 75, 76, 77, 78, 79, 80, 81]
00110010	[82, 83, 84]
10100000	[85, 86, 87, 88, 89, 90, 91, 92, 93]
00100010	[94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109]
00100011	[110, 111, 112, 113, 114, 115, 116, 119, 120, 121]
11101000	[117, 118]
01001010	[122, 123, 124, 125, 126, 127]
11000100	[128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140]
11101010	[141, 142, 143, 144, 145, 146, 147]
11100010	[148, 149, 150, 151, 152, 153, 154]
01101110	[155, 156, 157, 158]
11100110	[159, 160, 161, 162]
01100110	[163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175]
10101110	[176, 177]
10100110	[178, 179, 180, 181]
00111111	[182, 183, 184, 185, 186]
01111101	[187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204]
10111100	[205, 206, 207, 208, 209, 210, 211, 212, 213, 218, 219, 220, 221]
00111100	[214, 215, 216, 217]
11111010	[222]
00111001	[223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236]
01111000	[237]
10111101	[238, 239, 240, 241, 242, 243, 244, 245, 246, 247]

10110110	165	False
00011101	253	False
00101101	245	False
10011101	15	True
10101101	21	False
10000101	35	False
10001101	31	False
00001001	70	True
10000001	64	False
00110010	89	False
10100000	83	False
00100010	100	True
00100011	104	False
11101000	120	False
01001010	114	False
11000100	131	True
11101010	132	False
11100010	136	False
01101110	147	False
11100110	144	False
01100110	141	False
10101110	160	False
10100110	161	False
00111111	207	False
01111101	208	False
10111100	231	False
00111100	225	False
11111010	141	False
00111001	233	True
01111000	239	False
10111101	238	True

Number of codes used=31

Iteration=     60000 training nets give:
alice_loss.item()=0.006171956658363342	bob_loss.item()=0.17414408922195435

00011101	[0, 1, 2, 3, 4, 252, 253, 254, 255]
10100101	[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
10011101	[17, 18, 19, 20, 21, 22, 23]
10101101	[24, 25, 26, 27, 28]
00000111	[29, 30, 31, 32, 33, 34, 35, 36]
10001111	[37, 38, 39, 40, 41, 42, 43]
00001110	[44, 45, 46, 47, 48, 49, 50, 51, 52, 53]
00001000	[54, 55, 56, 57, 58, 59, 60]
00001111	[61, 62, 63, 64, 65, 66]
00001001	[67, 68, 69, 70, 71, 72, 73, 74]
10110010	[75, 76, 77, 78, 79]
10100000	[80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91]
00000011	[92]
00100010	[93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104]
00001010	[105, 106]
00100011	[107, 108, 109, 110]
00000010	[111, 112, 113, 114, 115, 116]
11101000	[117, 118, 119, 120, 121, 122]
11101111	[123, 124, 125, 126, 127, 128]
11000100	[129, 130, 131, 132, 133, 134, 135, 136, 137, 138]
11101010	[139, 140, 141]
11100010	[142, 143, 144, 145, 146, 147, 148]
11100110	[149, 150, 151, 152]
01101110	[153, 154]
11110110	[155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174]
10100110	[175, 176, 177, 178, 179, 180]
10111110	[181, 182, 183, 184, 185]
00111111	[186, 187, 188, 189, 190]
00111011	[191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203]
01111101	[204, 205, 206]
10111111	[207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219]
00111101	[220, 221, 222, 223, 224, 225]
00111001	[226, 227, 228, 229, 230, 231, 232, 233, 234]
01111000	[235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245]
00010101	[246, 247, 248, 249, 250, 251]

00011101	253	True
10100101	18	False
10011101	12	False
10101101	22	False
00000111	43	False
10001111	44	False
00001110	45	True
00001000	50	False
00001111	68	False
00001001	65	False
10110010	80	False
10100000	79	False
00000011	98	False
00100010	97	True
00001010	107	False
00100011	101	False
00000010	109	False
11101000	118	True
11101111	121	False
11000100	132	True
11101010	137	False
11100010	134	False
11100110	149	True
01101110	143	False
11110110	158	True
10100110	159	False
10111110	201	False
00111111	208	False
00111011	205	False
01111101	205	True
10111111	219	True
00111101	216	False
00111001	226	True
01111000	236	True
00010101	251	True

Number of codes used=35

Iteration=     70000 training nets give:
alice_loss.item()=0.00047111292951740324	bob_loss.item()=0.0007071280851960182

00011101	[0, 1, 253, 254, 255]
00011111	[2, 3, 4, 5, 6]
10011101	[7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
10100101	[17, 18, 19, 20, 21, 22]
10101101	[23, 24, 25, 26, 27, 28]
00000111	[29, 30, 31, 32, 33, 34, 35]
10101011	[36, 37, 38, 39, 40, 41]
00001110	[42, 43, 44, 45, 46, 47, 48, 49, 50]
00001000	[51, 52, 53, 54, 55, 56, 57]
10000001	[58, 59, 60, 61, 62, 63]
00001111	[64, 65, 66, 67, 68]
10110000	[69, 70, 71, 72, 73, 74, 75, 76]
10110010	[77, 78]
10100000	[79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]
10100010	[90, 91, 92]
00000011	[93, 94, 95, 96, 97, 98, 99]
00100010	[100, 101, 102]
00001010	[103, 104, 105, 106, 107]
00000010	[108, 109, 110, 111, 112, 113]
11101000	[114, 115, 116, 117, 118, 119, 120, 121, 122, 123]
11101111	[124, 125, 126]
11000100	[127, 128, 129]
01000010	[130, 131, 132, 133, 134, 135, 136]
11101010	[137, 138, 139]
01101110	[140, 141, 142, 143, 144, 145]
11100110	[146, 147, 148, 149, 150, 151]
11110110	[152, 153, 154, 155, 156, 157, 158, 159, 160, 161]
10101110	[162, 163, 164, 165, 166, 167]
10100110	[168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180]
10111110	[181, 182, 183, 184, 185, 186, 187]
00111011	[188]
00111111	[189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202]
01111101	[203, 204, 205, 206, 207, 208, 209, 210]
10111111	[211]
00111101	[212, 213, 214, 215, 216, 217, 218, 219]
00111000	[220, 221]
00111100	[222, 223, 224, 225]
00111001	[226, 227, 228, 229, 230, 231]
11111101	[232, 233, 234, 235, 236, 237, 238]
01111000	[239, 240, 241, 242]
00010101	[243, 244, 245, 246, 247, 248, 249, 250, 251, 252]

00011101	250	False
00011111	6	True
10011101	13	True
10100101	13	False
10101101	20	False
00000111	39	False
10101011	41	True
00001110	46	True
00001000	47	False
10000001	64	False
00001111	63	False
10110000	73	True
10110010	82	False
10100000	81	True
10100010	85	False
00000011	96	True
00100010	96	False
00001010	112	False
00000010	106	False
11101000	117	True
11101111	118	False
11000100	127	True
01000010	135	True
11101010	141	False
01101110	140	True
11100110	151	True
11110110	158	True
10101110	164	True
10100110	162	False
10111110	207	False
00111011	207	False
00111111	209	False
01111101	213	False
10111111	221	False
00111101	218	True
00111000	218	False
00111100	220	False
00111001	231	True
11111101	229	False
01111000	238	False
00010101	247	True

Number of codes used=41


End of hp run 3.  Result of run:
[(-0.9779365160239472, 70000), ('21-05-09_17:56:02BST_NLearn_model_3_Alice_iter70000', '21-05-09_17:56:02BST_NLearn_model_3_Bob_iter70000')]
(-0.9779365160239472, 70000)


>>>> hp_run=4 of 8, time elapsed 4:11:34 of estimated 11:10:51, 
implying ending at 05:06:52BST on Monday 10 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (508585, 487266, 751926, 247136),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': 500,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=     20000 training nets give:
alice_loss.item()=0.6351545453071594	bob_loss.item()=0.5407240390777588

00110110	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00000101	[66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81]
01000000	[82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103]
00010110	[104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241]
00111111	[136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]

00110110	51	True
00000101	63	False
01000000	69	False
00010110	72	False
00111111	66	False

Number of codes used=5

Iteration=     30000 training nets give:
alice_loss.item()=0.16269470751285553	bob_loss.item()=0.22444486618041992

00110110	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 254, 255]
00000101	[55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81]
01000000	[82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105]
10010001	[106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]
00011000	[160, 161, 162]
11000111	[163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182]
10101011	[183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245]
10101111	[246, 247, 248, 249, 250, 251, 252, 253]

00110110	29	True
00000101	77	True
01000000	102	True
10010001	131	True
00011000	121	False
11000111	185	False
10101011	201	True
10101111	218	False

Number of codes used=8

Iteration=     40000 training nets give:
alice_loss.item()=0.09808961302042007	bob_loss.item()=0.05029299110174179

00110100	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]
00110110	[23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
00110111	[48]
11010001	[49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 78, 79, 80, 81]
00000101	[68, 69, 70, 71, 72, 73, 74, 75, 76, 77]
00000000	[82, 83, 84, 85, 86, 87, 88, 89, 90, 91]
10011001	[92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120]
10010001	[121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152]
10010000	[153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175]
11000111	[176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187]
00101011	[188, 189, 190]
10111011	[191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201]
10101011	[202, 203, 204, 205, 206]
10101101	[207, 208, 209, 210, 211, 212, 213, 214]
10101111	[215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245]
01110100	[246, 247, 248, 249, 250, 251, 252, 253, 254, 255]

00110100	24	False
00110110	20	False
00110111	31	False
11010001	114	False
00000101	80	False
00000000	105	False
10011001	132	False
10010001	122	True
10010000	142	False
11000111	195	False
00101011	208	False
10111011	206	False
10101011	213	False
10101101	224	False
10101111	234	True
01110100	20	False

Number of codes used=16

Iteration=     50000 training nets give:
alice_loss.item()=0.12088294327259064	bob_loss.item()=0.11848004162311554

01110110	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 251, 252, 253, 254, 255]
10110100	[15, 16, 17, 18, 19]
00111100	[20, 21, 31]
00100111	[22, 23, 24, 25, 26, 27, 28, 29, 30]
00111110	[32]
00110011	[33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55]
01010101	[56, 57, 58, 59, 60, 61]
00000101	[62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82]
01001100	[83, 84, 85]
01000101	[86, 87]
00010101	[88, 89, 90, 91, 92, 93, 94]
00000100	[95, 96, 97, 98, 99, 100, 101, 102, 103, 104]
10000101	[105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116]
00010011	[117, 118, 119, 120]
10010101	[121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140]
10011101	[141, 142, 143]
10010100	[144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]
11010101	[160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174]
10110011	[175, 176, 177, 178]
11010111	[179, 180, 181, 182, 183, 184, 185, 186, 196]
11000101	[187, 188, 189, 190, 191, 192, 193, 194, 195]
10111011	[197, 198, 199, 200, 201, 202, 203, 204]
10101110	[205, 206]
10100111	[207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219]
10100011	[220]
10001111	[221, 222]
10101111	[223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244]
11101011	[245, 246, 247, 248, 249, 250]

01110110	18	False
10110100	22	False
00111100	25	False
00100111	30	True
00111110	22	False
00110011	33	True
01010101	71	False
00000101	76	True
01001100	103	False
01000101	78	False
00010101	74	False
00000100	85	False
10000101	172	False
00010011	115	False
10010101	127	True
10011101	139	False
10010100	147	True
11010101	163	True
10110011	171	False
11010111	185	True
11000101	186	False
10111011	200	True
10101110	212	False
10100111	221	False
10100011	220	True
10001111	226	False
10101111	238	True
11101011	226	False

Number of codes used=28

Iteration=     60000 training nets give:
alice_loss.item()=0.00933005940169096	bob_loss.item()=0.03778383880853653

00100110	[0, 1, 2, 3, 4, 5, 6, 7, 8, 255]
10110010	[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
00110010	[20, 21, 22]
00111100	[23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]
00110011	[35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]
01010101	[51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]
00000101	[73, 74, 75, 76, 77, 78, 79, 80]
00001101	[81, 82, 83, 84, 85, 86, 87, 88]
01101000	[89, 90, 91, 92]
01100001	[93, 94]
00000110	[95]
01001000	[96, 97, 98, 99, 100, 101]
01001100	[102, 103, 104, 105, 106]
01010000	[107, 108, 109, 110]
00011001	[111, 112, 113, 114]
10001001	[115, 116]
00010000	[117, 118, 119, 120, 121, 122]
10010001	[123, 124, 125, 126, 127, 128]
10011001	[129]
10010011	[130, 131, 132, 133, 134]
10110001	[135, 136, 137]
10010000	[138]
10011101	[139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150]
10010100	[151]
10010010	[152, 153, 154, 155, 156]
11010101	[157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167]
11011101	[168, 169, 170, 171, 172, 173, 174, 175]
11010110	[176, 177, 178, 179, 180]
11000101	[181, 182, 183, 184, 185, 186, 187, 188, 189, 190]
11001101	[191, 192, 193, 194, 195, 196, 197, 198, 199, 200]
10001010	[201, 202, 203, 204]
10101001	[205, 206, 207]
10101011	[208, 209, 210, 211, 212, 213, 214]
11101111	[215, 216, 217, 220, 221, 222, 223]
00101111	[218, 219]
11101011	[224]
10101101	[225, 226, 231]
10001111	[227, 228, 229, 230]
10101111	[232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254]

00100110	7	True
10110010	17	True
00110010	21	True
00111100	22	False
00110011	31	False
01010101	67	True
00000101	84	False
00001101	88	True
01101000	86	False
01100001	98	False
00000110	83	False
01001000	91	False
01001100	98	False
01010000	102	False
00011001	120	False
10001001	124	False
00010000	123	False
10010001	128	True
10011001	137	False
10010011	140	False
10110001	145	False
10010000	136	False
10011101	152	False
10010100	143	False
10010010	151	False
11010101	162	True
11011101	168	True
11010110	177	True
11000101	183	True
11001101	182	False
10001010	202	True
10101001	203	False
10101011	209	True
11101111	211	False
00101111	217	False
11101011	218	False
10101101	214	False
10001111	219	False
10101111	239	True

Number of codes used=39

Iteration=     70000 training nets give:
alice_loss.item()=0.00033650780096650124	bob_loss.item()=0.0005365513497963548

00100110	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 254, 255]
10110010	[12, 13, 14, 15]
10110100	[16, 17, 18, 19]
00110010	[20, 21]
00110000	[22, 23, 24, 25]
00100111	[26, 27, 28, 29]
00110011	[30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]
00010100	[43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]
01010101	[54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]
00000101	[73, 74, 75, 76, 77, 78, 79, 80, 81]
00001101	[82, 83, 84, 88, 89, 90, 91, 92]
01101000	[85, 86, 87]
00100001	[93]
01100001	[94]
10000000	[95, 96, 97, 98, 99, 100, 101]
00000001	[102, 103, 104, 105]
01010000	[106, 107]
10000101	[108, 109, 110]
00010011	[111, 112, 113, 114, 115, 116]
00010000	[117, 118, 119, 120, 121, 122, 123]
10010001	[124, 125, 126, 127, 128]
11110001	[129, 130, 131]
10010000	[132, 133, 134, 135, 136, 137, 138, 139]
10011000	[140, 141, 142, 143, 144, 145]
10011101	[146, 147]
10010010	[148, 149, 150, 151, 152, 153, 154, 155]
11010101	[156, 157, 158, 159, 160, 161, 162, 163, 164, 165]
11011101	[166, 167, 168, 169, 170, 171, 172, 173, 174, 175]
11010110	[176, 177, 178, 179]
11010111	[180, 181, 182, 183, 184, 185, 186, 187, 188]
11000111	[189, 190, 191, 192]
10111011	[193, 194, 195, 196, 197, 198, 199]
10100101	[200, 201, 202, 203]
10101001	[204, 205, 206]
10101011	[207, 208, 209, 210, 211]
10111111	[212, 213, 214, 215, 216, 217]
00101111	[218, 219, 220]
10001111	[221, 222, 223, 224, 225, 226, 227, 228]
10101111	[229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253]

00100110	0	True
10110010	20	False
10110100	18	True
00110010	20	True
00110000	22	True
00100111	25	False
00110011	29	False
00010100	39	False
01010101	65	True
00000101	79	True
00001101	95	False
01101000	85	True
00100001	96	False
01100001	94	True
10000000	98	True
00000001	100	False
01010000	100	False
10000101	109	True
00010011	113	True
00010000	122	True
10010001	129	False
11110001	137	False
10010000	137	True
10011000	144	True
10011101	158	False
10010010	147	False
11010101	154	False
11011101	167	True
11010110	176	True
11010111	185	True
11000111	189	True
10111011	196	True
10100101	206	False
10101001	199	False
10101011	214	False
10111111	210	False
00101111	214	False
10001111	214	False
10101111	241	True

Number of codes used=39


End of hp run 4.  Result of run:
[(-0.9850623721619061, 70000), ('21-05-09_17:56:02BST_NLearn_model_4_Alice_iter70000', '21-05-09_17:56:02BST_NLearn_model_4_Bob_iter70000')]
(-0.9850623721619061, 70000)


>>>> hp_run=5 of 8, time elapsed 5:35:37 of estimated 11:11:13, 
implying ending at 05:07:15BST on Monday 10 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (843402, 443788, 742412, 270619),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=     20000 training nets give:
alice_loss.item()=0.6190357208251953	bob_loss.item()=0.6449151039123535

10000001	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01001101	[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]
00111000	[38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58]
11010101	[59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160]
10111001	[161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224]

10000001	177	False
01001101	95	False
00111000	22	False
11010101	115	True
10111001	253	False

Number of codes used=5

Iteration=     30000 training nets give:
alice_loss.item()=0.05201655998826027	bob_loss.item()=0.10647451132535934

00111000	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 253, 254, 255]
01001111	[64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81]
11010101	[82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168]
11001000	[169, 170]
10110111	[171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252]

00111000	38	True
01001111	33	False
11010101	108	True
11001000	12	False
10110111	222	True

Number of codes used=5

Iteration=     40000 training nets give:
alice_loss.item()=0.10495888441801071	bob_loss.item()=0.07311929762363434

00111001	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 255]
00111010	[17]
00111000	[18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]
01001111	[29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]
00011000	[41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54]
01001110	[55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]
01010101	[80, 81, 82, 83, 84, 85, 86]
11010111	[87, 88, 89, 90, 91, 92, 93, 94]
11010100	[95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111]
10010101	[112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
11011101	[128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146]
10011101	[147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161]
10111111	[162, 163, 164]
10110101	[165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201]
00110111	[202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212]
10110111	[213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225]
10100111	[226, 227, 228, 229]
10110011	[230, 231, 232, 233, 234, 235, 236, 237, 238]
10111000	[239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254]

00111001	24	False
00111010	34	False
00111000	41	False
01001111	44	False
00011000	43	True
01001110	51	False
01010101	96	False
11010111	98	False
11010100	105	True
10010101	120	True
11011101	114	False
10011101	115	False
10111111	211	False
10110101	191	True
00110111	195	False
10110111	212	False
10100111	215	False
10110011	224	False
10111000	9	False

Number of codes used=19

Iteration=     50000 training nets give:
alice_loss.item()=0.060845836997032166	bob_loss.item()=0.19582456350326538

10111010	[0, 1, 2, 253, 254, 255]
00111011	[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
00111001	[16, 17, 18, 19]
00011011	[20, 21, 22, 23, 24]
00111010	[25, 26, 27, 28, 29, 30, 31]
00111100	[32, 33]
01001111	[34, 35, 36, 37, 38, 39, 40, 41]
00110100	[42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
00110000	[52, 53, 54, 55, 56, 57, 58]
01111000	[59, 60, 61]
01011000	[62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
01010001	[84, 85, 86]
11000101	[87, 88, 89, 90, 91, 92, 93, 94, 95, 96]
11010111	[97, 98, 99, 100, 101, 102, 103]
01010110	[104, 105, 106, 107, 108]
01010111	[109]
11011111	[110, 111, 112, 113, 114, 115, 116, 117]
10010100	[118, 119]
10010101	[120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145]
10011001	[146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162]
10110101	[163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181]
00110101	[182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197]
10111111	[198]
10101001	[199, 200, 201]
10110111	[202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212]
10111001	[213, 214, 215, 216, 217, 218, 219, 220, 221, 222]
10101111	[223, 224, 225, 226]
10011011	[227, 228, 229, 230, 231, 232, 233]
10000111	[234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244]
10110010	[245, 246, 247, 248, 249, 250, 251, 252]

10111010	8	False
00111011	13	True
00111001	10	False
00011011	21	True
00111010	19	False
00111100	27	False
01001111	43	False
00110100	50	True
00110000	45	False
01111000	43	False
01011000	69	True
01010001	94	False
11000101	98	False
11010111	98	True
01010110	102	False
01010111	100	False
11011111	104	False
10010100	115	False
10010101	121	True
10011001	152	True
10110101	188	False
00110101	193	True
10111111	210	False
10101001	225	False
10110111	214	False
10111001	222	True
10101111	223	True
10011011	235	False
10000111	237	True
10110010	241	False

Number of codes used=30

Iteration=     60000 training nets give:
alice_loss.item()=0.04739590734243393	bob_loss.item()=0.031545255333185196

10101010	[0, 1, 2, 3, 4, 5, 6, 252, 253, 254, 255]
00111011	[7, 8, 9, 10, 11, 12, 13, 14, 15]
00011011	[16, 17, 18, 19, 20, 21, 22, 23]
00011001	[24, 25, 26]
01011111	[27, 28, 29, 30, 31, 32, 33]
00001000	[34, 35, 36, 37, 38, 39]
00101000	[40, 41, 42]
01001010	[43, 44, 45, 46, 47, 48, 49]
00110000	[50, 51, 52, 53, 54, 55, 56, 57, 58]
01100000	[59]
01011000	[60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]
11011000	[80, 81, 82, 83, 84, 85, 86]
01110000	[87, 88]
11000111	[89, 90, 91, 92, 93, 94, 95, 96, 97]
11011110	[98]
11010111	[99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110]
01010111	[111, 112]
11011101	[113, 114, 115]
10010101	[116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136]
10011001	[137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169]
10110101	[170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193]
00110101	[194, 195]
01110111	[196, 197]
10110100	[198, 199, 200, 201, 202]
11100111	[203, 204, 205]
11110111	[206, 207]
10110110	[208, 209, 212, 213, 214, 215]
10100111	[210, 211]
00110110	[216, 217, 218]
10111001	[219, 220, 221]
10101111	[222, 223, 224, 225, 226, 227, 228]
10011011	[229, 230, 231, 232, 233, 234, 238, 239]
10000111	[235, 236, 237]
10110010	[240, 241, 242, 243, 244, 245, 246, 247]
10101000	[248, 249, 250, 251]

10101010	5	True
00111011	9	True
00011011	13	False
00011001	15	False
01011111	20	False
00001000	37	True
00101000	22	False
01001010	44	True
00110000	51	True
01100000	67	False
01011000	69	True
11011000	94	False
01110000	84	False
11000111	94	True
11011110	99	False
11010111	109	True
01010111	101	False
11011101	119	False
10010101	131	True
10011001	151	True
10110101	183	True
00110101	202	False
01110111	203	False
10110100	194	False
11100111	208	False
11110111	196	False
10110110	213	True
10100111	216	False
00110110	225	False
10111001	224	False
10101111	230	False
10011011	237	False
10000111	239	False
10110010	234	False
10101000	2	False

Number of codes used=35

Iteration=     70000 training nets give:
alice_loss.item()=0.0009809001348912716	bob_loss.item()=0.0008595981053076684

10101010	[0, 1, 2, 3, 4, 5, 6, 7]
00111011	[8, 9, 10, 11, 12, 13, 14]
00011011	[15, 16, 17]
00111010	[18, 19, 20, 21, 22, 23, 24, 25, 26, 27]
00111100	[28, 29, 30, 31, 32]
00001000	[33, 34, 35, 36, 37, 38, 39, 40, 41, 42]
01001010	[43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54]
01101000	[55]
01100000	[56, 57, 58, 59, 60, 61, 78, 79, 80]
01011000	[62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
01001000	[72, 73, 74, 75, 76, 77]
01011100	[81, 82, 83, 84, 85, 86, 87, 88, 89, 90]
11000111	[91, 92, 93, 94, 95, 96]
01010000	[97, 98, 99, 105, 106]
01010100	[100, 101, 102, 103, 104]
11000110	[107, 108, 109, 110, 111]
01110101	[112, 113, 114, 115, 116]
10010100	[117, 118, 119, 120, 121, 122]
10010101	[123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135]
10001101	[136, 137, 138, 139]
10011001	[140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168]
10110101	[169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190]
10110001	[191, 192, 193, 194, 195, 196, 197, 198]
10110100	[199, 200, 201]
11100111	[202, 203]
11111111	[204, 205, 206, 207, 208, 209]
10110110	[210, 211, 212, 213, 214, 215, 216, 217, 218]
10111001	[219, 220, 221, 222, 223, 224, 225]
10101111	[226, 227, 228, 229, 230]
10011011	[231, 232, 233, 234, 235, 236, 237, 238, 239, 240]
10000111	[241, 242]
10110010	[243, 244, 245, 246]
10101000	[247, 248, 249, 250, 251, 252, 253, 254, 255]

10101010	1	True
00111011	7	False
00011011	16	True
00111010	21	True
00111100	30	True
00001000	41	True
01001010	42	False
01101000	49	False
01100000	73	False
01011000	78	False
01001000	79	False
01011100	87	True
11000111	93	True
01010000	100	False
01010100	103	True
11000110	109	True
01110101	110	False
10010100	122	True
10010101	126	True
10001101	136	True
10011001	152	True
10110101	183	True
10110001	199	False
10110100	191	False
11100111	211	False
11111111	210	False
10110110	215	True
10111001	226	False
10101111	226	True
10011011	238	True
10000111	237	False
10110010	238	False
10101000	1	False

Number of codes used=33


End of hp run 5.  Result of run:
[(-0.9841157230597924, 70000), ('21-05-09_17:56:02BST_NLearn_model_5_Alice_iter70000', '21-05-09_17:56:02BST_NLearn_model_5_Bob_iter70000')]
(-0.9841157230597924, 70000)


>>>> hp_run=6 of 8, time elapsed 6:59:48 of estimated 11:11:41, 
implying ending at 05:07:42BST on Monday 10 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (843402, 443788, 742412, 270619),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': 500,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=     20000 training nets give:
alice_loss.item()=0.6872134804725647	bob_loss.item()=0.5332567691802979

00100010	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00110011	[35, 36, 37, 38, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102]
10100111	[39, 40]
10111100	[103, 104, 105, 106, 107, 108, 109, 110, 111, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225]
00011000	[112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137]
00110000	[138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162]

00100010	86	False
00110011	233	False
10100111	175	False
10111100	207	True
00011000	3	False
00110000	89	False

Number of codes used=6

Iteration=     30000 training nets give:
alice_loss.item()=0.12520331144332886	bob_loss.item()=0.15534614026546478

00101010	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00110010	[40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85]
01010011	[86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108]
00110000	[109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132]
00111000	[133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193]
10111100	[194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233]

00101010	4	True
00110010	61	True
01010011	82	False
00110000	146	False
00111000	187	True
10111100	215	True

Number of codes used=6

Iteration=     40000 training nets give:
alice_loss.item()=0.10960264503955841	bob_loss.item()=0.14696437120437622

00101010	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00001010	[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
00001110	[32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]
10110010	[46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57]
00110010	[58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75]
01010011	[76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91]
11010011	[92, 93, 94, 95, 96, 97, 98, 99, 100, 101]
10010000	[102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118]
00010000	[119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147]
01111000	[148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173]
00111000	[174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196]
10111100	[197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213]
00111100	[214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241]
10101010	[242, 243, 244, 245]

00101010	250	True
00001010	6	False
00001110	15	False
10110010	63	False
00110010	61	True
01010011	80	True
11010011	102	False
10010000	126	False
00010000	132	True
01111000	157	True
00111000	185	True
10111100	224	False
00111100	226	True
10101010	237	False

Number of codes used=14

Iteration=     50000 training nets give:
alice_loss.item()=0.13589441776275635	bob_loss.item()=0.1027456670999527

01101010	[0, 1, 2, 3, 4, 5, 6, 7, 8, 252, 253, 254, 255]
01101110	[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
00000010	[21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43]
00110011	[44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57]
00110010	[58, 59, 60, 61, 62, 63, 64]
00010011	[65, 66, 67, 68, 69, 70, 71, 72]
00010110	[73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
01010011	[84, 85, 86, 87, 88, 89, 90]
01011000	[91, 92, 93]
11010011	[94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104]
00010101	[105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
01110000	[115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130]
00111001	[131, 132, 133, 134, 135, 136]
00000000	[137, 138, 139, 140, 141, 142, 143, 144, 145, 146]
01111000	[147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165]
10111000	[166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181]
00111000	[182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201]
00101000	[202, 203, 204, 205]
10111100	[206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220]
00111100	[221, 222]
11111100	[223]
10101000	[224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241]
10111101	[242, 243, 244]
10001010	[245, 246, 247]
00101010	[248, 249, 250, 251]

01101010	250	False
01101110	5	False
00000010	36	True
00110011	40	False
00110010	60	True
00010011	78	False
00010110	77	True
01010011	87	True
01011000	137	False
11010011	104	True
00010101	106	True
01110000	136	False
00111001	147	False
00000000	133	False
01111000	156	True
10111000	184	False
00111000	180	False
00101000	188	False
10111100	217	True
00111100	228	False
11111100	211	False
10101000	224	True
10111101	227	False
10001010	4	False
00101010	251	True

Number of codes used=25

Iteration=     60000 training nets give:
alice_loss.item()=0.007436027284711599	bob_loss.item()=0.07101525366306305

00101011	[0, 1, 2, 3, 253, 254, 255]
11101110	[4, 5, 6, 7, 8, 9, 10]
00001110	[11, 12, 13, 14, 15, 16, 17, 18]
00100010	[19, 20, 21, 22]
11001010	[23, 24, 25, 26, 27, 28, 29, 30, 31, 32]
00000010	[33, 34, 35, 36, 37, 38, 39]
00110011	[40, 41, 42, 43, 44, 45, 46, 47, 48]
00110110	[49, 50, 51, 52, 53]
00110010	[54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]
10110011	[65, 66, 67, 68, 69]
01110010	[70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80]
00011011	[81, 82]
01010011	[83, 84, 85, 86, 87, 88, 89, 90, 91, 92]
01111011	[93]
11010011	[94, 95, 96, 97, 98, 99, 100, 101]
11011011	[102, 103, 104, 105, 106, 107]
01010001	[108, 109, 110, 111, 112]
11010001	[113, 114, 115, 116, 117, 118, 119, 120, 121]
01100000	[122, 123, 124, 125, 126, 127, 128]
00010000	[129, 130, 131, 132, 133, 134, 135]
00000001	[136, 137, 138, 139, 140, 141]
00110000	[142, 143]
11111000	[144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155]
01111000	[156, 157, 158]
00100000	[159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171]
00111000	[172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186]
00101000	[187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203]
10111100	[204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221]
10101000	[222, 223, 224, 225, 226, 227, 228, 229]
00111100	[230, 231, 232, 233]
10110100	[234, 235, 236, 237, 238]
10111110	[239, 240, 241, 242, 243, 244, 245]
00101010	[246, 247, 248, 249, 250, 251]
10101010	[252]

00101011	4	False
11101110	254	False
00001110	21	False
00100010	32	False
11001010	33	False
00000010	44	False
00110011	38	False
00110110	43	False
00110010	48	False
10110011	57	False
01110010	77	True
00011011	71	False
01010011	82	False
01111011	90	False
11010011	90	False
11011011	106	True
01010001	120	False
11010001	118	True
01100000	128	True
00010000	134	True
00000001	136	True
00110000	151	False
11111000	146	True
01111000	156	True
00100000	172	False
00111000	176	True
00101000	189	True
10111100	213	True
10101000	218	False
00111100	227	False
10110100	229	False
10111110	242	True
00101010	251	True
10101010	5	False

Number of codes used=34

Iteration=     70000 training nets give:
alice_loss.item()=0.000598694896325469	bob_loss.item()=0.0015325142303481698

10111010	[0, 251, 252, 253, 254, 255]
00101011	[1, 2, 3, 4, 5, 6, 7]
11101011	[8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
11101010	[20, 21, 22, 23]
11001010	[24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 41]
00000010	[38, 39, 40]
00110111	[42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]
01011010	[54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]
10110010	[65]
00111011	[66, 67, 68, 69, 70, 71]
01110010	[72, 73, 74, 75, 76, 77, 78, 79, 80]
01001001	[81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92]
01111011	[93, 94]
01010100	[95, 96, 97, 98, 99, 100]
11011011	[101, 102, 103, 104, 105, 106, 107, 108, 109, 110]
01010000	[111, 112, 113, 114, 115]
11010001	[116, 117, 118]
01000000	[119, 120]
01100000	[121, 122, 123, 124, 125, 126, 127]
10010001	[128, 129, 130, 131]
00010000	[132, 133, 134, 135, 136, 137]
01110000	[138, 139, 140]
01111001	[141, 142, 143, 144]
11111000	[145, 146, 147, 148, 149]
01111000	[150, 151, 152, 153, 154]
00110100	[155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166]
00100000	[167, 168, 169, 170, 171, 172, 173]
00111000	[174, 175, 176, 177, 178, 179, 180, 181]
10111000	[182, 183, 184, 185, 186, 187]
00101000	[188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202]
10111100	[203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220]
10101000	[221]
00111100	[222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233]
10110100	[234, 235, 236]
10111110	[237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247]
00101010	[248, 249, 250]

10111010	0	True
00101011	7	True
11101011	11	True
11101010	15	False
11001010	39	False
00000010	40	True
00110111	48	True
01011010	62	True
10110010	62	False
00111011	68	True
01110010	79	True
01001001	88	True
01111011	79	False
01010100	103	False
11011011	103	True
01010000	115	True
11010001	116	True
01000000	115	False
01100000	125	True
10010001	128	True
00010000	132	True
01110000	135	False
01111001	140	False
11111000	148	True
01111000	153	True
00110100	159	True
00100000	173	True
00111000	174	True
10111000	183	True
00101000	192	True
10111100	217	True
10101000	222	False
00111100	224	True
10110100	222	False
10111110	243	True
00101010	254	False

Number of codes used=36


End of hp run 6.  Result of run:
[(-0.9725035913603642, 70000), ('21-05-09_17:56:02BST_NLearn_model_6_Alice_iter70000', '21-05-09_17:56:02BST_NLearn_model_6_Bob_iter70000')]
(-0.9725035913603642, 70000)


>>>> hp_run=7 of 8, time elapsed 8:24:02 of estimated 11:12:02, 
implying ending at 05:08:04BST on Monday 10 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (420915, 961830, 723900, 510954),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=     20000 training nets give:
alice_loss.item()=0.6435544490814209	bob_loss.item()=0.5074812173843384

00010010	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01110110	[87, 88, 89, 90, 91, 92, 93, 94, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147]
11010100	[95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
00001100	[148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163]

00010010	76	True
01110110	169	False
11010100	87	False
00001100	69	False

Number of codes used=4

Iteration=     30000 training nets give:
alice_loss.item()=0.26436662673950195	bob_loss.item()=0.2932344079017639

11001011	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01000111	[32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56]
00010010	[57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73]
11010100	[74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126]
01110110	[127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167]
01000010	[168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]

11001011	10	True
01000111	34	True
00010010	21	False
11010100	99	True
01110110	123	False
01000010	204	True

Number of codes used=6

Iteration=     40000 training nets give:
alice_loss.item()=0.12269283831119537	bob_loss.item()=0.20003890991210938

11001011	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]
01010111	[25, 26, 27, 28, 29, 37, 38, 39]
01000111	[30, 31, 32, 33, 34, 35, 36]
11010111	[40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
11000101	[62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75]
10010100	[76, 77, 78, 79, 80, 81, 82]
11010100	[83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97]
01111110	[98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126]
01100110	[127, 128, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172]
01110110	[129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139]
01000110	[173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187]
01000010	[188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208]
01000000	[209]
11101010	[210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250]
11101011	[251, 252, 253, 254, 255]

11001011	255	False
01010111	53	False
01000111	25	False
11010111	70	False
11000101	68	True
10010100	77	True
11010100	109	False
01111110	115	True
01100110	147	True
01110110	125	False
01000110	189	False
01000010	200	True
01000000	198	False
11101010	234	True
11101011	252	True

Number of codes used=15

Iteration=     50000 training nets give:
alice_loss.item()=0.0791400820016861	bob_loss.item()=0.19509026408195496

01000011	[0, 1, 2, 255]
10000010	[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
11000011	[15]
10001011	[16, 17, 18, 19, 20, 21]
01000111	[22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]
11000111	[33, 34, 35, 36, 37, 38, 39, 40, 41, 42]
01010111	[43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]
11000101	[54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68]
11010111	[69, 70, 71, 72, 73, 74, 75, 76, 77]
11010101	[78]
11010110	[79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94]
11010100	[95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112]
01111110	[113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125]
01110110	[126, 127, 128, 129]
11100110	[130, 131, 132, 133, 134, 135, 136, 137]
01100100	[138, 139]
01100110	[140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167]
01000100	[168]
01101110	[169]
01000110	[170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189]
01001110	[190, 191, 192, 193, 194, 195, 196, 197, 198]
01000010	[199, 200, 201, 202, 203, 204, 205]
01010010	[206, 207, 208, 209, 210, 211, 212, 213]
11000010	[214, 215, 216, 217, 218, 219, 220]
11101010	[221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236]
11100011	[237, 238, 239, 240]
01101011	[241, 242, 243, 244, 245, 246, 247]
11101011	[248, 249, 250, 251, 252]
11001011	[253, 254]

01000011	0	True
10000010	237	False
11000011	9	False
10001011	18	True
01000111	30	True
11000111	37	True
01010111	48	True
11000101	56	True
11010111	67	False
11010101	71	False
11010110	85	True
11010100	98	True
01111110	126	False
01110110	132	False
11100110	141	False
01100100	144	False
01100110	161	True
01000100	167	False
01101110	168	False
01000110	191	False
01001110	188	False
01000010	201	True
01010010	211	True
11000010	218	True
11101010	230	True
11100011	234	False
01101011	244	True
11101011	235	False
11001011	3	False

Number of codes used=29

Iteration=     60000 training nets give:
alice_loss.item()=0.020725209265947342	bob_loss.item()=0.10933493077754974

01001011	[0, 1, 2, 3, 4, 252, 253, 254, 255]
01000011	[5]
11001111	[6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
00000111	[21, 22, 23, 24, 25, 26]
01000111	[27, 28, 29]
00001101	[30, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41]
11011111	[35]
11000111	[42, 43, 44]
01010111	[45, 46, 47, 48, 49, 50, 51, 52, 53]
11000101	[54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
11010111	[64, 65, 66, 67, 68, 69, 70, 71]
11010000	[72, 73, 74, 75, 76, 77, 78]
11110101	[79, 80, 81, 82]
10010100	[83]
11010110	[84, 85, 86, 87, 88, 89]
11011110	[90, 91, 92, 93, 94, 95, 96, 97]
11010100	[98, 99, 100, 101, 102]
01010100	[103, 104, 105, 106]
11110100	[107, 108, 109, 110, 111, 112, 113, 114, 115, 116]
00111110	[117]
01111110	[118, 119]
01110100	[120, 121, 122, 123, 124, 125, 126, 127]
01110110	[128, 129, 130, 131, 132, 133, 134]
11100110	[135, 136, 137, 138, 139, 140, 141, 142, 143, 144]
01011100	[145, 146, 147]
01100110	[148, 149, 150, 151, 152, 153, 154, 155, 156]
01000100	[157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171]
01101110	[172, 173, 174, 175, 176, 177, 178, 179]
01001100	[180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198]
01100000	[199, 200, 201]
01000010	[202, 203, 204, 205]
01010010	[206, 207, 208, 209, 210, 211, 212]
11000010	[213, 214, 215, 216, 217]
11100001	[218, 219, 220, 221, 222, 223, 224, 225]
11101010	[226, 227, 228, 229, 230, 231, 232]
11001010	[233, 234, 235]
11100011	[236, 237, 238, 239, 240, 241]
01101011	[242, 243, 244, 245, 246]
11001000	[247, 248, 249, 250, 251]

01001011	1	True
01000011	255	False
11001111	37	False
00000111	32	False
01000111	35	False
00001101	46	False
11011111	47	False
11000111	44	True
01010111	50	True
11000101	61	True
11010111	70	True
11010000	72	True
11110101	81	True
10010100	77	False
11010110	83	False
11011110	94	True
11010100	94	False
01010100	110	False
11110100	117	False
00111110	115	False
01111110	121	False
01110100	129	False
01110110	133	True
11100110	148	False
01011100	105	False
01100110	159	False
01000100	169	True
01101110	163	False
01001100	177	False
01100000	196	False
01000010	209	False
01010010	212	True
11000010	217	True
11100001	229	False
11101010	224	False
11001010	230	False
11100011	233	False
01101011	244	True
11001000	252	False

Number of codes used=39

Iteration=     70000 training nets give:
alice_loss.item()=0.00034804194001480937	bob_loss.item()=0.0003145223599858582

01000011	[0, 1, 2, 3, 253, 254, 255]
11000011	[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
10001011	[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]
00001111	[28, 29, 30, 31, 32, 33, 34, 35, 36, 37]
11000111	[38, 39, 40, 41, 42, 43, 44]
01010111	[45, 46, 47, 48, 49, 50, 51, 52, 53, 54]
11000101	[55, 56, 57, 58, 59, 62, 63]
01000101	[60, 61]
11010111	[64, 65, 66, 67, 68, 69, 70]
11010000	[71, 72, 73]
11010101	[74, 75, 76, 77, 78]
11110101	[79, 80, 81, 82, 83, 84]
11010110	[85, 86, 87, 88]
11011110	[89, 90, 91, 92, 93, 94, 95, 96]
11010100	[97, 98, 99, 100, 101, 102, 103]
01010100	[104, 105, 106, 107]
01111100	[108, 109]
11110100	[110, 111]
00111110	[112, 113, 114, 115, 116, 117, 118, 119, 120, 121]
01110100	[122, 123, 124, 125, 126, 127]
01110110	[128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139]
00100100	[140, 141, 142]
01101100	[143, 144, 145, 146, 147, 148, 149, 150, 151]
01100110	[152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163]
01000100	[164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175]
00100110	[176, 177, 178, 179, 180, 181, 182]
01000110	[183, 184, 185, 186, 187, 188, 189, 190]
01100010	[191, 192, 193, 194, 195, 196]
01100000	[197, 198, 199, 200]
01000000	[201]
01000010	[202, 203, 204, 205]
01010010	[206, 207, 208, 209, 210, 211, 212, 213]
11000010	[214, 215, 216, 217, 218, 219, 220]
11000000	[221, 222, 223]
11101010	[224, 225, 226, 227]
11100011	[228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240]
01101011	[241, 242, 243, 244, 245, 246]
11001000	[247, 248, 249, 250, 251, 252]

01000011	251	False
11000011	9	True
10001011	22	True
00001111	31	True
11000111	48	False
01010111	51	True
11000101	62	True
01000101	53	False
11010111	62	False
11010000	69	False
11010101	76	True
11110101	79	True
11010110	89	False
11011110	96	True
11010100	94	False
01010100	112	False
01111100	113	False
11110100	120	False
00111110	121	True
01110100	129	False
01110110	131	True
00100100	149	False
01101100	153	False
01100110	152	True
01000100	179	False
00100110	172	False
01000110	185	True
01100010	192	True
01100000	190	False
01000000	206	False
01000010	206	False
01010010	206	True
11000010	217	True
11000000	216	False
11101010	221	False
11100011	234	True
01101011	249	False
11001000	247	True

Number of codes used=38


End of hp run 7.  Result of run:
[(-0.9898152093236664, 70000), ('21-05-09_17:56:02BST_NLearn_model_7_Alice_iter70000', '21-05-09_17:56:02BST_NLearn_model_7_Bob_iter70000')]
(-0.9898152093236664, 70000)


>>>> hp_run=8 of 8, time elapsed 9:47:50 of estimated 11:11:48, 
implying ending at 05:07:50BST on Monday 10 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (420915, 961830, 723900, 510954),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': 500,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=     20000 training nets give:
alice_loss.item()=0.6600950360298157	bob_loss.item()=0.5041900873184204

00110100	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11110011	[49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
11111001	[72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245]
00110001	[85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134]
00010111	[135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195]

00110100	235	False
11110011	151	False
11111001	141	False
00110001	229	False
00010111	214	False

Number of codes used=5

Iteration=     30000 training nets give:
alice_loss.item()=0.207139253616333	bob_loss.item()=0.19099637866020203

01000011	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]
10011011	[65, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177]
01010000	[66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135]
00010111	[178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219]
00110100	[220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]

01000011	14	True
10011011	131	False
01010000	104	True
00010111	183	True
00110100	254	True

Number of codes used=5

Iteration=     40000 training nets give:
alice_loss.item()=0.09140999615192413	bob_loss.item()=0.09806237369775772

00110100	[0, 1, 2, 3, 4, 5, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01000010	[6, 7, 8, 12, 13]
01000001	[9, 10, 11]
01000011	[14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
11000011	[32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]
00010000	[49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78]
11010000	[79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93]
01010000	[94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111]
01010001	[112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122]
10010000	[123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133]
10011011	[134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148]
10111011	[149, 150, 151]
10011111	[152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174]
00010111	[175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197]
00010100	[198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225]
10110100	[226, 227, 228, 229, 230]
01100011	[231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242]

00110100	253	True
01000010	18	False
01000001	3	False
01000011	13	False
11000011	23	False
00010000	65	True
11010000	80	True
01010000	75	False
01010001	108	False
10010000	57	False
10011011	154	False
10111011	151	True
10011111	161	True
00010111	188	True
00010100	214	True
10110100	237	False
01100011	12	False

Number of codes used=17

Iteration=     50000 training nets give:
alice_loss.item()=0.030956551432609558	bob_loss.item()=0.10138881206512451

01001011	[0, 1, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00000010	[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
00100011	[15]
01000010	[16, 17, 18, 19]
11000011	[20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]
01100001	[37]
11000010	[38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
11100010	[48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60]
00110000	[61, 62, 63, 64, 65, 66, 67, 68]
00010000	[69, 70, 71, 72, 73, 74]
11001000	[75, 76, 77, 78, 79, 80]
01010100	[81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93]
11111000	[94, 95, 96, 97, 98, 99, 100, 101, 102]
01010001	[103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115]
11010001	[116, 117, 118, 119, 120, 121]
11011011	[122, 123, 124, 125]
10010001	[126, 127, 128, 129, 130, 131, 132, 133]
10010011	[134, 135]
10011011	[136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147]
01111011	[148]
10011101	[149, 150, 151, 152, 153, 154, 155]
10010111	[156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166]
00011001	[167, 168, 169, 170, 171, 172, 173, 174, 175, 176]
00000111	[177, 178]
00010111	[179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194]
00000100	[195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205]
00010100	[206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217]
00100100	[218, 219, 220, 221, 222, 223, 224, 225, 226]
10111100	[227, 228, 229, 230, 231]
00011100	[232]
10010100	[233, 234, 235, 236, 237, 238, 239, 240, 241, 242]

01001011	9	False
00000010	19	False
00100011	8	False
01000010	28	False
11000011	27	True
01100001	28	False
11000010	46	True
11100010	48	True
00110000	77	False
00010000	73	True
11001000	84	False
01010100	83	True
11111000	92	False
01010001	104	True
11010001	111	False
11011011	136	False
10010001	135	False
10010011	149	False
10011011	160	False
01111011	149	False
10011101	157	False
10010111	172	False
00011001	150	False
00000111	185	False
00010111	192	True
00000100	205	True
00010100	214	True
00100100	234	False
10111100	228	True
00011100	220	False
10010100	241	True

Number of codes used=31

Iteration=     60000 training nets give:
alice_loss.item()=0.002487759105861187	bob_loss.item()=0.01037612184882164

00110100	[0, 1, 2, 3, 4, 249, 250, 251, 252, 253, 254, 255]
11000111	[5]
01000001	[6, 7, 8, 9, 14, 15, 16]
00100011	[10, 11, 12, 13]
11001011	[17, 18, 19, 20, 21, 22, 23, 24]
11000011	[25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
10000010	[36, 37, 38]
11000010	[39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]
11100010	[51, 52, 53, 54, 55, 56, 57, 58]
11000000	[59, 60, 61, 62]
00010000	[63, 64, 65, 66, 67, 68, 69, 70]
11010100	[71, 72, 73, 74, 75, 76]
11001000	[77, 78, 79, 80, 81, 82]
01010000	[83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93]
11111000	[94, 95, 96, 97, 98]
00010001	[99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111]
11010001	[112, 113, 114, 115, 116, 117]
10010010	[118, 119, 120, 121]
11011001	[122, 123]
01011001	[124, 125, 126, 127, 128]
11011011	[129, 130, 131, 132, 133, 134, 135, 136]
10010001	[137, 138, 139, 140]
10011001	[141]
01111011	[142, 143, 144, 145, 146, 147, 148, 149, 150]
10011101	[151, 152, 153, 154, 155, 156]
10111011	[157, 158, 159, 160]
10000111	[161, 162, 163, 164, 165, 166, 167, 168]
00010011	[169, 170, 171, 172, 173, 174, 175, 176, 177]
10101111	[178, 179, 180, 181, 182]
00110111	[183, 184, 185, 186, 187, 188]
00010111	[189, 190, 191, 192, 193, 194, 195, 196, 197]
00000100	[198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208]
00010100	[209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222]
10111100	[223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233]
10010100	[234, 235, 236, 237, 238, 239, 240, 241, 242]
01100111	[243, 244, 245, 246, 247, 248]

00110100	0	True
11000111	20	False
01000001	23	False
00100011	7	False
11001011	27	False
11000011	31	True
10000010	30	False
11000010	42	True
11100010	46	False
11000000	77	False
00010000	88	False
11010100	67	False
11001000	82	True
01010000	96	False
11111000	95	True
00010001	110	True
11010001	109	False
10010010	116	False
11011001	115	False
01011001	126	True
11011011	129	True
10010001	129	False
10011001	123	False
01111011	146	True
10011101	155	True
10111011	168	False
10000111	164	True
00010011	174	True
10101111	180	True
00110111	186	True
00010111	183	False
00000100	207	True
00010100	210	True
10111100	235	False
10010100	242	True
01100111	248	True

Number of codes used=36

Iteration=     70000 training nets give:
alice_loss.item()=0.009007655084133148	bob_loss.item()=0.003198394551873207

00110100	[0, 1, 2, 3, 252, 253, 254, 255]
00100011	[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
01000011	[15, 16, 17, 18, 19, 20]
01100011	[21, 22]
01001010	[23, 24, 25, 26, 27]
11000011	[28, 29, 30, 31, 32, 33]
10000010	[34, 35, 36, 37]
11100011	[38, 39, 40, 41]
11000010	[42, 43, 44, 45, 46, 47, 48]
11100010	[49, 50, 51, 52, 53, 54, 55, 56, 57]
11000100	[58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68]
11010100	[69, 70, 71, 72, 73, 74, 75]
11001000	[76, 77, 78]
01010100	[79, 80, 81, 82, 83, 84, 85]
00010010	[86, 87, 88, 89, 90, 91]
01010000	[92, 93, 94, 95, 96]
11111000	[97, 98, 99, 100]
11010001	[101, 102, 103, 109, 110, 111, 112, 113, 114, 115, 116, 117]
00010001	[104, 105, 106, 107, 108]
11011001	[118, 119, 120, 121, 122, 123]
01011001	[124, 125, 126, 127, 128]
11011011	[129, 130, 131, 132, 133, 134, 135]
10010001	[136, 137]
10111001	[138, 139, 140, 141, 142, 143, 144]
01111011	[145, 146, 147, 148, 149, 150]
10011101	[151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161]
10000111	[162, 163, 164, 165]
10001011	[166, 167, 168, 169, 170]
00010011	[171, 172, 173, 174, 175, 176, 177]
10101111	[178, 179, 180, 181, 182, 183]
00000111	[184, 185, 186, 187]
00110111	[188, 189, 190, 191, 192]
00001101	[193, 194, 195, 196, 197, 198, 199]
00000100	[200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212]
00010100	[213, 214, 215, 216, 217, 218, 219, 220]
00011100	[221, 222]
00110110	[223, 224, 225, 226, 227, 228, 229, 230, 231]
10111100	[232, 233, 234]
10010100	[235, 236, 237, 238, 239, 240, 241]
01100111	[242, 243, 244]
01000101	[245, 246, 247, 248, 249, 250, 251]

00110100	1	True
00100011	6	True
01000011	20	True
01100011	14	False
01001010	24	True
11000011	34	False
10000010	32	False
11100011	38	True
11000010	43	True
11100010	59	False
11000100	69	False
11010100	81	False
11001000	85	False
01010100	84	True
00010010	86	True
01010000	87	False
11111000	93	False
11010001	105	False
00010001	111	False
11011001	124	False
01011001	112	False
11011011	125	False
10010001	119	False
10111001	141	True
01111011	146	True
10011101	158	True
10000111	167	False
10001011	169	True
00010011	172	True
10101111	178	True
00000111	190	False
00110111	186	False
00001101	204	False
00000100	210	True
00010100	214	True
00011100	217	False
00110110	225	True
10111100	230	False
10010100	245	False
01100111	249	False
01000101	249	True

Number of codes used=41


End of hp run 8.  Result of run:
[(-0.9793381550039557, 70000), ('21-05-09_17:56:02BST_NLearn_model_8_Alice_iter70000', '21-05-09_17:56:02BST_NLearn_model_8_Bob_iter70000')]
(-0.9793381550039557, 70000)



Time taken over all 8 given sets of hyperparameters=11:11:26, averaging 1:23:56 per run


 ---- Table of results ----

  code  hp_run  result
 00000       1  (-0.986, 70000)
 00001       2  (-0.987, 70000)
 10000       3  (-0.978, 70000)
 10001       4  (-0.985, 70000)
 20000       5  (-0.984, 70000)
 20001       6  (-0.973, 70000)
 30000       7  (-0.990, 70000)
 30001       8  (-0.979, 70000)
 --------------------------

++++ Best result was (-0.990, 70000) on hp_run=7 with
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (420915, 961830, 723900, 510954),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_LAYERS': 3,
	'BOB_WIDTH': 50,
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_STRATEGY': 'circular_vocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': ('SGD', '{"lr": 0.01}'),
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': ('torch.nn.MSE', {}),
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
	'n_rng': Generator(PCG64)
	'ne_rng': Generator(PCG64)
	't_rng': <torch._C.Generator object at 0x7f39753465f0>
	'te_rng': <torch._C.Generator object at 0x7f3975346650>
}


End closed log for run 21-05-09_17:56:02BST
