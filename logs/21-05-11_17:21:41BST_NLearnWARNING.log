****WARNING NOT CLOSED - MAY BE DUE TO ERROR***
SMOOTHING_LENGTH = 10000
SAVE_PERIOD = 100000
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # Used: "cuda"
MODEL_FOLDER = 'models'
CONFIGS_FOLDER = 'configs'
LOGS_FOLDER = 'logs'

hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': [(714844, 936892, 888616, 165835)],
	'ALICE_NET': 'MaxNet("In", 3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 16,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}



>>>> hp_run=1 of 1
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (714844, 936892, 888616, 165835),
	'ALICE_NET': 'MaxNet("In", 3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 16,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7705517503711219

Iteration=     20000 training nets give:
alice_loss.item()=0.6377224326133728	bob_loss.item()=0.5304603576660156

00011110	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10001001	[47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134]
01110100	[135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160]
01111100	[161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207]
10101001	[208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219]

00011110	0	True
10001001	0	False
01110100	0	False
01111100	0	False
10101001	0	False

Number of codes used=5

Iteration=     30000 training nets give:
alice_loss.item()=0.21044352650642395	bob_loss.item()=0.30473005771636963

00011110	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10000111	[48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125]
10010011	[126, 127, 128, 129]
10111100	[130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187]
00111001	[188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240]

00011110	0	True
10000111	0	False
10010011	0	False
10111100	0	False
00111001	0	False

Number of codes used=5

Iteration=     40000 training nets give:
alice_loss.item()=0.11954563856124878	bob_loss.item()=0.1701621115207672

00010110	[0, 1, 2, 3, 248, 249, 250, 251, 252, 253, 254, 255]
00011110	[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
10011110	[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]
00000011	[46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60]
00000111	[61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77]
10000111	[78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96]
10010011	[97, 98, 99]
11000111	[100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120]
11111100	[121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153]
10111100	[154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177]
10111000	[178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189]
10111110	[190, 191, 192]
00110001	[193, 194, 195, 196, 197, 198, 199, 200, 201]
00111000	[202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216]
00011001	[217, 218, 219, 220, 221, 222]
00111011	[223, 224, 225, 226, 227, 228, 229, 230, 231]
00101001	[232]
00011111	[233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247]

00010110	0	True
00011110	0	False
10011110	0	False
00000011	0	False
00000111	0	False
10000111	0	False
10010011	0	False
11000111	0	False
11111100	0	False
10111100	0	False
10111000	0	False
10111110	0	False
00110001	0	False
00111000	0	False
00011001	0	False
00111011	0	False
00101001	0	False
00011111	0	False

Number of codes used=18

Iteration=     50000 training nets give:
alice_loss.item()=0.13246361911296844	bob_loss.item()=0.17274457216262817

00001010	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 253, 254, 255]
00001111	[14, 15]
10011110	[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]
10001100	[40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]
00000101	[54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75]
10011111	[76]
10000110	[77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88]
10011011	[89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]
11100111	[101, 102, 103, 104, 105, 106, 107, 108, 109, 110]
10110011	[111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124]
10100111	[125]
11111100	[126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142]
11110100	[143, 144]
11110110	[145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157]
10111100	[158, 159, 160, 161, 162, 163, 164, 165, 166, 167]
10111000	[168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182]
10111110	[183, 184, 185, 186, 187, 188, 189, 190, 191]
10111010	[192, 193, 194, 195]
00111100	[196, 197, 198, 199, 200, 201, 202, 203]
00111001	[204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221]
01111000	[222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232]
00110010	[233, 234, 235, 236, 237, 238]
01111010	[239, 240, 241, 242, 243]
00011011	[244, 245, 246, 247, 248, 249, 250, 251, 252]

00001010	0	True
00001111	0	False
10011110	0	False
10001100	0	False
00000101	0	False
10011111	0	False
10000110	0	False
10011011	0	False
11100111	0	False
10110011	0	False
10100111	0	False
11111100	0	False
11110100	0	False
11110110	0	False
10111100	0	False
10111000	0	False
10111110	0	False
10111010	0	False
00111100	0	False
00111001	0	False
01111000	0	False
00110010	0	False
01111010	0	False
00011011	0	False

Number of codes used=24

Iteration=     60000 training nets give:
alice_loss.item()=0.019311655312776566	bob_loss.item()=0.09011763334274292

01011110	[0, 1, 2, 3, 4, 5, 6, 7, 8, 254, 255]
00001010	[9]
00001110	[10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]
10011110	[22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
11011110	[32, 33, 34, 35, 36, 37]
10001100	[38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
00000111	[52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]
00000101	[65, 66, 67, 68, 69, 70]
10000100	[71, 72, 73, 74, 75, 76, 77, 78]
10000110	[79, 80, 81, 82, 83, 84, 85]
10001011	[86, 87, 88, 89]
10011001	[90]
10011011	[91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102]
10010001	[103, 104, 105, 106, 107]
10100110	[108, 109, 110]
10110111	[111, 112, 113, 114, 115, 116]
10110011	[117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
11111000	[128, 129, 130, 131, 132, 137, 138, 139, 140]
11111100	[133, 134, 135, 136]
11110100	[141, 142, 143, 144, 145, 146, 147, 148, 149, 150]
11110110	[151]
01110100	[152, 153, 154, 155, 156, 157, 158, 159, 160, 161]
10110100	[162, 163, 164, 165, 166, 167]
10111000	[168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180]
10111110	[181, 182, 183, 184, 185]
00110100	[186, 187, 188, 189]
10111010	[190, 191, 192, 193, 194, 195, 196]
10111011	[197, 198, 199, 200, 201, 202, 203, 204, 205]
00110001	[206, 207]
00100000	[208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219]
00111011	[220, 221, 222]
00110011	[223, 224]
00011001	[225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239]
01111010	[240, 241, 242, 243, 244]
00011011	[245, 246, 247, 248, 249, 250, 251, 252, 253]

01011110	0	True
00001010	0	False
00001110	0	False
10011110	0	False
11011110	0	False
10001100	0	False
00000111	0	False
00000101	0	False
10000100	0	False
10000110	0	False
10001011	0	False
10011001	0	False
10011011	0	False
10010001	0	False
10100110	0	False
10110111	0	False
10110011	0	False
11111000	0	False
11111100	0	False
11110100	0	False
11110110	0	False
01110100	0	False
10110100	0	False
10111000	0	False
10111110	0	False
00110100	0	False
10111010	0	False
10111011	0	False
00110001	0	False
00100000	0	False
00111011	0	False
00110011	0	False
00011001	0	False
01111010	0	False
00011011	0	False

Number of codes used=35

Iteration=     70000 training nets give:
alice_loss.item()=0.00036634039133787155	bob_loss.item()=0.000860293977893889

00011010	[0, 1, 2, 255]
01011110	[3, 4, 5, 6, 7, 8]
00001110	[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
10011110	[21, 22, 23, 24, 25, 26, 27]
11011110	[28, 29, 30, 31, 32, 33, 34, 35, 36]
00000110	[37, 38, 39]
10001110	[40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]
01000011	[54]
00000101	[55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67]
00100111	[68, 69, 70, 71, 72]
10000100	[73, 74, 75, 76, 77, 78, 79, 80]
10000110	[81, 82]
10010111	[83, 84, 85, 86]
10000111	[87, 88, 89, 90, 91]
10011011	[92, 93]
11000110	[94, 95, 96, 97, 98]
10000011	[99, 100]
11100111	[101, 102, 103, 104, 105, 106, 107, 108]
11010011	[109, 110, 111, 112]
10110111	[113, 114]
10110011	[115, 116, 117, 118, 119]
11101100	[120, 121, 122, 123, 124, 125, 126, 127, 128, 129]
11111100	[130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140]
11110100	[141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151]
11111110	[152, 153, 154, 155, 156]
10110100	[157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167]
10111100	[168, 169]
10111000	[170, 171, 172, 173]
10111101	[174, 175, 176, 177, 178, 179, 180]
10111110	[181, 182, 183, 184, 185, 186]
00110100	[187, 188, 189, 190, 191]
00111100	[192, 193, 194, 195]
10111001	[196, 197, 198, 199, 200]
00110001	[201, 202, 203, 204, 205, 206]
01111001	[207, 208, 209, 210, 211, 212, 213]
00110000	[214, 215, 216, 217]
00110011	[218, 219, 220, 221, 222, 223, 224]
00100001	[225, 226, 227, 228, 229, 230]
00011001	[231, 232, 233, 234, 235]
00101001	[236, 237, 238, 239]
01011111	[240, 241, 242, 243, 244, 245]
00001000	[246]
00011011	[247, 248, 249, 250, 251, 252]
00010110	[253, 254]

00011010	0	True
01011110	0	False
00001110	0	False
10011110	0	False
11011110	0	False
00000110	0	False
10001110	0	False
01000011	0	False
00000101	0	False
00100111	0	False
10000100	0	False
10000110	0	False
10010111	0	False
10000111	0	False
10011011	0	False
11000110	0	False
10000011	0	False
11100111	0	False
11010011	0	False
10110111	0	False
10110011	0	False
11101100	0	False
11111100	0	False
11110100	0	False
11111110	0	False
10110100	0	False
10111100	0	False
10111000	0	False
10111101	0	False
10111110	0	False
00110100	0	False
00111100	0	False
10111001	0	False
00110001	0	False
01111001	0	False
00110000	0	False
00110011	0	False
00100001	0	False
00011001	0	False
00101001	0	False
01011111	0	False
00001000	0	False
00011011	0	False
00010110	0	False

Number of codes used=44

****WARNING NOT CLOSED - MAY BE DUE TO ERROR***
