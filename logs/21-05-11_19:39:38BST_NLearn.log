The closed log for run 21-05-11_19:39:38BST

SMOOTHING_LENGTH = 10000
SAVE_PERIOD = 100000
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # Used: "cuda"
MODEL_FOLDER = 'models'
CONFIGS_FOLDER = 'configs'
LOGS_FOLDER = 'logs'

hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': [(714844, 936892, 888616, 165835)],
	'ALICE_NET': ['MaxNet("In", 3, 50)', 'MaxNet("In", 3, 50, bias_included=False)'],
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 16,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}



>>>> hp_run=1 of 2
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (714844, 936892, 888616, 165835),
	'ALICE_NET': 'MaxNet("In", 3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 16,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7705517503711219

Iteration=     20000 training nets give:
alice_loss.item()=0.5436853170394897	bob_loss.item()=0.5285301208496094

00010101	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01111111	[83, 84, 85]
01101001	[86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152]
10010010	[153, 154, 155, 156, 157, 158, 159, 160]
10011011	[161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222]
00101110	[223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238]

00010101	167	False
01111111	122	False
01101001	137	True
10010010	144	False
10011011	145	False
00101110	139	False

Number of codes used=6

Iteration=     30000 training nets give:
alice_loss.item()=0.19484782218933105	bob_loss.item()=0.22553208470344543

00110111	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11011101	[32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
01111111	[72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117]
01101001	[118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150]
10010010	[151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215]

00110111	253	True
11011101	64	True
01111111	94	True
01101001	122	True
10010010	182	True

Number of codes used=5

Iteration=     40000 training nets give:
alice_loss.item()=0.10934203863143921	bob_loss.item()=0.11074751615524292

00110111	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
00110101	[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38]
11011101	[39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]
11001101	[54]
11011100	[55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78]
01111101	[79, 80, 81, 82, 83, 84]
01111110	[85, 86, 87, 88, 89, 102, 103]
01111111	[90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101]
01001001	[104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117]
01101001	[118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136]
01101011	[137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151]
01101000	[152]
10111011	[153, 154, 155, 156, 157]
10010010	[158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181]
10011010	[182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196]
10101000	[197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212]
10010110	[213, 214, 215, 216, 217, 218, 219]
01110110	[220, 221, 222, 223, 224, 225, 226, 227, 228]
10100111	[229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243]
00100111	[244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]

00110111	254	False
00110101	5	False
11011101	58	False
11001101	57	False
11011100	66	True
01111101	91	False
01111110	103	True
01111111	93	True
01001001	117	True
01101001	126	True
01101011	130	False
01101000	136	False
10111011	212	False
10010010	186	False
10011010	191	True
10101000	200	True
10010110	208	False
01110110	250	False
10100111	250	False
00100111	248	True

Number of codes used=20

Iteration=     50000 training nets give:
alice_loss.item()=0.09306006133556366	bob_loss.item()=0.19541913270950317

11010111	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 251, 252, 253, 254, 255]
01110101	[12, 13]
11110101	[14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]
11000101	[33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]
11001101	[46, 47, 48, 49]
01011100	[50, 51, 52, 53]
01011101	[54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]
11111111	[70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84]
01111101	[85, 86, 87, 88]
01111110	[89, 90, 91, 92, 93, 94, 95, 96, 97, 98]
01101111	[99, 100, 101, 102, 103, 104, 105, 106, 107]
01001001	[108, 109, 110]
01111011	[111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121]
11101001	[122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133]
01101011	[134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
01001000	[144, 145, 146, 147, 148]
00101000	[149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161]
10011011	[162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177]
00011011	[178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189]
10100000	[190]
10000111	[191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205]
10100010	[206, 207, 208, 209, 210, 211, 212, 213, 214, 215]
01010110	[216, 217, 218, 219, 220, 221, 222]
00011111	[223, 224, 225, 226, 227, 228, 229, 230, 231, 232]
00101111	[233, 234]
10111111	[235, 236, 237, 238, 239]
00010111	[240, 241, 242]
01110110	[243, 244, 245, 246, 247, 248, 249, 250]

11010111	0	True
01110101	4	False
11110101	26	True
11000101	38	True
11001101	54	False
01011100	67	False
01011101	66	True
11111111	79	True
01111101	81	False
01111110	96	True
01101111	108	False
01001001	117	False
01111011	120	True
11101001	127	True
01101011	137	True
01001000	136	False
00101000	143	False
10011011	177	True
00011011	183	True
10100000	186	False
10000111	196	True
10100010	196	False
01010110	232	False
00011111	228	True
00101111	239	False
10111111	239	True
00010111	249	False
01110110	240	False

Number of codes used=28

Iteration=     60000 training nets give:
alice_loss.item()=0.032274141907691956	bob_loss.item()=0.19358748197555542

00110111	[0, 1, 2, 251, 252, 253, 254, 255]
10110101	[3, 4, 5, 6]
00110101	[7, 8, 9, 10, 11, 12]
11110101	[13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
11010101	[36]
01010101	[37, 38, 39, 40]
11100101	[41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52]
10011100	[53, 54, 55, 56, 57, 58]
11011100	[59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]
11111111	[73, 74, 75, 76, 77, 78, 79, 80]
01011111	[81, 82, 83]
01111101	[84, 85, 86, 87, 88, 89, 90]
01111111	[91, 92, 93, 94, 95, 96]
00101101	[97, 98, 99, 100, 101, 102]
01101111	[103, 104, 105, 106, 107, 108, 109]
01011001	[110, 111, 112, 113, 114, 115, 116, 117, 118, 119]
01111011	[120, 121]
01111001	[122, 123, 124, 125, 126, 127, 128, 129]
11101011	[130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
00101000	[144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162]
10011011	[163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174]
00011011	[175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185]
10001011	[186, 187]
10011010	[188]
10001001	[189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213]
10010110	[214, 215]
10110110	[216, 217, 218, 219, 220]
11010110	[221, 222, 223, 224, 225, 226, 227, 228]
10111110	[229]
00111110	[230, 231, 232]
01000111	[233, 234, 235, 236, 237, 238, 239, 240, 241, 242]
00110110	[243, 244, 245]
01100111	[246, 247, 248]
01110111	[249, 250]

00110111	4	False
10110101	7	False
00110101	9	True
11110101	24	True
11010101	29	False
01010101	30	False
11100101	47	True
10011100	62	False
11011100	62	True
11111111	73	True
01011111	78	False
01111101	83	False
01111111	91	True
00101101	95	False
01101111	107	True
01011001	111	True
01111011	120	True
01111001	128	True
11101011	138	True
00101000	156	True
10011011	169	True
00011011	179	True
10001011	179	False
10011010	192	False
10001001	202	True
10010110	214	True
10110110	218	True
11010110	221	True
10111110	224	False
00111110	230	True
01000111	236	True
00110110	251	False
01100111	245	False
01110111	252	False

Number of codes used=34

Iteration=     70000 training nets give:
alice_loss.item()=0.0004928247653879225	bob_loss.item()=0.000973197806160897

00100101	[0, 254, 255]
11010111	[1, 2, 3]
10110101	[4, 5, 6, 7]
00110101	[8, 9]
01110101	[10, 11, 12, 13, 14, 15, 16, 17]
11110101	[18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]
11010101	[29, 30, 31, 32, 33, 34, 35]
11000101	[36, 37, 38, 39]
11010100	[40, 41, 42, 43, 44]
10011101	[45, 46, 47, 48]
11001101	[49, 50, 51, 52, 53, 54]
11001100	[55, 56, 57, 58, 59, 60, 61]
11111100	[62]
01001100	[63, 64, 65, 66, 67, 68, 69, 70, 71, 72]
11011111	[73, 74, 75, 76, 77, 78]
11111110	[79]
01011111	[80, 81, 82, 83, 84]
01111101	[85, 86, 87, 88, 89, 90]
01111111	[91, 92]
01111100	[93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103]
01101111	[104, 105, 106, 107, 108, 109]
01000001	[110, 111, 112, 113]
01001001	[114, 115, 116, 117, 118, 119, 120]
01111011	[121, 122]
01110001	[123, 124, 125, 126, 127]
11001001	[128, 129, 130, 131, 132, 133, 134]
01101000	[135, 136, 137, 138]
11101011	[139, 140, 141, 142, 143, 144, 145, 146]
00101000	[147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158]
00001000	[159, 160, 161, 162, 163, 164, 165, 166, 167]
10011011	[168, 169, 170, 171, 172, 173, 174, 175, 176]
00011011	[177, 178, 179, 180, 181]
10001011	[182, 183, 184, 185]
10011010	[186, 187, 188]
10100000	[189, 190]
10000111	[191, 192, 193, 194, 195, 196, 197]
10001001	[198, 199, 200, 201, 202, 203, 204]
10101010	[205]
10111000	[206, 207, 208]
10010110	[209, 210, 211, 212, 213, 214, 215]
10110110	[216, 217, 218, 219, 220]
00110011	[221, 222, 223, 224]
00011111	[225, 226]
10111110	[227, 228, 229, 230]
00111110	[231, 232, 233, 234]
01000111	[235, 236, 237, 238, 239, 240, 241]
01100111	[242, 243, 244, 245]
01110111	[246, 247, 248, 249, 250]
00100111	[251, 252, 253]

00100101	250	False
11010111	1	True
10110101	6	True
00110101	8	True
01110101	16	True
11110101	25	True
11010101	33	True
11000101	39	True
11010100	41	True
10011101	45	True
11001101	48	False
11001100	59	True
11111100	61	False
01001100	68	True
11011111	80	False
11111110	81	False
01011111	81	True
01111101	87	True
01111111	90	False
01111100	99	True
01101111	103	False
01000001	113	True
01001001	116	True
01111011	117	False
01110001	124	True
11001001	132	True
01101000	135	True
11101011	136	False
00101000	155	True
00001000	166	True
10011011	171	True
00011011	177	True
10001011	181	False
10011010	192	False
10100000	193	False
10000111	200	False
10001001	207	False
10101010	201	False
10111000	200	False
10010110	213	True
10110110	221	False
00110011	217	False
00011111	224	False
10111110	224	False
00111110	228	False
01000111	237	True
01100111	245	True
01110111	251	False
00100111	253	True

Number of codes used=49


End of hp run 1.  Result of run:
[(-0.9919411030565034, 70000), ('21-05-11_19:39:38BST_NLearn_model_1_Alice_iter70000', '21-05-11_19:39:38BST_NLearn_model_1_Bob_iter70000')]
(-0.9919411030565034, 70000)


>>>> hp_run=2 of 2, time elapsed 0:33:35 of estimated 1:07:10, 
implying ending at 20:46:48BST on Tuesday 11 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (714844, 936892, 888616, 165835),
	'ALICE_NET': 'MaxNet("In", 3, 50, bias_included=False)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 16,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7705517503711219

Iteration=     20000 training nets give:
alice_loss.item()=0.841773271560669	bob_loss.item()=0.7985044717788696

10111000	[0, 1, 2, 3, 4, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01111000	[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
01100110	[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]
00011101	[35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84]
00101000	[85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]
10001100	[120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154]
11100101	[155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236]

10111000	64	False
01111000	72	False
01100110	94	False
00011101	82	True
00101000	85	True
10001100	33	False
11100101	117	False

Number of codes used=7

Iteration=     30000 training nets give:
alice_loss.item()=0.21804393827915192	bob_loss.item()=0.16344933211803436

11111011	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00011101	[49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80]
00101000	[81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]
10000001	[120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194]
11110110	[195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246]

11111011	24	True
00011101	59	True
00101000	98	True
10000001	153	True
11110110	245	True

Number of codes used=5

Iteration=     40000 training nets give:
alice_loss.item()=0.1116660088300705	bob_loss.item()=0.15006840229034424

11100110	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
11001011	[32, 33, 34]
11011011	[35, 36]
00011100	[37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57]
00111011	[58, 59, 60, 61, 62, 63, 64, 65, 66, 67]
00011101	[68, 69, 70, 71, 72, 73, 74, 75, 76]
10110011	[77, 78]
00101010	[79]
00101100	[80]
00101000	[81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110]
10000011	[111, 112, 113, 114, 115, 116, 117]
10000101	[118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139]
10100001	[140, 141]
10000001	[142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183]
10000000	[184, 185, 186]
11110100	[187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246]
11110110	[217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236]
11110010	[247, 248, 249]
11111010	[250, 251, 252, 253, 254, 255]

11100110	233	False
11001011	27	False
11011011	28	False
00011100	62	False
00111011	54	False
00011101	59	False
10110011	11	False
00101010	87	False
00101100	97	False
00101000	106	True
10000011	138	False
10000101	130	True
10100001	156	False
10000001	152	True
10000000	148	False
11110100	226	False
11110110	237	False
11110010	238	False
11111010	3	False

Number of codes used=19

Iteration=     50000 training nets give:
alice_loss.item()=0.09366163611412048	bob_loss.item()=0.15267892181873322

11111010	[0, 1, 2, 3, 4, 5, 6, 7, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11111001	[8, 9, 10, 11]
10111011	[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]
11111011	[25, 26, 27, 28]
10011011	[29, 30, 31, 32, 33, 34, 35, 36, 37]
01100110	[38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
00111011	[48, 49]
00011101	[50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60]
00111001	[61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77]
00101110	[78, 79, 80, 81]
00101011	[82, 83, 84, 85, 86, 87]
00100010	[88, 89, 90, 91, 92, 93, 94, 95, 96, 97]
00101000	[98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111]
00100000	[112, 113, 114, 115, 116, 117, 118]
10000100	[119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129]
10000101	[130, 131, 132]
10001001	[133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152]
10100001	[153, 154]
10101001	[155, 156, 157, 158, 159]
11100101	[160, 161, 162, 163, 164, 165, 166, 167, 168, 169]
11100001	[170, 171, 172, 173, 174]
11100000	[175, 176, 177, 178, 179]
11000000	[180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195]
11110100	[196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212]
10110100	[213, 214, 215, 216, 217, 218, 219]
11010010	[220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230]
11110110	[231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245]

11111010	2	True
11111001	5	False
10111011	23	True
11111011	19	False
10011011	39	False
01100110	244	False
00111011	50	False
00011101	64	False
00111001	75	True
00101110	79	True
00101011	84	True
00100010	94	True
00101000	97	False
00100000	113	True
10000100	129	True
10000101	131	True
10001001	144	True
10100001	162	False
10101001	140	False
11100101	165	True
11100001	174	True
11100000	192	False
11000000	180	True
11110100	202	True
10110100	214	True
11010010	225	True
11110110	233	True

Number of codes used=27

Iteration=     60000 training nets give:
alice_loss.item()=0.016717737540602684	bob_loss.item()=0.058451246470212936

11111100	[0, 1, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11111010	[2, 3, 4, 5, 6, 7, 8]
11101101	[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]
11111011	[23]
11011111	[24, 25, 26, 27, 28, 29, 30, 31, 32]
01011011	[33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43]
01011101	[44, 45, 46, 47, 48]
00111011	[49, 50, 51, 52, 53, 54, 55, 56, 57, 58]
00011101	[59, 60, 61, 62, 63]
00111010	[64, 65, 66, 67, 68, 69, 70, 71]
00011001	[72, 73, 74, 75, 76, 77, 78]
00101110	[79, 80]
01101000	[81, 82, 83, 84, 85, 86, 87, 88]
00101101	[89, 90, 91, 92, 93, 94, 95, 96, 97, 98]
00101000	[99, 100, 101, 102]
00001001	[103, 104, 105, 106, 107, 108, 109]
00000000	[110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120]
10001000	[121, 122, 123, 124, 125, 126, 127, 128, 129, 130]
10000111	[131, 132, 133]
10100101	[134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146]
11100011	[147]
10100001	[148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166]
11100101	[167, 168]
11100001	[169, 170, 171, 172, 173]
11000000	[174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192]
10110000	[193, 194, 195]
11110100	[196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]
11010100	[210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220]
11010010	[221, 222, 223, 224, 225, 226]

11111100	236	True
11111010	1	False
11101101	10	True
11111011	22	False
11011111	29	True
01011011	40	True
01011101	43	False
00111011	50	True
00011101	54	False
00111010	65	True
00011001	68	False
00101110	76	False
01101000	78	False
00101101	85	False
00101000	103	False
00001001	106	True
00000000	113	True
10001000	127	True
10000111	123	False
10100101	137	True
11100011	153	False
10100001	156	True
11100101	162	False
11100001	177	False
11000000	184	True
10110000	198	False
11110100	204	True
11010100	220	True
11010010	230	False

Number of codes used=29

Iteration=     70000 training nets give:
alice_loss.item()=0.001167195732705295	bob_loss.item()=0.0011833182070404291

10111110	[0, 254, 255]
11111010	[1, 2, 3, 4, 5, 6]
11111001	[7, 8, 9, 10]
11101101	[11]
11111101	[12, 13, 14, 15, 16]
11101001	[17, 18, 19, 20, 21, 22, 23]
01111111	[24, 25]
11001011	[26, 27, 28, 29, 30, 31]
01111011	[32, 33, 34, 35, 36]
01011100	[37, 38, 39, 40, 41, 42]
01011101	[43, 44, 45, 46, 47, 48, 49, 50]
00111011	[51, 52, 53]
00111111	[54, 55, 56, 57]
00011110	[58, 59, 60, 61]
00010101	[62]
00011101	[63]
00011011	[64]
00111010	[65, 66, 67, 68, 69]
00011001	[70, 71, 72, 73]
00101110	[74, 75, 76, 77, 78, 79, 80]
00001100	[81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
00101000	[100, 101, 102, 103]
00001001	[104, 105, 106, 107, 108, 109, 110]
00100000	[111, 112, 113, 114, 115]
01000101	[116, 117, 118, 119, 120]
10001000	[121, 122, 123, 124, 125, 126, 127, 128, 129]
10000111	[130, 131]
10000101	[132, 133, 134]
00000001	[135, 136]
10100101	[137, 138, 139, 140, 141, 142, 143]
10001001	[144, 145, 146, 147]
11001001	[148, 149, 150, 151, 152, 153, 154, 155]
10100000	[156, 157]
11000001	[158, 159, 160, 161, 162]
11100101	[163, 164, 165, 166, 167, 168, 169]
11100001	[170, 171, 172, 173, 174, 175, 176, 177]
11000000	[178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190]
10110000	[191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202]
11110100	[203, 204, 205, 206]
11010100	[207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225]
11010010	[226, 227, 228, 229]
11110010	[230, 231, 232, 233, 234, 235, 236, 237, 238]
11111000	[239]
01110110	[240, 241, 242, 243, 244, 245, 246]
11111110	[247, 248, 249, 250, 251]
11010111	[252, 253]

10111110	0	True
11111010	0	False
11111001	16	False
11101101	12	False
11111101	13	True
11101001	20	True
01111111	23	False
11001011	26	True
01111011	35	True
01011100	37	True
01011101	48	True
00111011	55	False
00111111	55	True
00011110	58	True
00010101	67	False
00011101	64	False
00011011	67	False
00111010	67	True
00011001	70	True
00101110	80	True
00001100	91	True
00101000	103	True
00001001	108	True
00100000	116	False
01000101	116	True
10001000	129	True
10000111	126	False
10000101	136	False
00000001	130	False
10100101	140	True
10001001	144	True
11001001	149	True
10100000	155	False
11000001	152	False
11100101	163	True
11100001	168	False
11000000	185	True
10110000	196	True
11110100	204	True
11010100	218	True
11010010	228	True
11110010	237	True
11111000	242	False
01110110	245	True
11111110	251	True
11010111	255	False

Number of codes used=46


End of hp run 2.  Result of run:
[(-0.9945807747703163, 70000), ('21-05-11_19:39:38BST_NLearn_model_2_Alice_iter70000', '21-05-11_19:39:38BST_NLearn_model_2_Bob_iter70000')]
(-0.9945807747703163, 70000)



Time taken over all 2 given sets of hyperparameters=1:06:43, averaging 0:33:21 per run


 ---- Table of results ----

 code  hp_run  result
   00       1  (-0.992, 70000)
   01       2  (-0.995, 70000)
 --------------------------

++++ Best result was (-0.995, 70000) on hp_run=2 with
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (714844, 936892, 888616, 165835),
	'ALICE_NET': 'MaxNet("In", 3, 50, bias_included=False)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 16,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
	'n_rng': Generator(PCG64)
	'ne_rng': Generator(PCG64)
	't_rng': <torch._C.Generator object at 0x7ff909616fd0>
	'te_rng': <torch._C.Generator object at 0x7ff909616f70>
}


End closed log for run 21-05-11_19:39:38BST
