The closed log for run 21-05-12_21:01:48BST

SMOOTHING_LENGTH = 10000
SAVE_PERIOD = 100000
CODE_BOOK_PERIOD = 10000
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # Used: "cuda"
MODEL_FOLDER = 'models'
CONFIGS_FOLDER = 'configs'
LOGS_FOLDER = 'logs'

hyperparameters = {
	'N_ITERATIONS': 200000,
	'RANDOM_SEEDS': [(714844, 936892, 888616, 165835)],
	'ALICE_NET': ['MaxNet("In", 3, 50)', 'MaxNet("In", 3, 50, bias_included=0.5)'],
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}



>>>> hp_run=1 of 2
hyperparameters = {
	'N_ITERATIONS': 200000,
	'RANDOM_SEEDS': (714844, 936892, 888616, 165835),
	'ALICE_NET': 'MaxNet("In", 3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=     20000 training nets give:
alice_loss.item()=0.4718320965766907	bob_loss.item()=0.5107722282409668

01001001	[0, 1, 2, 3, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10010001	[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]
00010101	[51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92]
10001101	[93, 94, 95, 96]
11011000	[97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196]
00111100	[197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243]

01001001	135	False
10010001	80	False
00010101	1	False
10001101	63	False
11011000	112	True
00111100	18	False

Number of codes used=6

Iteration=     30000 training nets give:
alice_loss.item()=0.12701448798179626	bob_loss.item()=0.20184016227722168

01110010	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00010101	[24, 25]
10001101	[26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88]
11011000	[89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133]
10011110	[134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194]
00110110	[195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207]

01110010	238	True
00010101	64	False
10001101	57	True
11011000	138	False
10011110	161	True
00110110	239	False

Number of codes used=6

Iteration=     40000 training nets give:
alice_loss.item()=0.09797856211662292	bob_loss.item()=0.10364385694265366

01110010	[0, 1, 2, 3, 4, 5, 6, 7, 8, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10010101	[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
10011101	[30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
10000101	[48]
10001101	[49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67]
10001111	[68, 69, 70, 71]
11001101	[72, 73, 74]
11010000	[75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109]
10011000	[110, 111, 112, 113, 114, 115, 116, 128]
11011000	[117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
11011100	[129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149]
10011110	[150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169]
10001110	[170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182]
11110110	[183, 184, 185, 186, 187, 188]
01111010	[189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206]
01010010	[207, 208, 209, 210, 211, 212, 213, 214, 215, 216]
01100010	[217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245]

01110010	229	False
10010101	42	False
10011101	52	False
10000101	57	False
10001101	52	True
10001111	56	False
11001101	61	False
11010000	95	True
10011000	114	True
11011000	126	True
11011100	132	True
10011110	171	False
10001110	181	True
11110110	197	False
01111010	210	False
01010010	211	True
01100010	234	True

Number of codes used=17

Iteration=     50000 training nets give:
alice_loss.item()=0.08772006630897522	bob_loss.item()=0.15637627243995667

00110000	[0, 1, 2, 3, 4, 5, 6, 7, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00100011	[8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]
01100001	[19, 20, 21, 22]
00011101	[23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]
10010101	[40, 41, 42, 43, 44, 45]
00111101	[46]
10011111	[47, 48, 49, 50, 51, 52, 53]
10000111	[54, 55, 56, 57, 58, 59, 60, 61, 62]
11001101	[63, 64, 65, 66, 67]
11011001	[68, 69, 70]
10010000	[71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87]
11010000	[88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105]
10011000	[106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]
11011000	[120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132]
11011100	[133, 134, 135, 136, 137, 138]
11111000	[139, 140, 141, 142]
11011010	[143, 144, 145, 146, 147, 148]
11011110	[149, 150, 151, 152, 153]
10011010	[154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164]
11101010	[165, 166, 167, 168]
11101110	[169]
11010110	[170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184]
01100111	[185, 186, 187, 188, 189, 190]
01111110	[191, 192, 193]
11110110	[194, 195, 196]
10100010	[197, 198, 199, 200, 201, 202, 203, 204, 205, 206]
11110010	[207, 208, 209, 210]
01100011	[211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224]
01100110	[225, 226, 227, 228]
01100010	[229, 230]

00110000	242	True
00100011	237	False
01100001	1	False
00011101	42	False
10010101	35	False
00111101	47	False
10011111	54	False
10000111	60	True
11001101	59	False
11011001	73	False
10010000	76	True
11010000	93	True
10011000	115	True
11011000	120	True
11011100	131	False
11111000	128	False
11011010	139	False
11011110	145	False
10011010	157	True
11101010	170	False
11101110	175	False
11010110	182	True
01100111	221	False
01111110	181	False
11110110	183	False
10100010	222	False
11110010	210	True
01100011	224	True
01100110	226	True
01100010	232	False

Number of codes used=30

Iteration=     60000 training nets give:
alice_loss.item()=0.0302828811109066	bob_loss.item()=0.31812936067581177

10110011	[0, 1, 2, 3, 4, 5, 253, 254, 255]
01110001	[6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
00110101	[20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
10111101	[31, 32, 33]
00010101	[34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]
00011001	[45, 46, 47, 48, 49]
10001001	[50, 51, 52]
11010011	[53, 54, 55, 56]
00011011	[57, 58, 59, 60, 61, 62, 63, 64, 65, 66]
10000000	[67, 68, 69, 70, 71, 72, 73, 74]
00010000	[75, 76]
10010000	[77, 78, 79, 80, 81, 82]
11110000	[83, 84, 85, 86, 87, 88, 89, 90]
11010000	[91, 92, 93, 94, 95, 96]
11001100	[97, 98, 99, 100, 101, 102, 103, 104]
00011000	[105, 106, 107, 108]
11001000	[109, 110, 111, 112, 113, 114, 115, 116, 117, 118]
11011000	[119, 120, 121, 122, 123, 124]
11111000	[125, 126, 127, 128, 129, 130, 131, 132, 133]
11011010	[134, 135, 136, 137, 138, 139, 140, 141, 142]
11111010	[143, 144, 145, 146, 147, 148, 149, 150]
10011010	[151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161]
01011110	[162]
00011010	[163, 164, 165, 166]
11001110	[167, 168, 169, 170]
11010110	[171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188]
11100111	[189, 190, 191, 192, 193, 194]
01010111	[195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205]
01000011	[206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230]
00110010	[231]
01110011	[232, 233, 234, 235, 236]
00110011	[237, 238, 239, 240, 241, 242, 243, 244, 245]
00110000	[246, 247, 248, 249, 250, 251, 252]

10110011	0	True
01110001	13	True
00110101	25	True
10111101	27	False
00010101	34	True
00011001	52	False
10001001	57	False
11010011	68	False
00011011	64	True
10000000	67	True
00010000	71	False
10010000	76	False
11110000	88	True
11010000	92	True
11001100	101	True
00011000	106	True
11001000	115	True
11011000	119	True
11111000	125	True
11011010	140	True
11111010	148	True
10011010	154	True
01011110	163	False
00011010	165	True
11001110	174	False
11010110	180	True
11100111	173	False
01010111	212	False
01000011	212	True
00110010	235	False
01110011	234	True
00110011	243	True
00110000	241	False

Number of codes used=33

Iteration=     70000 training nets give:
alice_loss.item()=0.013829459436237812	bob_loss.item()=0.3024791479110718

01100001	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01110001	[12, 13, 14, 15, 16]
00110101	[17, 18, 19, 20, 21, 22, 23, 24, 25, 26]
10110101	[27, 28, 29, 30, 31]
00010101	[32, 33, 34, 35, 36, 37, 38]
01010101	[39, 40, 41, 42]
00010001	[43, 44, 45, 46, 47]
01000101	[48]
11000001	[49, 50, 51, 52, 53, 54, 55]
11101101	[56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67]
11100000	[68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86]
11011001	[87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]
01010000	[101, 102]
00011000	[103, 104, 105, 106]
11101100	[107, 108, 109, 110, 111, 112, 113, 114, 115, 116]
11001000	[117]
11011000	[118, 119, 120, 121, 122, 123]
11111000	[124, 125, 126, 127]
11011100	[128, 129, 130, 131, 132, 133, 134, 135, 136]
11011010	[137, 138, 139, 140, 141]
10011100	[142, 143]
10111100	[144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]
00001010	[160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188]
10100110	[189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]
00101010	[200, 201, 202, 203, 204, 205, 206]
00000010	[207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218]
00100000	[219, 220, 221, 222, 223, 224]
01100011	[225, 226, 227]
01100010	[228, 229]
01100110	[230, 231, 232]

01100001	248	True
01110001	12	True
00110101	25	True
10110101	29	True
00010101	31	False
01010101	40	True
00010001	48	False
01000101	41	False
11000001	57	False
11101101	61	True
11100000	74	True
11011001	118	False
01010000	99	False
00011000	107	False
11101100	109	True
11001000	115	False
11011000	120	True
11111000	124	True
11011100	127	False
11011010	144	False
10011100	140	False
10111100	148	True
00001010	177	True
10100110	187	False
00101010	201	True
00000010	212	True
00100000	222	True
01100011	227	True
01100010	226	False
01100110	231	True

Number of codes used=30

Iteration=     80000 training nets give:
alice_loss.item()=0.010046714916825294	bob_loss.item()=0.2521797716617584

00110001	[0, 1, 2, 3, 4, 5, 6, 7, 8]
01110001	[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
01111101	[20, 21, 22, 23, 24, 25, 26, 27, 28]
10110101	[29, 30, 31, 32]
00010101	[33]
00111001	[34, 35, 36, 37]
10110001	[38, 39, 40, 41]
01000101	[42, 43, 44, 45, 46]
00000101	[47, 48]
00011001	[49, 50, 51, 52, 53, 54, 55, 56]
10001001	[57, 58, 59, 60]
10001111	[61, 62]
00001100	[63, 64, 65]
11000100	[66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77]
10010000	[78, 79, 80]
10110000	[81, 82, 83, 84]
01000000	[85, 86, 87, 88, 89, 90, 91]
10001100	[92, 93, 94, 95]
11101000	[96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108]
11101100	[109, 110, 111, 112]
10101000	[113, 114]
11001000	[115, 116, 117]
10101100	[118, 119, 120, 121]
11011001	[122, 123]
11011000	[124]
11111000	[125]
11011100	[126, 127, 128, 129, 130, 131, 132]
11111100	[133, 134, 135, 136]
10011100	[137, 138, 139, 140]
11011010	[141]
11011011	[142, 143, 144, 145, 146]
11111010	[147, 148, 149, 150]
10011010	[151, 152, 153, 154]
11001010	[155, 156, 157, 158, 159, 160, 161]
01101100	[162, 163, 164, 165]
10011110	[166, 167, 168]
00011110	[169, 170, 171, 172, 173, 174]
11100111	[175, 176, 177, 178, 179, 180, 181, 182]
10100110	[183, 184, 185, 186, 187, 188, 189, 190, 191, 192]
01000110	[193, 194]
10000110	[195, 196, 197]
11100110	[198, 199, 200, 201, 202, 203, 204]
01000011	[205]
01010010	[206]
11110011	[207, 208, 209, 210, 211, 212, 213, 214]
00101011	[215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239]
01100101	[240, 241, 242, 243, 244, 245, 246]
10110011	[247, 248, 249, 250, 251, 252, 253, 254, 255]

00110001	8	True
01110001	15	True
01111101	26	True
10110101	30	True
00010101	33	True
00111001	26	False
10110001	37	False
01000101	46	True
00000101	46	False
00011001	44	False
10001001	57	True
10001111	58	False
00001100	65	True
11000100	72	True
10010000	77	False
10110000	82	True
01000000	89	True
10001100	85	False
11101000	114	False
11101100	107	False
10101000	111	False
11001000	110	False
10101100	115	False
11011001	117	False
11011000	120	False
11111000	125	True
11011100	130	True
11111100	131	False
10011100	131	False
11011010	139	False
11011011	142	True
11111010	147	True
10011010	159	False
11001010	155	True
01101100	160	False
10011110	167	True
00011110	174	True
11100111	188	False
10100110	186	True
01000110	191	False
10000110	186	False
11100110	202	True
01000011	208	False
01010010	209	False
11110011	211	True
00101011	226	True
01100101	246	True
10110011	252	True

Number of codes used=48

Iteration=     90000 training nets give:
alice_loss.item()=0.01374269649386406	bob_loss.item()=0.2494749128818512

01110101	[0, 1, 2, 254, 255]
00100001	[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]
01111101	[18, 19, 20, 21, 22, 23, 24, 25, 26, 27]
00010101	[28, 29, 30, 31, 32, 33, 34]
11100101	[35, 36, 37, 38, 39, 40, 41, 42, 43, 44]
00001001	[45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62]
00000100	[63, 64, 65, 66]
11101001	[67, 68, 69, 70, 71]
11000100	[72]
11000000	[73, 74, 75, 76, 77]
10010000	[78, 79, 80, 81]
01001100	[82, 83, 84, 85]
01000000	[86, 87, 88, 89, 90, 91, 92, 93]
11110000	[94]
01010000	[95, 96, 97, 98, 99, 100, 101, 102]
00011000	[103, 104, 105]
10101000	[106, 107, 108, 109, 110, 111, 112]
11001000	[113, 114, 115]
10101100	[116]
10111000	[117, 118]
11011000	[119]
10011000	[120, 121, 122]
10011001	[123, 124, 125, 126, 127]
11011100	[128, 129, 130, 131]
11010100	[132, 133, 134, 135, 136, 137, 138]
11011011	[139, 140, 141, 142, 143]
01111000	[144, 145, 146, 147]
10111100	[148, 149, 150, 151, 152, 153]
11001010	[154, 155, 156]
01101100	[157, 158, 159, 160, 161, 162, 163, 164]
01011010	[165, 166, 167, 168]
10010010	[169, 170, 171, 172, 173]
00111100	[174, 175, 176, 177, 178]
11000010	[179, 180, 181]
00011010	[182]
01111010	[183, 184, 185]
10100111	[186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206]
00101111	[207]
00000010	[208, 209, 210]
00101011	[211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238]
01110010	[239, 240, 241]
00110011	[242, 243, 244, 245]
10110011	[246, 247, 248, 249, 250, 251, 252, 253]

01110101	1	True
00100001	5	True
01111101	20	True
00010101	30	True
11100101	37	True
00001001	50	True
00000100	67	False
11101001	69	True
11000100	74	False
11000000	75	True
10010000	82	False
01001100	82	True
01000000	89	True
11110000	89	False
01010000	103	False
00011000	103	True
10101000	106	True
11001000	112	False
10101100	109	False
10111000	123	False
11011000	127	False
10011000	127	False
10011001	123	True
11011100	131	True
11010100	132	True
11011011	150	False
01111000	144	True
10111100	146	False
11001010	158	False
01101100	161	True
01011010	162	False
10010010	179	False
00111100	173	False
11000010	176	False
00011010	177	False
01111010	184	True
10100111	201	True
00101111	205	False
00000010	207	False
00101011	224	True
01110010	235	False
00110011	245	True
10110011	244	False

Number of codes used=43

Iteration=    100000 training nets give:
alice_loss.item()=0.0035054844338446856	bob_loss.item()=0.14976447820663452

00100101	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00110101	[16, 17]
11110101	[18, 19, 20, 21, 22, 23, 24]
00111101	[25, 26, 27]
01000001	[28, 29]
10111101	[30, 31, 32, 33, 34]
10010001	[35, 36, 37, 38, 39, 40, 41, 42, 43, 44]
00011111	[45, 46, 47]
10100100	[48, 49, 50, 51, 52, 53, 54, 55, 56]
10010100	[57, 58, 59, 60, 61, 62]
00000100	[63, 64]
11100000	[65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87]
11110000	[88, 89, 90, 91, 92, 93, 94]
00011000	[95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110]
11101100	[111, 112, 113, 114, 115]
10111000	[116, 117, 118, 119, 120, 121]
10111001	[122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133]
11010100	[134]
00011100	[135, 136]
01010100	[137, 138]
00111000	[139, 140, 141, 142, 143, 144, 145, 146]
11011010	[147, 148, 149]
11011110	[150, 151, 152, 153, 154]
11001010	[155, 156]
01111100	[157, 158, 159, 160, 161, 162, 163]
10001010	[164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175]
11000110	[176, 177, 178]
01001010	[179, 180, 181]
11101111	[182, 183, 184, 185, 186, 187, 188, 189, 190]
01101111	[191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214]
01010111	[215]
00111010	[216, 217, 218, 219, 220, 221, 222, 223, 224]
10110111	[225]
01110010	[226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241]
00110011	[242, 243, 244, 245]
01100101	[246]

00100101	3	True
00110101	7	False
11110101	16	False
00111101	22	False
01000001	30	False
10111101	27	False
10010001	37	True
00011111	39	False
10100100	48	True
10010100	59	True
00000100	67	False
11100000	79	True
11110000	92	True
00011000	102	True
11101100	109	False
10111000	132	False
10111001	124	True
11010100	136	False
00011100	136	True
01010100	136	False
00111000	141	True
11011010	145	False
11011110	152	True
11001010	155	True
01111100	156	False
10001010	160	False
11000110	175	False
01001010	185	False
11101111	189	True
01101111	197	True
01010111	213	False
00111010	221	True
10110111	229	False
01110010	237	True
00110011	240	False
01100101	249	False

Number of codes used=36

Iteration=    110000 training nets give:
alice_loss.item()=0.011804726906120777	bob_loss.item()=0.20435018837451935

01100001	[0, 1, 2, 3, 4, 5, 253, 254, 255]
00100001	[6]
11110101	[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
00111001	[20, 21, 22, 23, 24]
00010101	[25, 26, 27, 28]
10100101	[29, 30, 31, 32]
10110001	[33, 34, 35]
11100001	[36, 37, 38, 39, 40, 41, 42, 43, 44, 45]
11000101	[46, 47, 48, 49, 50]
00001101	[51]
00001001	[52, 53, 54, 55, 56, 57]
01000100	[58, 59, 60, 61, 62, 63]
00000100	[64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]
11000000	[75, 76, 77, 78]
10010000	[79, 80, 81, 82, 83]
10110000	[84, 85]
10001100	[86, 87]
00010000	[88]
11110000	[89, 90, 91, 92]
00000000	[93, 94, 95, 96, 97, 98, 99, 100, 101]
10001000	[102, 103]
01010000	[104, 105, 106, 107, 108, 109, 110, 111]
11101100	[112, 113, 114, 115, 116, 117, 118]
11111001	[119, 120, 121, 122, 123, 124]
10011000	[125, 126, 127]
10111000	[128, 129, 130]
00011100	[131, 132]
01011100	[133, 134, 135, 136]
01011001	[137]
00111000	[138, 139, 140, 141]
01111000	[142, 143]
11011010	[144, 145, 146]
01011010	[147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167]
10010010	[168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185]
11100110	[186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]
10110110	[210]
01010011	[211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222]
10010011	[223, 224, 225, 226]
01100110	[227, 228, 229]
00100010	[230, 231, 232, 233]
00011011	[234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246]
01100101	[247, 248, 249, 250, 251]
00110000	[252]

01100001	3	True
00100001	254	False
11110101	12	True
00111001	24	True
00010101	27	True
10100101	29	True
10110001	33	True
11100001	45	True
11000101	26	False
00001101	52	False
00001001	55	True
01000100	61	True
00000100	84	False
11000000	80	False
10010000	80	True
10110000	85	True
10001100	89	False
00010000	97	False
11110000	99	False
00000000	101	True
10001000	99	False
01010000	118	False
11101100	115	True
11111001	121	True
10011000	126	True
10111000	128	True
00011100	129	False
01011100	130	False
01011001	141	False
00111000	137	False
01111000	135	False
11011010	147	False
01011010	158	True
10010010	173	True
11100110	195	True
10110110	212	False
01010011	215	True
10010011	220	False
01100110	223	False
00100010	228	False
00011011	224	False
01100101	252	False
00110000	253	False

Number of codes used=43

Iteration=    120000 training nets give:
alice_loss.item()=0.00515917781740427	bob_loss.item()=0.2990356385707855

01100001	[0, 1, 2]
01110101	[3]
01110100	[4, 5, 6]
11110101	[7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
01111101	[17, 18, 19]
00111001	[20, 21, 22, 23, 24, 25]
00010101	[26, 27, 28, 29, 30]
10100101	[31, 32]
10010001	[33]
00011111	[34, 35, 36, 37, 38, 39, 40, 41, 42]
01011101	[43, 44, 45, 46]
11101101	[47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
01001100	[72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]
10001100	[90]
01000000	[91, 92, 93, 94]
00001000	[95, 96, 97, 98, 99, 100, 101, 102]
10101100	[103, 104, 105, 106, 107, 108, 109]
10101000	[110]
11101100	[111, 112, 113, 114, 115, 116, 117, 118, 119, 120]
10111001	[121]
11111000	[122, 123, 124]
10011000	[125, 126, 127, 128]
10111100	[129, 130, 131, 132]
11011100	[133, 134, 135, 136, 137, 138]
00111000	[139, 140, 141, 142, 143, 144]
11011010	[145, 146, 147, 148]
01011010	[149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164]
11011111	[165, 166, 167, 168, 169, 170]
01111010	[171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185]
11110110	[186, 187, 188]
11000011	[189]
00101010	[190, 191, 192, 193]
00000111	[194, 195, 196, 197]
10100111	[198, 199, 200, 201, 202]
10101111	[203]
01000111	[204, 205, 206, 207]
11100010	[208, 209, 210, 211, 212, 213, 214, 215]
00101011	[216, 217, 218]
10100011	[219, 220, 221, 222]
10010011	[223, 224, 225, 226]
00100010	[227]
10110111	[228, 229, 230]
01110011	[231, 232, 233, 234, 235, 236, 237]
00110010	[238, 239, 240]
10111111	[241, 242, 243, 244, 245, 246, 247]
01100101	[248, 249, 250, 251, 252, 253, 254, 255]

01100001	8	False
01110101	11	False
01110100	6	True
11110101	12	True
01111101	19	True
00111001	24	True
00010101	29	True
10100101	30	False
10010001	29	False
00011111	37	True
01011101	44	True
11101101	59	True
01001100	74	True
10001100	86	False
01000000	89	False
00001000	102	True
10101100	108	True
10101000	107	False
11101100	116	True
10111001	120	False
11111000	123	True
10011000	126	True
10111100	132	True
11011100	136	True
00111000	140	True
11011010	151	False
01011010	161	True
11011111	174	False
01111010	175	True
11110110	182	False
11000011	192	False
00101010	194	False
00000111	202	False
10100111	200	True
10101111	203	True
01000111	207	True
11100010	201	False
00101011	218	True
10100011	220	True
10010011	218	False
00100010	228	False
10110111	232	False
01110011	233	True
00110010	239	True
10111111	244	True
01100101	0	False

Number of codes used=46

Iteration=    130000 training nets give:
alice_loss.item()=0.014015723019838333	bob_loss.item()=0.26860684156417847

01100101	[0, 1, 250, 251, 252, 253, 254, 255]
00100001	[2, 3]
01100001	[4]
01110100	[5, 6]
11110101	[7, 8, 9, 10, 11, 12, 13, 14, 15]
01000101	[16, 17]
01111101	[18]
00111101	[19, 20, 21, 22, 23, 24, 25]
10010001	[26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38]
00011111	[39, 40, 41]
10100100	[42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52]
00101001	[53, 54]
11101101	[55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65]
11100000	[66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87]
01000000	[88, 89, 90, 91, 92, 93]
10001000	[94]
00011000	[95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112]
11011000	[113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129]
00101100	[130, 131, 132, 133, 134, 135, 136, 137]
01010100	[138, 139, 140, 141, 142]
11110100	[143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154]
01111100	[155, 156, 157, 158, 159, 160, 161, 162]
11001110	[163, 164]
10011111	[165, 166, 167, 168, 169, 170, 171, 172, 173]
10010010	[174]
11000110	[175, 176, 177, 178, 179, 180]
00001110	[181, 182, 183, 184, 185, 186]
11101011	[187, 188, 189, 190]
10000111	[191, 192, 193, 194, 195, 196, 197, 198, 199, 200]
10000011	[201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222]
01100110	[223, 224]
00100011	[225, 226, 227, 228]
10110111	[229, 230, 231, 232, 233, 234]
00110111	[235, 236, 237]
00110011	[238, 239, 240, 241]
10111111	[242, 243, 244, 245, 246, 247]
01100100	[248, 249]

01100101	254	True
00100001	7	False
01100001	5	False
01110100	6	True
11110101	15	True
01000101	13	False
01111101	21	False
00111101	27	False
10010001	30	True
00011111	49	False
10100100	48	True
00101001	53	True
11101101	59	True
11100000	81	True
01000000	88	True
10001000	95	False
00011000	97	True
11011000	122	True
00101100	138	False
01010100	142	True
11110100	151	True
01111100	156	True
11001110	166	False
10011111	167	True
10010010	173	False
11000110	176	True
00001110	177	False
11101011	187	True
10000111	192	True
10000011	206	True
01100110	223	True
00100011	228	True
10110111	239	False
00110111	235	True
00110011	240	True
10111111	242	True
01100100	251	False

Number of codes used=37

Iteration=    140000 training nets give:
alice_loss.item()=0.01978469267487526	bob_loss.item()=0.3104325532913208

00110000	[0, 1, 2, 3]
00100001	[4, 5, 6, 7]
01110101	[8, 9, 10, 11]
01110001	[12, 13]
01110000	[14, 15, 16, 17, 18, 19, 20]
10010101	[21, 22]
10110001	[23, 24, 25, 26, 27, 28, 29, 30, 31]
10110100	[32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]
10010100	[49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60]
00001001	[61, 62, 63, 64]
00001100	[65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84]
10010000	[85]
01000000	[86, 87, 88, 89, 90, 91, 92]
01001000	[93, 94]
10001000	[95, 96, 97]
11010000	[98, 99, 100]
10101000	[101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111]
11001000	[112, 113, 114]
01011000	[115, 116, 117, 118, 119, 120, 121]
11111001	[122, 123, 124, 125, 126, 127, 128, 129]
10011001	[130, 131]
00111000	[132, 133, 134, 135]
00101100	[136, 137, 138, 139]
01010100	[140, 141, 142]
00111100	[143, 144, 145, 146, 147, 148, 149, 150]
11011010	[151]
01011010	[152, 153, 154, 155, 156, 157, 158, 159, 160, 161]
10111010	[162]
11011111	[163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182]
01001010	[183, 184]
00101010	[185, 186, 187, 188, 189, 190]
11100110	[191, 192]
11101111	[193, 194, 195, 196, 197]
00000111	[198, 199]
10101111	[200, 201]
01000011	[202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227]
01100111	[228, 229]
00100000	[230, 231, 232, 233, 234]
01110111	[235]
00110010	[236, 237, 238]
00100100	[239, 240, 241]
10111111	[242, 243, 244]
00110100	[245, 246, 247, 248, 249]
01100100	[250, 251, 252, 253]
01100101	[254, 255]

00110000	6	False
00100001	5	True
01110101	9	True
01110001	14	False
01110000	19	True
10010101	21	True
10110001	32	False
10110100	41	True
10010100	51	True
00001001	61	True
00001100	69	True
10010000	83	False
01000000	89	True
01001000	95	False
10001000	96	True
11010000	99	True
10101000	103	True
11001000	109	False
01011000	120	True
11111001	127	True
10011001	134	False
00111000	133	True
00101100	141	False
01010100	143	False
00111100	145	True
11011010	149	False
01011010	159	True
10111010	163	False
11011111	174	True
01001010	185	False
00101010	177	False
11100110	192	True
11101111	194	True
00000111	199	True
10101111	200	True
01000011	211	True
01100111	226	False
00100000	232	True
01110111	238	False
00110010	239	False
00100100	241	True
10111111	241	False
00110100	250	False
01100100	251	True
01100101	252	False

Number of codes used=45

Iteration=    150000 training nets give:
alice_loss.item()=0.019201047718524933	bob_loss.item()=0.16000041365623474

01100101	[0, 252, 253, 254, 255]
00110000	[1]
00100001	[2, 3, 4, 5, 6, 7, 8, 9]
01010001	[10, 11, 12, 13, 14]
00000101	[15]
00111101	[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]
10110100	[34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]
00010100	[54, 55, 56]
11100100	[57]
10000100	[58, 59]
00001001	[60, 61]
10001101	[62, 63, 64, 65, 66]
11000100	[67, 68, 69, 70, 71, 72]
11100000	[73, 74, 75, 76]
11000000	[77, 78, 79, 80]
10010000	[81, 82, 83, 84, 85, 86]
10100000	[87]
01000000	[88]
00000000	[89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102]
00011100	[103, 104, 105, 106]
11101000	[107, 108, 109]
11001000	[110, 111, 112, 113]
11001100	[114, 115, 116, 117]
10111001	[118, 119, 120, 121]
11111000	[122]
10011100	[123, 124]
11111001	[125, 126, 127, 128]
10011001	[129, 130, 131, 132, 133]
11010100	[134, 135, 136, 137]
00101100	[138, 139, 140, 141]
00111100	[142, 143, 144, 145, 146]
01101100	[147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167]
10010110	[168, 169, 170, 171, 172, 173]
11101110	[174, 175, 176, 177, 178, 179]
11000110	[180]
11110110	[181, 182]
01001010	[183]
11101011	[184, 185, 186]
11010011	[187, 188, 189, 190, 191]
11100110	[192, 193, 194]
01001111	[195, 196, 197, 198, 199, 200, 201]
00000010	[202]
01101011	[203, 204, 205, 206]
00101011	[207, 208, 209, 210]
11110011	[211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221]
01110010	[222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249]
01100100	[250, 251]

01100101	250	False
00110000	6	False
00100001	6	True
01010001	12	True
00000101	20	False
00111101	25	True
10110100	42	True
00010100	53	False
11100100	55	False
10000100	63	False
00001001	67	False
10001101	61	False
11000100	73	False
11100000	78	False
11000000	77	True
10010000	85	True
10100000	83	False
01000000	89	False
00000000	98	True
00011100	120	False
11101000	106	False
11001000	110	True
11001100	112	False
10111001	120	True
11111000	123	False
10011100	123	True
11111001	127	True
10011001	135	False
11010100	133	False
00101100	142	False
00111100	148	False
01101100	158	True
10010110	172	True
11101110	175	True
11000110	182	False
11110110	182	True
01001010	183	True
11101011	188	False
11010011	189	True
11100110	197	False
01001111	196	True
00000010	202	True
01101011	205	True
00101011	207	True
11110011	219	True
01110010	241	True
01100100	244	False

Number of codes used=47

Iteration=    160000 training nets give:
alice_loss.item()=0.012816328555345535	bob_loss.item()=0.26162946224212646

01000001	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 253, 254, 255]
10111101	[22, 23, 24, 25]
10110001	[26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]
01001101	[40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57]
10010100	[58, 59]
00001001	[60, 61, 62, 63, 64, 65, 66, 67]
11101001	[68, 69, 70, 71, 72, 73]
00001100	[74]
11000000	[75, 76, 77, 78, 79, 80, 81]
10010000	[82, 83, 84, 85, 86, 87, 88]
10001100	[89, 90, 91, 92, 93, 94]
01001000	[95, 96]
00011000	[97, 98, 99, 100, 101, 102, 103, 104, 105]
11101000	[106, 107]
10101100	[108, 109, 110, 111, 112, 113]
11001100	[114, 115, 116]
01011000	[117]
10011100	[118, 119, 120, 121, 122, 123]
11111001	[124, 125, 126, 127]
10011001	[128, 129, 130, 131, 132, 133, 134, 135, 136, 137]
11011100	[138, 139]
01010100	[140]
00111100	[141, 142, 143, 144, 145, 146]
11011110	[147, 148]
10011010	[149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164]
11111011	[165]
10011111	[166, 167, 168, 169, 170]
10010110	[171, 172, 173, 174]
00101110	[175, 176]
10001011	[177, 178, 179, 180, 181, 182]
11001111	[183, 184, 185, 186, 187]
11010011	[188]
11101111	[189, 190, 191, 192, 193]
00000010	[194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210]
10010011	[211, 212, 213, 214, 215, 216, 217]
10010111	[218, 219]
10110110	[220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231]
00100011	[232]
10110011	[233]
00110011	[234, 235, 236, 237, 238, 239, 240, 241, 242]
00100100	[243]
01100000	[244, 245, 246, 247, 248]
00110100	[249, 250, 251, 252]

01000001	9	True
10111101	30	False
10110001	34	True
01001101	47	True
10010100	60	False
00001001	63	True
11101001	72	True
00001100	75	False
11000000	78	True
10010000	88	True
10001100	96	False
01001000	94	False
00011000	104	True
11101000	104	False
10101100	115	False
11001100	113	False
01011000	123	False
10011100	117	False
11111001	126	True
10011001	127	False
11011100	134	False
01010100	144	False
00111100	146	True
11011110	146	False
10011010	157	True
11111011	168	False
10011111	169	True
10010110	172	True
00101110	172	False
10001011	182	True
11001111	185	True
11010011	184	False
11101111	192	True
00000010	201	True
10010011	211	True
10010111	216	False
10110110	226	True
00100011	229	False
10110011	232	False
00110011	237	True
00100100	242	False
01100000	247	True
00110100	2	False

Number of codes used=43

Iteration=    170000 training nets give:
alice_loss.item()=0.0025639357045292854	bob_loss.item()=0.26795724034309387

00100101	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 249, 250, 251, 252, 253, 254, 255]
01110001	[15, 16, 17]
00010101	[18, 19, 20]
10100001	[21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]
10110100	[34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
01001001	[52, 53, 54]
00010100	[55, 56, 57]
00001001	[58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]
11101001	[73, 74]
11000000	[75, 76, 77, 78]
11001001	[79, 80, 81, 82, 83]
10010000	[84, 85, 86, 87, 88, 89, 90]
01001000	[91, 92, 93, 94, 95]
00000000	[96, 97, 98]
00011000	[99, 100, 101, 102, 103, 104]
00011100	[105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123]
01011000	[124]
10011001	[125, 126, 127, 128, 129, 130, 131]
01011100	[132, 133]
01101000	[134, 135, 136, 137, 138, 139]
00101100	[140, 141, 142, 143, 144]
11011010	[145, 146, 147, 148, 149, 150, 151, 152]
00011010	[153, 154, 155, 156]
10111110	[157, 158, 159, 160, 161, 162]
11001110	[163, 164]
11111011	[165, 166, 167]
11010010	[168]
10011111	[169]
00101010	[170, 171, 172, 173]
11010110	[174, 175, 176, 177]
11000110	[178, 179, 180, 181, 182]
11010011	[183, 184, 185, 186, 187]
11010111	[188, 189, 190]
11101111	[191, 192]
01001111	[193, 194, 195]
11100110	[196, 197, 198, 199, 200, 201, 202, 203, 204]
11100010	[205, 206]
01000011	[207, 208]
00000011	[209]
10100110	[210, 211, 212]
11110011	[213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226]
01100111	[227, 228, 229, 230]
10110011	[231, 232, 233]
00111011	[234]
00100000	[235, 236, 237]
00110010	[238]
10111111	[239, 240, 241, 242, 243, 244, 245, 246, 247, 248]

00100101	0	True
01110001	21	False
00010101	24	False
10100001	26	True
10110100	40	True
01001001	54	True
00010100	58	False
00001001	61	True
11101001	71	False
11000000	72	False
11001001	83	True
10010000	84	True
01001000	92	True
00000000	94	False
00011000	103	True
00011100	110	True
01011000	124	True
10011001	125	True
01011100	129	False
01101000	138	True
00101100	140	True
11011010	149	True
00011010	160	False
10111110	169	False
11001110	163	True
11111011	175	False
11010010	169	False
10011111	170	False
00101010	169	False
11010110	174	True
11000110	184	False
11010011	183	True
11010111	183	False
11101111	190	False
01001111	195	True
11100110	206	False
11100010	210	False
01000011	209	False
00000011	209	True
10100110	215	False
11110011	219	True
01100111	227	True
10110011	233	True
00111011	234	True
00100000	238	False
00110010	241	False
10111111	245	True

Number of codes used=47

Iteration=    180000 training nets give:
alice_loss.item()=0.0013256208039820194	bob_loss.item()=0.22330525517463684

00111111	[0, 1, 2, 254, 255]
00100001	[3, 4, 5, 6, 7, 8]
01110101	[9]
00110000	[10, 11, 12]
01110000	[13, 14]
00000101	[15, 16, 17, 18, 19, 20]
00111101	[21]
01010101	[22, 23, 24, 25, 26]
11111101	[27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]
00101001	[50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67]
10001001	[68, 69, 70]
00000100	[71, 72, 73]
10100000	[74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97]
10001100	[98, 99, 100]
10001000	[101]
00011000	[102, 103, 104, 105, 106, 107, 108, 109]
11001100	[110]
11001000	[111, 112, 113]
10101100	[114, 115, 116, 117]
10011100	[118]
10011000	[119]
11011000	[120, 121, 122]
11101100	[123, 124, 125, 126]
11010100	[127, 128, 129, 130, 131, 132]
00111000	[133]
11011100	[134, 135]
00101000	[136]
11011101	[137, 138, 139, 140, 141, 142, 143]
11011110	[144, 145, 146]
11011010	[147, 148, 149, 150, 151]
00111010	[152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163]
10011111	[164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178]
10001011	[179]
11000110	[180, 181, 182, 183, 184]
01010010	[185]
01011111	[186, 187, 188, 189]
11000111	[190, 191]
01001111	[192, 193, 194]
01000110	[195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211]
01010011	[212, 213]
01000111	[214, 215]
00010111	[216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233]
00111011	[234, 235, 236, 237, 238, 239, 240, 241]
00100100	[242, 243, 244, 245]
01100000	[246, 247, 248, 249, 250, 251]
01110100	[252, 253]

00111111	250	False
00100001	10	False
01110101	7	False
00110000	16	False
01110000	9	False
00000101	22	False
00111101	29	False
01010101	26	True
11111101	36	True
00101001	57	True
10001001	69	True
00000100	79	False
10100000	86	True
10001100	99	True
10001000	103	False
00011000	107	True
11001100	119	False
11001000	118	False
10101100	117	True
10011100	119	False
10011000	118	False
11011000	125	False
11101100	124	True
11010100	131	True
00111000	131	False
11011100	135	True
00101000	136	True
11011101	140	True
11011110	150	False
11011010	148	True
00111010	152	True
10011111	171	True
10001011	179	True
11000110	179	False
01010010	184	False
01011111	185	False
11000111	189	False
01001111	191	False
01000110	206	True
01010011	204	False
01000111	214	True
00010111	226	True
00111011	234	True
00100100	245	True
01100000	248	True
01110100	1	False

Number of codes used=46

Iteration=    190000 training nets give:
alice_loss.item()=0.0005583210731856525	bob_loss.item()=0.11610318720340729

01000001	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 250, 251, 252, 253, 254, 255]
01110001	[17, 18, 19, 20, 21, 22]
01111101	[23]
10010101	[24, 25, 26]
11010101	[27, 28, 29, 30, 31]
01101101	[32, 33, 34]
10000001	[35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54]
11010001	[55, 56, 57, 58, 59, 60, 61]
10010100	[62]
11000100	[63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75]
00001100	[76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]
00000000	[90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101]
10001000	[102, 103, 104, 105]
11101000	[106, 107, 108, 109]
00011100	[110, 111, 112, 113, 114]
10101100	[115, 116]
10011000	[117, 118]
11111001	[119, 120]
11111000	[121, 122, 123, 124, 125]
10111100	[126, 127, 128, 129, 130]
00111000	[131]
11011100	[132, 133]
00101000	[134, 135]
01101000	[136, 137, 138]
00101100	[139]
11011101	[140]
01111100	[141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169]
01101110	[170]
00001010	[171]
10101110	[172, 173, 174, 175]
01001010	[176, 177, 178, 179, 180]
11000110	[181, 182]
11001111	[183, 184, 185]
10000111	[186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205]
01000010	[206, 207, 208]
11100010	[209, 210, 211]
01000111	[212, 213, 214]
10100011	[215, 216, 217, 218]
00100110	[219, 220]
10110110	[221, 222, 223, 224, 225]
01100111	[226, 227, 228]
00100011	[229, 230]
10110111	[231, 232, 233, 234]
00110011	[235, 236, 237, 238]
01110110	[239]
01110111	[240, 241, 242, 243, 244]
01100000	[245, 246, 247, 248, 249]

01000001	7	True
01110001	21	True
01111101	27	False
10010101	36	False
11010101	35	False
01101101	39	False
10000001	47	True
11010001	64	False
10010100	63	False
11000100	71	True
00001100	85	True
00000000	93	True
10001000	106	False
11101000	108	True
00011100	117	False
10101100	120	False
10011000	120	False
11111001	119	True
11111000	123	True
10111100	129	True
00111000	127	False
11011100	137	False
00101000	132	False
01101000	132	False
00101100	134	False
11011101	144	False
01111100	149	True
01101110	167	False
00001010	167	False
10101110	171	False
01001010	176	True
11000110	180	False
11001111	185	True
10000111	193	True
01000010	206	True
11100010	208	False
01000111	216	False
10100011	216	True
00100110	223	False
10110110	226	False
01100111	227	True
00100011	225	False
10110111	234	True
00110011	242	False
01110110	244	False
01110111	241	True
01100000	252	False

Number of codes used=47

Iteration=    200000 training nets give:
alice_loss.item()=0.00042680720798671246	bob_loss.item()=0.002233646810054779

01110100	[0, 1, 254, 255]
01100001	[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
00110000	[15, 16]
00110101	[17, 18, 19, 20, 21, 22]
10100001	[23, 24, 25, 26]
01111001	[27, 28, 29]
10110101	[30, 31, 32, 33]
11111101	[34, 35, 36, 37, 38]
10110001	[39]
11000001	[40, 41, 42, 43, 44]
10101101	[45]
11101101	[46, 47, 48, 49, 50, 51, 52, 53]
11001101	[54, 55, 56, 57, 58, 59]
10001001	[60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]
00001100	[75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93]
01001000	[94]
00000000	[95, 96]
10001100	[97, 98, 99]
11010000	[100, 101, 102]
10001000	[103, 104, 105, 106]
11101000	[107, 108, 109, 110, 111, 112]
00011100	[113, 114, 115]
10011000	[116, 117, 118, 119, 120]
11111001	[121]
11101100	[122, 123, 124, 125]
10111100	[126, 127, 128, 129, 130]
00111000	[131, 132]
11011100	[133, 134, 135, 136]
11111100	[137]
11011001	[138]
01111100	[139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166]
01101110	[167, 168]
11010010	[169, 170]
01001110	[171, 172, 173, 174]
11000010	[175, 176, 177, 178]
11111111	[179, 180, 181, 182]
01010010	[183, 184, 185]
11001111	[186, 187]
10001111	[188, 189, 190, 191]
10000111	[192, 193]
11110111	[194, 195, 196, 197, 198, 199]
10101111	[200, 201]
01000110	[202, 203]
10010111	[204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231]
10110011	[232, 233]
00111011	[234, 235, 236]
01111011	[237, 238, 239, 240]
01110111	[241, 242, 243, 244]
00100100	[245, 246, 247]
01100000	[248]
01100101	[249, 250, 251, 252, 253]

01110100	3	False
01100001	13	True
00110000	12	False
00110101	19	True
10100001	24	True
01111001	27	True
10110101	31	True
11111101	39	False
10110001	34	False
11000001	39	False
10101101	43	False
11101101	47	True
11001101	56	True
10001001	72	True
00001100	84	True
01001000	88	False
00000000	92	False
10001100	102	False
11010000	104	False
10001000	105	True
11101000	103	False
00011100	114	True
10011000	121	False
11111001	118	False
11101100	122	True
10111100	129	True
00111000	130	False
11011100	137	False
11111100	132	False
11011001	132	False
01111100	151	True
01101110	163	False
11010010	171	False
01001110	174	True
11000010	177	True
11111111	180	True
01010010	181	False
11001111	185	False
10001111	189	True
10000111	193	True
11110111	197	True
10101111	196	False
01000110	207	False
10010111	214	True
10110011	235	False
00111011	239	False
01111011	239	True
01110111	243	True
00100100	248	False
01100000	250	False
01100101	252	True

Number of codes used=51


End of hp run 1.  Result of run:
[(-0.9909643473595524, 200000), ('21-05-12_21:01:48BST_NLearn_model_1_Alice_iter200000', '21-05-12_21:01:48BST_NLearn_model_1_Bob_iter200000')]
(-0.9909643473595524, 200000)


>>>> hp_run=2 of 2, time elapsed 5:56:47 of estimated 11:53:34, 
implying ending at 08:55:22BST on Thursday 13 May 2021
hyperparameters = {
	'N_ITERATIONS': 200000,
	'RANDOM_SEEDS': (714844, 936892, 888616, 165835),
	'ALICE_NET': 'MaxNet("In", 3, 50, bias_included=0.5)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=     20000 training nets give:
alice_loss.item()=0.674911379814148	bob_loss.item()=0.605137825012207

01100110	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00110100	[37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58]
10101001	[59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93]
00111110	[94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167]
00101010	[168, 169, 170, 171, 172, 173, 174, 175, 176]
01100010	[177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204]

01100110	146	False
00110100	97	False
10101001	85	True
00111110	139	True
00101010	110	False
01100010	147	False

Number of codes used=6

Iteration=     30000 training nets give:
alice_loss.item()=0.14566436409950256	bob_loss.item()=0.27127179503440857

10001100	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 253, 254, 255]
10101001	[65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111]
00111110	[112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152]
00111010	[153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188]
00001010	[189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252]

10001100	40	True
10101001	79	True
00111110	137	True
00111010	173	True
00001010	208	True

Number of codes used=5

Iteration=     40000 training nets give:
alice_loss.item()=0.07288802415132523	bob_loss.item()=0.12230190634727478

00001000	[0, 1, 2, 3, 4, 5, 6, 7, 8, 254, 255]
00001100	[9, 10, 11]
11001100	[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
10001100	[30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]
10101100	[49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
10111001	[60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78]
10101001	[79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103]
01111110	[104, 105, 106]
10111110	[107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120]
00111110	[121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149]
00011110	[150, 151, 152, 153, 154, 155, 156, 157]
00111010	[158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171]
00011010	[172, 173, 174, 175, 176, 177, 178, 179]
00000010	[180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197]
00001010	[198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214]
01001011	[215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232]
11001010	[233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253]

00001000	248	False
00001100	9	True
11001100	17	True
10001100	33	True
10101100	56	True
10111001	78	True
10101001	71	False
01111110	157	False
10111110	129	False
00111110	144	True
00011110	153	True
00111010	175	False
00011010	180	False
00000010	213	False
00001010	204	True
01001011	224	True
11001010	242	True

Number of codes used=17

Iteration=     50000 training nets give:
alice_loss.item()=0.055761195719242096	bob_loss.item()=0.08063100278377533

00001101	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 250, 251, 252, 253, 254, 255]
00001100	[10, 11, 12, 13]
11001100	[14, 15, 16, 17]
00101100	[18, 19, 20, 21, 22, 23, 24, 25, 26, 27]
11000000	[28, 29]
10001101	[30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]
00101001	[41, 42, 43, 44, 45, 46, 47]
10101100	[48, 49, 50, 51, 52, 53]
10011100	[54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68]
10101001	[69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
10101101	[84, 85]
10111101	[86, 87]
10101011	[88]
10110001	[89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106]
10110101	[107, 108, 109, 110, 111, 112]
10110011	[113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132]
00111110	[133, 134, 135, 136, 137, 138, 139, 140, 141, 142]
00111101	[143, 144, 145, 146, 147, 148, 149]
00101110	[150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161]
00010011	[162, 163, 164]
00111010	[165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175]
00011010	[176, 177, 178, 179]
01100010	[180, 181, 182, 183, 184]
01111010	[185, 186, 187, 188, 189, 190]
01011010	[191, 192, 193, 194]
00100010	[195, 196, 197]
00001010	[198, 199, 200, 201, 202]
01000111	[203, 204, 205]
00001110	[206, 207, 208, 209, 210, 211, 212, 213]
11101010	[214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233]
11001010	[234, 235, 236, 237, 238, 239, 240, 241, 242]
00001000	[243, 244, 245, 246, 247, 248, 249]

00001101	9	True
00001100	8	False
11001100	16	True
00101100	27	True
11000000	23	False
10001101	31	True
00101001	49	False
10101100	63	False
10011100	64	True
10101001	78	True
10101101	80	False
10111101	91	False
10101011	93	False
10110001	101	True
10110101	101	False
10110011	117	True
00111110	143	False
00111101	132	False
00101110	144	False
00010011	164	True
00111010	168	True
00011010	180	False
01100010	178	False
01111010	177	False
01011010	194	True
00100010	193	False
00001010	205	False
01000111	211	False
00001110	219	False
11101010	228	True
11001010	234	True
00001000	250	False

Number of codes used=32

Iteration=     60000 training nets give:
alice_loss.item()=0.0572337880730629	bob_loss.item()=0.21062418818473816

00001000	[0, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00001001	[1, 2, 3, 4, 5, 6, 7, 8, 9]
00001101	[10]
11000101	[11, 12]
11001100	[13, 14, 15, 16, 17, 18, 19]
00101100	[20, 21, 22, 23, 24, 25, 26, 27]
10001000	[28]
10001101	[29, 30]
10001110	[31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]
00101001	[42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52]
10001001	[53]
10100000	[54, 55]
10011100	[56, 57, 58, 59, 60, 61, 62, 63, 64, 65]
11101001	[66, 67, 68, 69, 70, 71]
10101101	[72, 73, 74, 75, 76, 77, 78, 79, 80, 81]
10010001	[82, 83]
11111101	[84, 85, 86]
10111101	[87, 88, 89]
10101011	[90, 91, 92]
10110001	[93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105]
10111111	[106, 107, 108, 109, 110, 111]
10010111	[112, 113, 114, 115]
10011110	[116, 117, 118, 119, 120, 121, 122, 123, 124]
10111110	[125]
00110110	[126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145]
00011110	[146, 147, 148, 149, 150, 151, 152]
00111011	[153, 154, 155, 156, 157, 158, 159, 160, 161, 162]
00111000	[163, 164, 165, 166, 167, 168, 169, 170, 171, 172]
00111010	[173, 174, 175, 176]
00011010	[177, 178, 179]
01100010	[180, 181, 182, 183]
00101010	[184, 185, 186, 187, 188, 189, 190, 191, 192]
01011010	[193, 194, 195, 196, 197, 198, 199]
01101010	[200, 201, 202, 203, 204, 205, 206, 207]
00100011	[208, 209, 210, 211, 212]
00001110	[213, 214, 215, 216, 217, 218, 219]
01101011	[220, 221, 222]
00001111	[223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238]
11001010	[239, 240, 241, 242]

00001000	2	False
00001001	10	False
00001101	13	False
11000101	20	False
11001100	14	True
00101100	27	True
10001000	30	False
10001101	32	False
10001110	33	True
00101001	50	True
10001001	58	False
10100000	65	False
10011100	63	True
11101001	65	False
10101101	81	True
10010001	92	False
11111101	86	True
10111101	90	False
10101011	90	True
10110001	102	True
10111111	112	False
10010111	125	False
10011110	122	True
10111110	127	False
00110110	137	True
00011110	148	True
00111011	158	True
00111000	159	False
00111010	171	False
00011010	175	False
01100010	177	False
00101010	198	False
01011010	191	False
01101010	197	False
00100011	210	True
00001110	214	True
01101011	224	False
00001111	241	False
11001010	230	False

Number of codes used=39

Iteration=     70000 training nets give:
alice_loss.item()=0.008581724017858505	bob_loss.item()=0.13847725093364716

11011000	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11000000	[17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]
01101001	[28, 29, 30, 31, 32, 33, 34, 35, 36, 37]
10000100	[38, 39, 40, 41, 42, 43, 44, 45, 46]
00101001	[47, 48, 49, 50, 51]
10101100	[52, 53, 54, 55, 56, 57, 58]
11101011	[59, 60, 61, 62]
10101000	[63, 64, 65]
10011100	[66]
11111001	[67, 68, 69, 70, 71, 72, 73]
10101001	[74, 75, 76, 77, 78, 79, 80]
10101101	[81, 82]
10010001	[83, 84, 85, 86]
11111101	[87, 88, 89]
10111101	[90]
10101110	[91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102]
00110100	[103, 104, 105, 106, 107, 108, 109, 110, 111]
10110000	[112, 113, 114, 115]
10110111	[116, 117, 118, 119, 120]
10011110	[121, 122, 123]
00110101	[124]
10111110	[125, 126, 127, 128, 129, 130, 131]
00111100	[132]
00111111	[133, 134]
00100110	[135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154]
01111111	[155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166]
00011111	[167]
00111010	[168, 169, 170, 171, 172]
01110000	[173]
11010111	[174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190]
01011010	[191, 192, 193]
00000010	[194, 195, 196, 197]
01000110	[198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212]
00011000	[213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224]
00100000	[225, 226, 227, 228, 229, 230, 231, 232, 233]
01011001	[234, 235, 236, 237, 238, 239]

11011000	15	True
11000000	25	True
01101001	29	True
10000100	40	True
00101001	45	False
10101100	63	False
11101011	61	True
10101000	65	True
10011100	66	True
11111001	73	True
10101001	78	True
10101101	82	True
10010001	74	False
11111101	84	False
10111101	94	False
10101110	102	True
00110100	106	True
10110000	107	False
10110111	121	False
10011110	126	False
00110101	124	True
10111110	129	True
00111100	130	False
00111111	146	False
00100110	149	True
01111111	161	True
00011111	162	False
00111010	171	True
01110000	179	False
11010111	148	False
01011010	190	False
00000010	195	True
01000110	203	True
00011000	226	False
00100000	233	True
01011001	249	False

Number of codes used=36

Iteration=     80000 training nets give:
alice_loss.item()=0.0079876147210598	bob_loss.item()=0.25218793749809265

01101100	[0, 1, 2, 3, 4, 250, 251, 252, 253, 254, 255]
00001100	[5, 6, 7]
11001100	[8, 9, 10, 11]
01100100	[12, 13, 14, 15, 16, 17]
00001101	[18, 19]
01101101	[20, 21]
11000000	[22, 23, 24]
10001111	[25, 26, 27, 28, 29, 30, 31]
11000100	[32]
10001110	[33, 34]
11011100	[35, 36, 37, 38, 39]
00100100	[40, 41]
11101100	[42, 43, 44, 45, 46]
11010100	[47, 48, 49]
00101001	[50, 51, 52]
11100001	[53, 54]
10001001	[55, 56, 57, 58]
11101011	[59, 60]
10101000	[61, 62, 63, 64, 65, 66, 67, 68, 69]
10100001	[70, 71, 72, 73]
10101001	[74, 75, 76, 77, 78, 79]
10101101	[80, 81]
01111001	[82, 83, 84, 85]
10101011	[86, 87, 88]
10010000	[89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107]
11111011	[108, 109, 110, 111]
11110110	[112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132]
11100110	[133, 134, 135, 136, 137, 138, 139, 140, 141]
00111111	[142]
01111110	[143, 144, 145, 146, 147, 148, 149]
00110010	[150, 151, 152, 153, 154, 155]
00111011	[156, 157, 158, 159, 160]
00110000	[161, 162]
00011111	[163, 164, 165, 166]
00111000	[167, 168, 169]
00111010	[170]
01111011	[171, 172, 173, 174, 175]
01110000	[176, 177, 178, 179, 180, 181, 182]
11111010	[183]
01000010	[184]
01011010	[185, 186, 187, 188, 189, 190, 191]
00000010	[192]
00010000	[193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205]
10000010	[206, 207, 208, 209]
00100011	[210, 211, 212, 213]
01001010	[214]
01011000	[215, 216, 217, 218, 219, 220, 221]
00100000	[222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242]
01000000	[243]
01011001	[244, 245, 246, 247, 248, 249]

01101100	8	False
00001100	8	False
11001100	15	False
01100100	12	True
00001101	11	False
01101101	16	False
11000000	28	False
10001111	26	True
11000100	26	False
10001110	33	True
11011100	46	False
00100100	39	False
11101100	47	False
11010100	46	False
00101001	46	False
11100001	60	False
10001001	58	True
11101011	66	False
10101000	66	True
10100001	69	False
10101001	75	True
10101101	84	False
01111001	88	False
10101011	85	False
10010000	94	True
11111011	98	False
11110110	119	True
11100110	124	False
00111111	145	False
01111110	145	True
00110010	150	True
00111011	155	False
00110000	157	False
00011111	162	False
00111000	165	False
00111010	165	False
01111011	179	False
01110000	185	False
11111010	179	False
01000010	188	False
01011010	180	False
00000010	190	False
00010000	202	True
10000010	206	True
00100011	208	False
01001010	217	False
01011000	226	False
00100000	234	True
01000000	243	True
01011001	251	False

Number of codes used=50

Iteration=     90000 training nets give:
alice_loss.item()=0.005534594878554344	bob_loss.item()=0.1386205404996872

01001100	[0, 1, 2, 3, 4, 254, 255]
00001100	[5, 6, 7, 8]
11001100	[9, 10]
01100100	[11, 12, 13, 14, 15, 16, 17, 18]
11001000	[19, 20, 21, 22, 23, 24]
11000000	[25, 26, 27, 28, 29, 30]
00100101	[31, 32, 33, 34]
11000001	[35, 36, 37, 38, 39]
10000100	[40, 41]
11011100	[42, 43, 44]
11010100	[45, 46, 47, 48]
11100100	[49, 50]
10000110	[51, 52, 53, 54, 55, 56, 57]
10001001	[58]
10100100	[59, 60, 61]
11101101	[62, 63, 64, 65, 66]
10111001	[67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91]
10010000	[92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104]
10110000	[105, 106, 107, 108, 109]
00110100	[110, 111, 112]
11110011	[113, 114, 115]
11111111	[116, 117, 118]
00111101	[119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132]
00110111	[133, 134]
01110101	[135, 136]
00010110	[137, 138]
00111110	[139, 140]
01111110	[141, 142]
00010111	[143, 144, 145]
00110010	[146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]
00110011	[160, 161]
00011111	[162, 163, 164, 165, 166]
00111010	[167]
01111000	[168, 169, 170, 171, 172, 173, 174, 175]
01110000	[176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194]
00000010	[195]
11100010	[196, 197]
01010011	[198, 199]
01000110	[200]
00000011	[201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226]
01010001	[227, 228, 229, 230, 231]
01001011	[232, 233, 234]
11001010	[235, 236]
01011101	[237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247]
01000001	[248, 249, 250, 251, 252]
00011001	[253]

01001100	3	True
00001100	3	False
11001100	11	False
01100100	14	True
11001000	22	True
11000000	30	True
00100101	33	True
11000001	35	True
10000100	40	True
11011100	50	False
11010100	46	True
11100100	64	False
10000110	54	True
10001001	56	False
10100100	66	False
11101101	69	False
10111001	75	True
10010000	92	True
10110000	105	True
00110100	112	True
11110011	117	False
11111111	116	True
00111101	121	True
00110111	132	False
01110101	134	False
00010110	132	False
00111110	139	True
01111110	133	False
00010111	149	False
00110010	149	True
00110011	157	False
00011111	165	True
00111010	159	False
01111000	167	False
01110000	180	True
00000010	188	False
11100010	195	False
01010011	197	False
01000110	202	False
00000011	212	True
01010001	225	False
01001011	231	False
11001010	240	False
01011101	244	True
01000001	245	False
00011001	250	False

Number of codes used=46

Iteration=    100000 training nets give:
alice_loss.item()=0.016556117683649063	bob_loss.item()=0.3057529926300049

00000001	[0, 1, 2, 3, 4, 5, 6, 255]
01011100	[7]
00000101	[8, 9, 10]
01101101	[11, 12, 13, 14, 15, 16]
11000101	[17]
11001000	[18, 19, 20, 21, 22, 23]
10001111	[24, 25, 26, 27, 28, 29, 30, 31]
10001110	[32, 33, 34, 35, 36]
10000000	[37, 38, 39]
00100100	[40, 41, 42, 43]
11101100	[44, 45, 46, 47, 48]
00101001	[49, 50]
10000110	[51, 52, 53, 54, 55, 56, 57]
11010001	[58, 59, 60, 61, 62, 63, 64, 65, 66]
10111001	[67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98]
11110000	[99, 100, 101, 102, 103, 104, 105, 106]
10100110	[107, 108]
00110100	[109, 110, 111, 112, 113, 114]
10011110	[115]
10110110	[116, 117, 118, 119]
11100110	[120, 121, 122, 123, 124, 125, 126, 127, 128]
10111110	[129, 130, 131]
00110111	[132]
01110101	[133, 134, 135]
00111111	[136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158]
01010110	[159, 160, 161, 162, 163, 164, 165, 166, 167]
01111011	[168, 169, 170, 171, 172, 173]
11111010	[174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197]
01011111	[198, 199, 200]
01101010	[201]
00010000	[202]
01000110	[203, 204, 205]
10000010	[206, 207, 208]
11000010	[209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233]
01100000	[234, 235]
11001010	[236, 237, 238, 239, 240, 241, 242]
01011101	[243, 244, 245, 246]
01000000	[247]
00011001	[248, 249, 250, 251, 252, 253]
00011101	[254]

00000001	254	False
01011100	9	False
00000101	6	False
01101101	8	False
11000101	23	False
11001000	21	True
10001111	22	False
10001110	33	True
10000000	36	False
00100100	41	True
11101100	49	False
00101001	43	False
10000110	55	True
11010001	62	True
10111001	79	True
11110000	99	True
10100110	108	True
00110100	113	True
10011110	122	False
10110110	119	True
11100110	120	True
10111110	134	False
00110111	134	False
01110101	129	False
00111111	151	True
01010110	164	True
01111011	172	True
11111010	187	True
01011111	199	True
01101010	191	False
00010000	203	False
01000110	208	False
10000010	202	False
11000010	224	True
01100000	225	False
11001010	241	True
01011101	244	True
01000000	245	False
00011001	247	False
00011101	246	False

Number of codes used=40

Iteration=    110000 training nets give:
alice_loss.item()=0.0412302240729332	bob_loss.item()=0.26997894048690796

01000101	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 251, 252, 253, 254, 255]
11001000	[21, 22, 23]
10001111	[24]
11000100	[25, 26, 27, 28, 29]
01101001	[30, 31, 32, 33, 34, 35, 36]
11011101	[37, 38, 39, 40]
11010101	[41, 42, 43, 44, 45]
11101100	[46, 47, 48, 49]
10101100	[50, 51, 52, 53, 54, 55, 56]
10001001	[57, 58, 59, 60, 61]
10011100	[62, 63, 64, 65, 66, 67, 68, 69, 70]
10100100	[71, 72, 73, 74]
10101001	[75, 76, 77, 78]
11101111	[79, 80, 81, 82, 83, 84, 85]
10101111	[86]
11111011	[87, 88]
01111101	[89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
10110110	[115, 116, 117, 118, 119, 120, 121, 122, 123, 124]
11100110	[125, 126]
11010110	[127, 128, 129, 130, 131, 132, 133, 134]
00101110	[135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154]
01111111	[155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166]
01111011	[167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180]
00010011	[181, 182, 183]
11111010	[184, 185, 186]
01000010	[187, 188, 189, 190, 191]
01101010	[192, 193, 194, 195, 196, 197, 198, 199, 200, 201]
00000111	[202, 203, 204, 205, 206, 207, 208, 209, 210]
00000011	[211]
01000011	[212]
11011011	[213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230]
11101010	[231, 232]
00100000	[233]
00001111	[234, 235, 236, 237]
10001010	[238, 239, 240, 241]
11001010	[242]
01011101	[243, 244]
00011001	[245, 246, 247, 248, 249, 250]

01000101	3	True
11001000	22	True
10001111	23	False
11000100	34	False
01101001	32	True
11011101	36	False
11010101	48	False
11101100	52	False
10101100	53	True
10001001	62	False
10011100	73	False
10100100	65	False
10101001	79	False
11101111	83	True
10101111	87	False
11111011	111	False
01111101	100	True
10110110	118	True
11100110	121	False
11010110	129	True
00101110	149	True
01111111	158	True
01111011	177	True
00010011	180	False
11111010	185	True
01000010	188	True
01101010	197	True
00000111	206	True
00000011	213	False
01000011	218	False
11011011	230	True
11101010	222	False
00100000	223	False
00001111	234	True
10001010	236	False
11001010	239	False
01011101	248	False
00011001	253	False

Number of codes used=38

Iteration=    120000 training nets give:
alice_loss.item()=0.00960471760481596	bob_loss.item()=0.286861777305603

01011001	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 249, 250, 251, 252, 253, 254, 255]
00000101	[10, 11, 12]
01001001	[13, 14, 15, 16, 17, 18]
00101101	[19, 20]
11001000	[21, 22]
10000101	[23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]
11010100	[42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60]
10001001	[61, 62]
11101001	[63, 64]
10100100	[65, 66, 67, 68]
10011100	[69, 70, 71, 72, 73, 74, 75, 76]
11111101	[77, 78, 79, 80, 81, 82, 83, 84]
11110001	[85, 86]
10010000	[87, 88, 89, 90, 91, 92, 93]
11110100	[94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112]
11110111	[113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129]
00111101	[130]
00110101	[131]
00110111	[132, 133, 134, 135, 136]
01110100	[137]
00100110	[138, 139, 140, 141, 142, 143]
00111110	[144, 145, 146, 147, 148, 149]
00111111	[150, 151, 152, 153, 154, 155]
00111000	[156, 157, 158, 159, 160, 161, 162]
01111111	[163]
00111010	[164, 165, 166, 167, 168]
01111000	[169, 170, 171, 172, 173]
01100110	[174, 175, 176, 177, 178, 179, 180, 181]
01110000	[182, 183, 184, 185, 186, 187, 188]
00000110	[189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]
11011010	[200, 201, 202, 203, 204, 205, 206, 207]
00001010	[208, 209, 210, 211]
01010000	[212, 213, 214]
00011000	[215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236]
00001111	[237, 238, 239]
01000001	[240]
11000111	[241, 242, 243, 244, 245, 246, 247, 248]

01011001	254	True
00000101	6	False
01001001	7	False
00101101	21	False
11001000	22	True
10000101	35	True
11010100	48	True
10001001	61	True
11101001	62	False
10100100	67	True
10011100	77	False
11111101	87	False
11110001	83	False
10010000	89	True
11110100	99	True
11110111	120	True
00111101	132	False
00110101	135	False
00110111	133	True
01110100	137	True
00100110	142	True
00111110	147	True
00111111	162	False
00111000	153	False
01111111	161	False
00111010	165	True
01111000	177	False
01100110	181	True
01110000	174	False
00000110	198	True
11011010	207	True
00001010	210	True
01010000	217	False
00011000	226	True
00001111	238	True
01000001	234	False
11000111	252	False

Number of codes used=37

Iteration=    130000 training nets give:
alice_loss.item()=0.004376969765871763	bob_loss.item()=0.16780132055282593

01000101	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 248, 249, 250, 251, 252, 253, 254, 255]
01101101	[15, 16]
00101100	[17, 18, 19, 20]
11001000	[21, 22, 23, 24, 25, 26, 27]
10001111	[28]
01101001	[29]
11000101	[30, 31, 32, 33, 34]
11000100	[35, 36, 37, 38, 39]
00100100	[40, 41, 42, 43, 44, 45, 46]
10011000	[47, 48, 49, 50, 51, 52]
11100000	[53]
10000001	[54, 55, 56]
11101101	[57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80]
11101111	[81, 82, 83]
10111101	[84]
10101111	[85]
11111100	[86]
11110000	[87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98]
10111100	[99, 100, 101, 102]
10100111	[103, 104]
11111011	[105, 106, 107]
10111011	[108, 109, 110, 111]
10010011	[112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128]
00110110	[129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147]
00010110	[148, 149]
00011110	[150, 151, 152, 153]
01010110	[154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170]
11110010	[171, 172, 173, 174, 175]
01111011	[176, 177, 178]
01010010	[179, 180, 181, 182, 183]
00100111	[184, 185, 186, 187, 188, 189]
01000010	[190, 191]
10011010	[192, 193, 194, 195, 196, 197, 198]
00100011	[199, 200, 201]
01011111	[202, 203]
11010011	[204, 205, 206, 207, 208, 209, 210, 211, 212, 213]
00011000	[214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238]
01010101	[239]
01100001	[240, 241, 242, 243, 244]
01101000	[245, 246]
00011101	[247]

01000101	6	True
01101101	19	False
00101100	16	False
11001000	25	True
10001111	29	False
01101001	32	False
11000101	33	True
11000100	37	True
00100100	36	False
10011000	53	False
11100000	54	False
10000001	62	False
11101101	66	True
11101111	70	False
10111101	86	False
10101111	83	False
11111100	91	False
11110000	95	True
10111100	99	True
10100111	102	False
11111011	104	False
10111011	114	False
10010011	123	True
00110110	132	True
00010110	146	False
00011110	152	True
01010110	164	True
11110010	171	True
01111011	183	False
01010010	180	True
00100111	190	False
01000010	195	False
10011010	185	False
00100011	199	True
01011111	199	False
11010011	202	False
00011000	224	True
01010101	237	False
01100001	244	True
01101000	244	False
00011101	253	False

Number of codes used=41

Iteration=    140000 training nets give:
alice_loss.item()=0.01473834179341793	bob_loss.item()=0.301482617855072

00011101	[0, 1, 2, 3, 4, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00000101	[5, 6, 7, 8, 9]
01001001	[10, 11, 12]
11001111	[13]
00000100	[14, 15, 16]
00100101	[17, 18, 19, 20, 21, 22]
11001000	[23, 24, 25, 26]
10001111	[27, 28, 29]
11001101	[30, 31, 32, 33]
10001100	[34, 35, 36, 37, 38]
10000101	[39, 40]
11001001	[41, 42, 43, 44]
11010100	[45, 46, 47, 48, 49, 50, 51, 52]
11100000	[53, 54, 55, 56, 57]
10000001	[58, 59, 60, 61]
11100101	[62, 63, 64, 65, 66, 67]
11100001	[68, 69]
11101111	[70]
10011101	[71, 72, 73, 74, 75, 76, 77, 78, 79]
11100011	[80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92]
11110001	[93, 94, 95]
11100111	[96]
10111100	[97, 98, 99, 100]
11111011	[101, 102, 103, 104, 105, 106]
10110000	[107, 108, 109, 110, 111]
10110111	[112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128]
10100010	[129]
11010110	[130, 131, 132, 133, 134, 135, 136, 137]
00010101	[138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153]
00011110	[154]
00111010	[155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167]
01110000	[168, 169, 170, 171, 172]
00010010	[173, 174]
01111010	[175, 176, 177, 178, 179]
01111011	[180]
11010010	[181, 182, 183, 184]
00010011	[185, 186]
01100110	[187, 188, 189]
01000010	[190, 191]
11100010	[192, 193, 194, 195, 196, 197]
00100011	[198, 199, 200]
00000111	[201, 202, 203, 204, 205, 206, 207, 208]
01000111	[209, 210, 211, 212, 213, 214, 215, 216, 217]
00101011	[218, 219, 220, 221, 222, 223]
00011000	[224, 225, 226, 227]
01001011	[228]
01001111	[229, 230, 231, 232, 233, 234, 235]
01010101	[236, 237, 238, 239]

00011101	253	True
00000101	5	True
01001001	7	False
11001111	9	False
00000100	17	False
00100101	14	False
11001000	26	True
10001111	28	True
11001101	27	False
10001100	37	True
10000101	39	True
11001001	43	True
11010100	46	True
11100000	56	True
10000001	64	False
11100101	62	True
11100001	66	False
11101111	65	False
10011101	77	True
11100011	87	True
11110001	92	False
11100111	92	False
10111100	100	True
11111011	105	True
10110000	107	True
10110111	122	True
10100010	121	False
11010110	140	False
00010101	145	True
00011110	150	False
00111010	159	True
01110000	171	True
00010010	175	False
01111010	175	True
01111011	178	False
11010010	198	False
00010011	189	False
01100110	190	False
01000010	195	False
11100010	201	False
00100011	205	False
00000111	206	True
01000111	216	True
00101011	219	True
00011000	229	False
01001011	225	False
01001111	233	True
01010101	240	False

Number of codes used=48

Iteration=    150000 training nets give:
alice_loss.item()=0.0057923635467886925	bob_loss.item()=0.28175753355026245

00101000	[0, 1, 2, 3, 4, 255]
01001000	[5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
00100101	[15, 16, 17, 18, 19]
11011000	[20, 21, 22, 23, 24, 25]
11001000	[26]
11001101	[27, 28, 29]
10001000	[30, 31, 32]
10001110	[33, 34]
10001100	[35, 36, 37, 38, 39]
11001001	[40, 41, 42, 43, 44, 45, 46, 47]
10001101	[48, 49, 50, 51, 52]
11100000	[53, 54]
10101100	[55, 56, 57, 58, 59]
10000001	[60]
11010000	[61, 62]
11100001	[63, 64, 65, 66]
11101111	[67, 68, 69, 70, 71, 72]
11111001	[73, 74, 75, 76, 77, 78]
10011101	[79]
11101011	[80]
10111000	[81, 82, 83, 84, 85, 86]
10111101	[87, 88, 89, 90]
11110000	[91, 92]
10111100	[93, 94, 95, 96]
00011100	[97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117]
01111100	[118, 119, 120, 121, 122, 123, 124]
01110101	[125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144]
01110111	[145, 146, 147, 148]
00111110	[149, 150, 151]
10111010	[152]
01111111	[153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166]
00100110	[167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186]
01100110	[187, 188]
00000010	[189, 190, 191, 192]
01011111	[193, 194, 195, 196, 197, 198]
00101110	[199, 200, 201, 202, 203, 204]
00100011	[205, 206]
00000111	[207]
01101011	[208]
00101111	[209, 210]
01001010	[211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223]
11000010	[224, 225, 226, 227]
00011000	[228, 229, 230, 231]
00100000	[232, 233, 234]
01100000	[235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254]

00101000	1	True
01001000	2	False
00100101	11	False
11011000	22	True
11001000	25	False
11001101	28	True
10001000	31	True
10001110	30	False
10001100	33	False
11001001	41	True
10001101	46	False
11100000	62	False
10101100	59	True
10000001	63	False
11010000	60	False
11100001	63	True
11101111	73	False
11111001	74	True
10011101	75	False
11101011	81	False
10111000	81	True
10111101	88	True
11110000	95	False
10111100	97	False
00011100	110	True
01111100	119	True
01110101	133	True
01110111	144	False
00111110	150	True
10111010	152	True
01111111	161	True
00100110	175	True
01100110	187	True
00000010	195	False
01011111	196	True
00101110	194	False
00100011	208	False
00000111	208	False
01101011	222	False
00101111	213	False
01001010	220	True
11000010	219	False
00011000	231	True
00100000	239	False
01100000	249	True

Number of codes used=45

Iteration=    160000 training nets give:
alice_loss.item()=0.0032646777108311653	bob_loss.item()=0.21566228568553925

11000111	[0, 1, 2, 3, 4, 250, 251, 252, 253, 254, 255]
01000000	[5, 6, 7, 8, 9, 10]
01101100	[11]
01101101	[12, 13, 14]
00000100	[15, 16]
10000111	[17, 18, 19, 20, 21]
00101101	[22, 23, 24]
10000101	[25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]
10001101	[45, 46, 47]
11101100	[48, 49, 50, 51, 52, 53, 54]
11010000	[55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]
10100101	[70, 71, 72]
10111001	[73, 74, 75, 76, 77, 78, 79, 80, 81, 82]
11101011	[83, 84]
11111000	[85, 86, 87, 88, 89, 90]
11110000	[91, 92, 93]
10100011	[94, 95]
10101110	[96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110]
10110011	[111, 112, 113, 114, 115]
10100110	[116, 117, 118]
10111110	[119, 120, 121, 122, 123, 124, 125, 126, 127, 128]
00111100	[129]
00010101	[130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158]
00111000	[159, 160, 161]
11111010	[162, 163, 164, 165, 166]
01110000	[167, 168, 169, 170, 171, 172, 173]
00100110	[174]
01111011	[175, 176, 177, 178, 179, 180]
00011111	[181, 182]
01010010	[183, 184, 185, 186]
01100010	[187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201]
10000010	[202, 203]
11100010	[204, 205, 206, 207, 208, 209, 210, 211]
01101011	[212, 213, 214, 215, 216, 217]
01001010	[218, 219, 220, 221]
01001011	[222, 223, 224, 225, 226]
00011000	[227, 228, 229, 230, 231, 232, 233, 234, 235]
00100000	[236, 237, 238]
01010101	[239, 240]
01000001	[241, 242, 243, 244]
01101000	[245, 246]
01100000	[247, 248, 249]

11000111	6	False
01000000	12	False
01101100	6	False
01101101	13	True
00000100	8	False
10000111	23	False
00101101	23	True
10000101	28	True
10001101	41	False
11101100	55	False
11010000	57	True
10100101	78	False
10111001	75	True
11101011	84	True
11111000	94	False
11110000	97	False
10100011	92	False
10101110	105	True
10110011	113	True
10100110	119	False
10111110	121	True
00111100	133	False
00010101	140	True
00111000	158	False
11111010	158	False
01110000	169	True
00100110	178	False
01111011	179	True
00011111	183	False
01010010	183	True
01100010	188	True
10000010	208	False
11100010	210	True
01101011	214	True
01001010	221	True
01001011	222	True
00011000	236	False
00100000	234	False
01010101	245	False
01000001	246	False
01101000	242	False
01100000	248	True

Number of codes used=42

Iteration=    170000 training nets give:
alice_loss.item()=0.0011994719970971346	bob_loss.item()=0.25918442010879517

01001000	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 248, 249, 250, 251, 252, 253, 254, 255]
11001100	[16, 17, 18, 19, 20]
11001000	[21, 22, 23, 24, 25, 26, 27, 28]
10001000	[29, 30, 31]
11011100	[32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]
10001001	[51, 52, 53, 54, 55, 56, 57]
11000001	[58, 59, 60]
10101100	[61, 62, 63, 64]
11100100	[65, 66, 67]
11101111	[68, 69, 70, 71, 72, 73, 74, 75, 76, 77]
10011100	[78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88]
11111101	[89]
11110001	[90, 91]
10100011	[92, 93]
11111100	[94, 95, 96, 97, 98, 99, 100]
11111011	[101, 102, 103, 104, 105, 106, 107, 108]
10111111	[109]
10110011	[110, 111, 112, 113, 114]
01111100	[115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130]
10011110	[131, 132, 133, 134, 135, 136, 137]
00110101	[138, 139]
10010010	[140, 141, 142, 143, 144]
00010110	[145, 146]
00110001	[147, 148]
00011110	[149, 150, 151, 152, 153, 154, 155]
00110010	[156]
01111111	[157, 158, 159, 160]
00111010	[161, 162, 163, 164, 165]
00111011	[166]
11110010	[167, 168, 169, 170]
01110010	[171, 172, 173, 174, 175, 176]
01111011	[177, 178, 179, 180, 181]
00011111	[182, 183, 184, 185, 186]
01100110	[187, 188]
00100010	[189, 190, 191, 192, 193, 194]
00101110	[195, 196]
00101010	[197, 198, 199, 200]
01100111	[201, 202, 203, 204]
11000010	[205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228]
00011000	[229, 230, 231, 232, 233, 234, 235, 236, 237]
01010001	[238]
01010101	[239, 240, 241, 242, 243, 244]
00011101	[245, 246, 247]

01001000	11	True
11001100	14	False
11001000	21	True
10001000	29	True
11011100	40	True
10001001	58	False
11000001	61	False
10101100	66	False
11100100	67	True
11101111	70	True
10011100	95	False
11111101	85	False
11110001	89	False
10100011	91	False
11111100	96	True
11111011	104	True
10111111	117	False
10110011	109	False
01111100	125	True
10011110	133	True
00110101	139	True
10010010	142	True
00010110	145	True
00110001	149	False
00011110	154	True
00110010	155	False
01111111	160	True
00111010	162	True
00111011	170	False
11110010	169	True
01110010	168	False
01111011	175	False
00011111	185	True
01100110	181	False
00100010	195	False
00101110	196	True
00101010	198	True
01100111	205	False
11000010	215	True
00011000	232	True
01010001	233	False
01010101	244	True
00011101	254	False

Number of codes used=43

Iteration=    180000 training nets give:
alice_loss.item()=0.010221741162240505	bob_loss.item()=0.3524244427680969

01011101	[0, 1, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00001100	[2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
01101101	[12, 13, 14]
00100001	[15, 16, 17, 18, 19, 20]
10000111	[21]
10000000	[22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]
11011101	[50, 51]
11101100	[52, 53, 54]
10001011	[55, 56, 57]
10100000	[58, 59, 60]
11100000	[61, 62, 63, 64, 65]
11101101	[66]
10011001	[67, 68]
10000011	[69, 70, 71, 72, 73]
10101011	[74, 75, 76, 77, 78]
11101011	[79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91]
11110001	[92]
10100011	[93, 94, 95]
11111100	[96]
10111000	[97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107]
10011011	[108]
10110011	[109, 110]
10111111	[111]
10111011	[112, 113, 114]
11100110	[115, 116, 117, 118]
11101110	[119, 120, 121, 122]
01111100	[123, 124, 125, 126]
00010100	[127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138]
11111110	[139]
00010111	[140, 141]
10111010	[142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]
01010110	[160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174]
01111010	[175, 176, 177]
00100110	[178, 179, 180]
00011111	[181, 182, 183, 184, 185, 186]
01100010	[187, 188]
00011011	[189, 190, 191, 192]
11010011	[193, 194, 195, 196, 197, 198, 199, 200]
00001010	[201, 202, 203, 204, 205, 206]
01001110	[207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227]
11000011	[228, 229, 230, 231, 232, 233, 234]
01011000	[235, 236, 237, 238, 239, 240, 241]
01010101	[242, 243, 244, 245]

01011101	253	True
00001100	5	True
01101101	13	True
00100001	18	True
10000111	24	False
10000000	36	True
11011101	43	False
11101100	51	False
10001011	61	False
10100000	54	False
11100000	59	False
11101101	66	True
10011001	71	False
10000011	72	True
10101011	75	True
11101011	83	True
11110001	93	False
10100011	98	False
11111100	93	False
10111000	98	True
10011011	105	False
10110011	108	False
10111111	116	False
10111011	113	True
11100110	119	False
11101110	121	True
01111100	126	True
00010100	133	True
11111110	140	False
00010111	145	False
10111010	149	True
01010110	166	True
01111010	175	True
00100110	177	False
00011111	178	False
01100010	194	False
00011011	190	True
11010011	196	True
00001010	206	True
01001110	218	True
11000011	227	False
01011000	243	False
01010101	245	True

Number of codes used=43

Iteration=    190000 training nets give:
alice_loss.item()=0.0028488198295235634	bob_loss.item()=0.08309467136859894

01011101	[0, 1, 2, 3, 248, 249, 250, 251, 252, 253, 254, 255]
00001101	[4]
00101100	[5, 6, 7, 8, 9]
01101101	[10, 11, 12, 13]
00100001	[14, 15, 16, 17, 18, 19]
11000101	[20, 21]
11001000	[22, 23]
10001111	[24]
10000111	[25, 26]
10001110	[27, 28, 29]
11001101	[30, 31, 32, 33, 34]
10001100	[35, 36]
11010101	[37, 38, 39, 40, 41]
10101000	[42, 43, 44, 45]
11101100	[46, 47, 48, 49, 50, 51, 52, 53]
10101100	[54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
11101111	[72]
11111001	[73]
10101011	[74, 75, 76, 77, 78, 79, 80]
11110101	[81, 82]
11101011	[83, 84, 85]
11111101	[86, 87]
10110101	[88, 89, 90, 91, 92, 93, 94, 95, 96, 97]
10101010	[98, 99, 100, 101, 102, 103]
10011011	[104, 105, 106, 107]
11111111	[108, 109, 110, 111, 112, 113, 114, 115, 116]
11110111	[117]
10110110	[118, 119, 120]
01010100	[121, 122, 123, 124, 125, 126, 127]
00110110	[128]
01111110	[129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141]
00010110	[142, 143, 144, 145]
11011110	[146]
00111110	[147, 148, 149, 150, 151]
00011110	[152, 153, 154, 155, 156, 157]
01111111	[158, 159, 160, 161, 162, 163, 164]
00111011	[165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181]
01100110	[182]
01010111	[183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198]
00001010	[199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212]
00101011	[213, 214, 215]
00001011	[216, 217, 218, 219, 220, 221, 222, 223]
00001111	[224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242]
01010101	[243, 244, 245, 246, 247]

01011101	4	False
00001101	3	False
00101100	10	False
01101101	14	False
00100001	18	True
11000101	22	False
11001000	20	False
10001111	20	False
10000111	21	False
10001110	33	False
11001101	28	False
10001100	30	False
11010101	40	True
10101000	41	False
11101100	46	True
10101100	57	True
11101111	71	False
11111001	73	True
10101011	76	True
11110101	87	False
11101011	84	True
11111101	87	True
10110101	94	True
10101010	103	True
10011011	109	False
11111111	116	True
11110111	116	False
10110110	119	True
01010100	120	False
00110110	124	False
01111110	132	True
00010110	147	False
11011110	141	False
00111110	146	False
00011110	148	False
01111111	162	True
00111011	166	True
01100110	184	False
01010111	199	False
00001010	213	False
00101011	215	True
00001011	218	True
00001111	231	True
01010101	248	False

Number of codes used=44

Iteration=    200000 training nets give:
alice_loss.item()=0.00011739073670469224	bob_loss.item()=0.0005436095525510609

00011101	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01101101	[11, 12]
11001000	[13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
10000101	[32]
11011100	[33, 34, 35, 36]
11010101	[37, 38, 39, 40, 41, 42]
11011101	[43, 44, 45]
10101100	[46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68]
11101111	[69, 70, 71, 72]
10101011	[73, 74, 75, 76, 77, 78, 79]
11110101	[80, 81, 82, 83, 84, 85]
11111101	[86, 87]
10110101	[88, 89, 90, 91, 92, 93, 94, 95, 96, 97]
01111101	[98, 99, 100]
10101010	[101, 102, 103]
10011011	[104, 105, 106, 107, 108, 109, 110]
10011111	[111]
10111011	[112, 113, 114, 115]
11110111	[116]
10110110	[117, 118, 119, 120]
10111110	[121, 122, 123, 124, 125, 126, 127, 128, 129]
01111110	[130, 131, 132]
01110100	[133, 134, 135, 136]
10110010	[137]
00110101	[138]
00111111	[139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160]
01111111	[161, 162]
01011110	[163, 164, 165]
01110010	[166, 167]
01110000	[168, 169, 170]
01111000	[171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182]
01100110	[183, 184, 185]
00010011	[186, 187, 188]
01111001	[189, 190, 191, 192, 193]
01011111	[194, 195, 196, 197]
01010011	[198, 199]
00101110	[200, 201, 202, 203]
01100111	[204, 205]
00000111	[206, 207, 208, 209, 210, 211, 212, 213]
00001110	[214, 215, 216, 217, 218]
00001011	[219, 220, 221, 222]
01000011	[223]
01001111	[224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244]

00011101	254	True
01101101	13	False
11001000	19	True
10000101	32	True
11011100	35	True
11010101	39	True
11011101	44	True
10101100	53	True
11101111	69	True
10101011	72	False
11110101	85	True
11111101	88	False
10110101	93	True
01111101	96	False
10101010	103	True
10011011	108	True
10011111	116	False
10111011	113	True
11110111	116	True
10110110	118	True
10111110	124	True
01111110	135	False
01110100	135	True
10110010	135	False
00110101	133	False
00111111	140	True
01111111	157	False
01011110	164	True
01110010	165	False
01110000	169	True
01111000	172	True
01100110	182	False
00010011	180	False
01111001	193	True
01011111	193	False
01010011	198	True
00101110	201	True
01100111	203	False
00000111	209	True
00001110	214	True
00001011	220	True
01000011	226	False
01001111	232	True

Number of codes used=43


End of hp run 2.  Result of run:
[(-0.99061553135987, 200000), ('21-05-12_21:01:48BST_NLearn_model_2_Alice_iter200000', '21-05-12_21:01:48BST_NLearn_model_2_Bob_iter200000')]
(-0.99061553135987, 200000)



Time taken over all 2 given sets of hyperparameters=11:52:01, averaging 5:56:01 per run


 ---- Table of results ----

 code  hp_run  result
   00       1  (-0.991, 200000)
   01       2  (-0.991, 200000)
 --------------------------

++++ Best result was (-0.991, 200000) on hp_run=1 with
hyperparameters = {
	'N_ITERATIONS': 200000,
	'RANDOM_SEEDS': (714844, 936892, 888616, 165835),
	'ALICE_NET': 'MaxNet("In", 3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
	'n_rng': Generator(PCG64)
	'ne_rng': Generator(PCG64)
	't_rng': <torch._C.Generator object at 0x7fd586c62cf0>
	'te_rng': <torch._C.Generator object at 0x7fd586c62970>
}


End closed log for run 21-05-12_21:01:48BST
