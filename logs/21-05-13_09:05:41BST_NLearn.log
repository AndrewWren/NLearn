The closed log for run 21-05-13_09:05:41BST

SMOOTHING_LENGTH = 10000
SAVE_PERIOD = 100000
CODE_BOOK_PERIOD = 10000
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # Used: "cuda"
MODEL_FOLDER = 'models'
CONFIGS_FOLDER = 'configs'
LOGS_FOLDER = 'logs'

hyperparameters = {
	'N_ITERATIONS': 200000,
	'RANDOM_SEEDS': [(714844, 936892, 888616, 165835)],
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}



>>>> hp_run=1 of 1
hyperparameters = {
	'N_ITERATIONS': 200000,
	'RANDOM_SEEDS': (714844, 936892, 888616, 165835),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=     20000 training nets give:
alice_loss.item()=0.6578581929206848	bob_loss.item()=0.6110206842422485

01101011	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01000011	[31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121]
01110000	[122, 123, 124]
01001101	[125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216]

01101011	117	False
01000011	108	True
01110000	148	False
01001101	65	False

Number of codes used=4

Iteration=     30000 training nets give:
alice_loss.item()=0.10643632709980011	bob_loss.item()=0.22639504075050354

00111101	[0, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10101100	[1]
10110001	[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76]
01000011	[77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121]
01110000	[122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183]

00111101	211	True
10101100	239	False
10110001	45	True
01000011	92	True
01110000	139	True

Number of codes used=5

Iteration=     40000 training nets give:
alice_loss.item()=0.04863444343209267	bob_loss.item()=0.14201942086219788

00111111	[0, 1, 2, 3, 4, 5, 6, 7, 250, 251, 252, 253, 254, 255]
10100001	[8, 9, 10, 11, 12, 13, 14, 15]
10101110	[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66]
10010001	[67, 68, 69, 70, 71, 72, 73, 74]
11000001	[75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85]
01000011	[86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]
01010011	[101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]
11110100	[120, 121, 122, 123, 124, 125, 126, 127]
01110000	[128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147]
01110100	[148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179]
01111101	[180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]
00111101	[200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226]
10101100	[227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249]

00111111	245	False
10100001	44	False
10101110	7	False
10010001	47	False
11000001	83	True
01000011	89	True
01010011	99	False
11110100	147	False
01110000	137	True
01110100	161	True
01111101	199	True
00111101	201	True
10101100	6	False

Number of codes used=13

Iteration=     50000 training nets give:
alice_loss.item()=0.07550019025802612	bob_loss.item()=0.21210627257823944

00111001	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10101101	[11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
10110100	[26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]
10100100	[37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54]
10010011	[55, 56, 57, 58]
11001011	[59, 60]
10110111	[61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76]
11000011	[77, 78, 79, 80, 81, 82, 83]
11000001	[84]
10100111	[85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109]
01010111	[110, 111, 112, 113, 114, 115, 116]
01110010	[117, 118, 119, 120, 121, 122, 123]
01011000	[124, 125]
01110000	[126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141]
00110100	[142, 143, 144, 145]
00100100	[146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157]
01110101	[158, 159, 160, 161, 162, 163, 164, 165, 166]
01111000	[167, 168, 169]
01111100	[170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183]
01101101	[184, 185, 186, 187, 188, 189, 190, 191, 192]
10101011	[193, 194, 195]
00111101	[196, 197, 198, 199]
01111101	[200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211]
00111100	[212]
10001100	[213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226]
10111101	[227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242]
10111111	[243, 244]

00111001	255	True
10101101	3	False
10110100	23	False
10100100	41	True
10010011	55	True
11001011	69	False
10110111	61	True
11000011	82	True
11000001	86	False
10100111	39	False
01010111	108	False
01110010	111	False
01011000	130	False
01110000	140	True
00110100	141	False
00100100	158	False
01110101	158	True
01111000	162	False
01111100	180	True
01101101	190	True
10101011	24	False
00111101	196	True
01111101	206	True
00111100	196	False
10001100	237	False
10111101	235	True
10111111	251	False

Number of codes used=27

Iteration=     60000 training nets give:
alice_loss.item()=0.014855923131108284	bob_loss.item()=0.032117150723934174

00111001	[0, 1, 2, 249, 250, 251, 252, 253, 254, 255]
11111110	[3, 4, 5, 6]
00101011	[7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
00111010	[17]
11111001	[18, 19, 20, 21, 22, 23, 24, 25]
10000101	[26, 27]
11110001	[28, 29, 30]
00110001	[31, 32]
10111001	[33, 34, 35, 36, 37, 38, 39]
10100000	[40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]
10110001	[51, 52]
10010011	[53, 54, 55, 56, 57, 58, 59, 60, 61]
10010111	[62, 63, 64]
10110111	[65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75]
11010001	[76, 77, 78, 79, 80]
11000001	[81, 82, 83, 84, 85, 86, 87]
01000011	[88, 89, 90]
01100010	[91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103]
00010000	[104, 105, 106, 107]
01010001	[108]
01100001	[109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121]
01000101	[122, 123]
01000000	[124, 125]
01010110	[126, 127, 128, 129]
00110000	[130, 131, 132, 133, 134]
11110110	[135]
01110000	[136, 137, 138, 139, 140, 141, 142, 143, 144, 145]
01001100	[146, 147, 148]
01100101	[149, 150, 151, 152, 153, 154, 155, 156, 157, 158]
01110101	[159]
01011100	[160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170]
01111110	[171, 172]
01111100	[173, 174, 175, 176, 177, 178, 179, 180, 181, 182]
01101101	[183, 184, 185, 186, 187, 188, 189, 190, 191, 192]
00111101	[193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203]
01011101	[204, 205, 206, 207]
11111101	[208, 209, 210, 211, 212]
00011100	[213, 214, 215, 216, 217, 218, 219, 220, 221, 222]
00101101	[223, 224, 225, 226, 227, 228, 230, 231]
00111111	[229]
10111101	[232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243]
00111110	[244]
10111111	[245, 246, 247, 248]

00111001	254	True
11111110	10	False
00101011	2	False
00111010	12	False
11111001	20	True
10000101	39	False
11110001	35	False
00110001	27	False
10111001	27	False
10100000	39	False
10110001	43	False
10010011	63	False
10010111	55	False
10110111	74	True
11010001	73	False
11000001	81	True
01000011	93	False
01100010	100	True
00010000	113	False
01010001	105	False
01100001	118	True
01000101	121	False
01000000	125	True
01010110	126	True
00110000	138	False
11110110	132	False
01110000	151	False
01001100	162	False
01100101	157	True
01110101	158	False
01011100	169	True
01111110	173	False
01111100	192	False
01101101	190	True
00111101	198	True
01011101	200	False
11111101	204	False
00011100	219	True
00101101	227	True
00111111	247	False
10111101	229	False
00111110	238	False
10111111	248	True

Number of codes used=43

Iteration=     70000 training nets give:
alice_loss.item()=0.022226957604289055	bob_loss.item()=0.11653482913970947

10101000	[0, 1, 2, 3, 249, 250, 251, 252, 253, 254, 255]
10011100	[4, 5]
00011000	[6]
00011111	[7]
11101110	[8, 9, 10, 11]
10100101	[12, 13, 14]
11111001	[15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
00011001	[26, 27]
11110001	[28, 29, 30, 31, 32, 33, 34, 35, 36]
10100001	[37, 38, 39, 40, 41, 42]
10110001	[43, 44, 45, 46, 47, 48, 49]
00100011	[50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60]
10010011	[61, 62]
11110111	[63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]
11010001	[75, 76, 77, 78]
11100111	[79, 80, 81, 82, 83, 84, 85]
00010111	[86, 87, 88, 89, 90, 91, 92, 93]
01000010	[94]
00110010	[95, 96, 97]
01010111	[98, 99, 100, 101, 102, 103, 104]
01010001	[105, 106, 107]
11011000	[108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120]
01000101	[121, 122, 123, 124]
01010110	[125, 126, 127]
11110110	[128, 129, 130, 131]
01011000	[132, 133, 134, 135, 136, 137, 138, 139]
01010100	[140, 141, 142, 143, 144, 145]
11111000	[146, 147, 148, 149, 150, 151, 152, 153, 154, 155]
00110101	[156, 157, 158, 159, 160, 161, 162, 163, 164]
01011100	[165, 166, 167, 168]
01111110	[169, 170, 171, 172, 173, 174]
01101100	[175, 176, 177, 178, 179, 180, 181, 182, 183]
01101101	[184, 185, 186, 187, 188]
01111100	[189, 190, 191, 192, 193]
00111101	[194, 195, 196, 197, 198, 199]
01111101	[200, 201, 202, 203]
11111101	[204, 205, 206, 207, 208, 209]
00111100	[210]
00011100	[211, 212, 213, 214, 215, 216, 217, 218, 219, 220]
00101101	[221, 222]
11011101	[223, 224, 225, 226, 227, 228, 229, 230]
11111111	[231, 232, 233, 234, 235, 236, 237, 238, 239, 240]
00001101	[241, 242, 243, 244, 245]
10111111	[246, 247, 248]

10101000	242	False
10011100	249	False
00011000	9	False
00011111	7	True
11101110	5	False
10100101	17	False
11111001	22	True
00011001	10	False
11110001	36	True
10100001	45	False
10110001	43	True
00100011	62	False
10010011	60	False
11110111	73	True
11010001	73	False
11100111	87	False
00010111	83	False
01000010	96	False
00110010	104	False
01010111	104	True
01010001	111	False
11011000	121	False
01000101	125	False
01010110	126	True
11110110	130	True
01011000	134	True
01010100	141	True
11111000	155	True
00110101	162	True
01011100	182	False
01111110	172	True
01101100	188	False
01101101	188	True
01111100	181	False
00111101	208	False
01111101	199	False
11111101	201	False
00111100	206	False
00011100	211	True
00101101	208	False
11011101	224	True
11111111	235	True
00001101	237	False
10111111	246	True

Number of codes used=44

Iteration=     80000 training nets give:
alice_loss.item()=0.08697804063558578	bob_loss.item()=0.23673728108406067

10001000	[0, 1, 2, 253, 254, 255]
10011110	[3, 4, 5]
00111000	[6, 7, 8]
00011001	[9, 10, 11, 12, 13, 14, 15, 16, 17]
01111011	[18, 19, 20, 21, 22, 23, 24]
10110100	[25, 26, 27, 28, 29, 30, 31, 32, 33]
10100000	[34, 35, 36, 37, 38, 39]
10010101	[40, 41, 42, 43, 44, 45, 46]
10110011	[47, 48, 49, 50, 51, 52, 53, 54]
11110011	[55, 56, 57, 58, 59, 60, 61]
10100111	[62, 63, 64, 65, 66, 67]
10000010	[68, 69, 70, 71, 72, 73, 74, 75, 76, 77]
00010011	[78, 79, 80, 81]
11010011	[82, 83, 84]
11100111	[85]
00000111	[86]
00000101	[87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]
01000001	[101, 102, 103, 104]
01110111	[105, 106, 107, 108]
11011000	[109, 110, 111, 112, 113, 114, 115, 116, 117, 118]
01100111	[119, 120, 121]
01101010	[122, 123, 124, 125]
11010100	[126, 127, 128, 129, 130, 131, 132, 133, 134]
01011000	[135, 136, 137]
01100110	[138, 139, 140]
01010100	[141, 142, 143, 144, 145, 146, 147, 148, 149]
01001110	[150, 151, 152, 153, 154, 155, 156]
01011110	[157, 158, 159, 160, 161]
01111000	[162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172]
01111110	[173, 174, 175, 176, 177]
01101100	[178, 179, 180, 181, 182]
01101101	[183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193]
11111110	[194, 195, 196, 197, 198, 199, 200, 202]
00111101	[201]
00111100	[203, 204, 205, 206, 207, 208, 209, 210]
00101101	[211, 212, 213, 214, 215, 216, 217]
00011101	[218, 219, 220]
11101101	[221, 222, 223, 224, 225, 226, 227, 228]
10001100	[229, 230, 231]
00111111	[232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252]

10001000	253	True
10011110	252	False
00111000	7	True
00011001	12	True
01111011	26	False
10110100	27	True
10100000	39	True
10010101	34	False
10110011	56	False
11110011	57	True
10100111	55	False
10000010	67	False
00010011	80	True
11010011	79	False
11100111	84	False
00000111	99	False
00000101	80	False
01000001	99	False
01110111	110	False
11011000	119	False
01100111	129	False
01101010	129	False
11010100	130	True
01011000	132	False
01100110	140	True
01010100	145	True
01001110	155	True
01011110	153	False
01111000	164	True
01111110	173	True
01101100	183	False
01101101	185	True
11111110	200	True
00111101	195	False
00111100	205	True
00101101	214	True
00011101	220	True
11101101	217	False
10001100	231	True
00111111	233	True

Number of codes used=40

Iteration=     90000 training nets give:
alice_loss.item()=0.01684795692563057	bob_loss.item()=0.33328214287757874

10001110	[0, 1, 2, 3, 4, 5, 6, 253, 254, 255]
10111011	[7]
00011000	[8, 9, 10, 11]
00011010	[12, 13, 14, 15, 16, 17, 18, 19, 20, 21]
01111011	[22, 23, 24, 25, 26, 27, 28, 29]
10000101	[30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]
10100110	[47, 48, 49, 50, 51, 52, 53, 54]
10000110	[55, 56]
10100111	[57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67]
10000010	[68]
11010001	[69, 70, 71, 72, 73, 74]
00000011	[75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86]
01100011	[87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101]
00110010	[102, 103, 104, 105, 106]
00100110	[107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118]
11010101	[119, 120, 121]
01110001	[122]
11110000	[123, 124, 125, 126, 127, 128, 129, 130, 131, 132]
11110110	[133, 134]
00110000	[135, 136, 137, 138, 139]
01010100	[140, 141, 142, 143, 144, 145, 146, 147, 148, 149]
01100100	[150, 151]
01001100	[152, 153]
01110101	[154, 155, 156, 157, 158, 159, 160]
00110101	[161, 162]
01011100	[163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175]
01101110	[176]
01101100	[177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187]
01101111	[188, 189, 190, 191]
01111111	[192, 193, 194, 195, 196]
11111110	[197, 198, 199, 200]
00111110	[201, 202, 203, 204, 205, 206, 207, 208]
00011100	[209, 210, 211, 212, 213, 214, 215, 216, 217]
11101101	[218, 219]
00101100	[220]
00011101	[221, 222, 223, 224, 225]
10111101	[226, 227, 228, 229]
00011110	[230, 231, 232, 233, 234, 235, 236, 237, 238]
10011101	[239, 240, 241, 242, 243, 244, 245, 246, 247, 248]
10111111	[249]
10101100	[250, 251, 252]

10001110	2	True
10111011	5	False
00011000	11	True
00011010	22	False
01111011	31	False
10000101	53	False
10100110	41	False
10000110	56	True
10100111	53	False
10000010	79	False
11010001	78	False
00000011	84	True
01100011	80	False
00110010	106	True
00100110	119	False
11010101	122	False
01110001	120	False
11110000	135	False
11110110	132	False
00110000	129	False
01010100	144	True
01100100	153	False
01001100	150	False
01110101	155	True
00110101	164	False
01011100	164	True
01101110	178	False
01101100	185	True
01101111	191	True
01111111	191	False
11111110	203	False
00111110	199	False
00011100	208	False
11101101	214	False
00101100	216	False
00011101	220	False
10111101	220	False
00011110	227	False
10011101	239	True
10111111	235	False
10101100	244	False

Number of codes used=41

Iteration=    100000 training nets give:
alice_loss.item()=0.017211925238370895	bob_loss.item()=0.22338099777698517

00101001	[0, 1, 249, 250, 251, 252, 253, 254, 255]
10011111	[2, 3, 4, 5, 6, 7, 8, 9]
00011001	[10, 11, 12, 13, 14]
00011000	[15, 16]
00001011	[17, 18, 19, 20, 21, 22]
00101000	[23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]
10101001	[34, 35, 36]
11101011	[37, 38, 39, 40, 41]
10100000	[42, 43, 44]
11111011	[45, 46, 47, 48, 49]
10010001	[50, 51, 52, 53]
11110011	[54, 55, 56, 57, 58, 59, 60, 61]
00100001	[62, 63, 64, 65, 66, 67, 68, 69, 70]
11010010	[71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81]
11000001	[82, 83, 84, 85, 86, 87]
01000011	[88, 89, 90]
01010011	[91, 92]
01000001	[93, 94]
00010110	[95, 96]
11000110	[97, 98, 99, 100]
01011011	[101]
01010111	[102]
11100010	[103]
00110010	[104]
01100010	[105, 106]
01011001	[107]
01100001	[108, 109, 110, 111, 112, 113, 114]
00100110	[115, 116, 117, 118, 119, 120, 121]
01000110	[122]
00010100	[123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135]
01011000	[136, 137, 138]
01110000	[139, 140, 141, 142, 143]
01010100	[144, 145, 146]
11111000	[147, 148, 149, 150, 151, 152, 153, 154]
01001100	[155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168]
00110101	[169]
01111110	[170, 171, 172, 173, 174, 175]
01001101	[176, 177, 178, 179, 180]
01101101	[181, 182, 183, 184, 185]
01111111	[186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197]
00111101	[198, 199, 200, 201, 202, 203, 204, 205, 206]
00111100	[207]
11111101	[208, 209]
11101101	[210, 211, 212, 213, 214, 215, 216]
11111111	[217, 218, 219, 220, 221, 222]
00101111	[223, 224, 225, 226, 227, 228]
00101110	[229, 230, 231]
00011110	[232]
10111111	[233, 234, 235, 236]
10001101	[237, 238]
00001101	[239, 240, 241, 242, 243, 244, 245, 246, 247, 248]

00101001	250	True
10011111	1	False
00011001	6	False
00011000	9	False
00001011	18	True
00101000	20	False
10101001	38	False
11101011	29	False
10100000	39	False
11111011	40	False
10010001	49	False
11110011	67	False
00100001	69	True
11010010	73	True
11000001	85	True
01000011	88	True
01010011	92	True
01000001	96	False
00010110	94	False
11000110	98	True
01011011	102	False
01010111	102	True
11100010	98	False
00110010	103	False
01100010	110	False
01011001	107	True
01100001	109	True
00100110	120	True
01000110	128	False
00010100	139	False
01011000	132	False
01110000	144	False
01010100	143	False
11111000	157	False
01001100	169	False
00110101	165	False
01111110	171	True
01001101	176	True
01101101	183	True
01111111	180	False
00111101	203	True
00111100	200	False
11111101	211	False
11101101	216	True
11111111	209	False
00101111	225	True
00101110	231	True
00011110	232	True
10111111	228	False
10001101	251	False
00001101	243	True

Number of codes used=51

Iteration=    110000 training nets give:
alice_loss.item()=0.02237866446375847	bob_loss.item()=0.1057901680469513

10011111	[0, 1, 2, 253, 254, 255]
10011000	[3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
10001010	[13, 14, 15, 16]
11101000	[17, 18, 19, 20, 21, 22, 23, 24, 25]
10100100	[26, 27, 28, 29, 30, 31, 32, 33]
10011001	[34]
10101001	[35, 36, 37, 38, 39, 40, 41]
10010101	[42, 43, 44]
10000110	[45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55]
00110001	[56, 57, 58, 59, 60, 61]
11110011	[62, 63, 64]
10010011	[65, 66, 67, 68, 69, 70]
11100011	[71, 72, 73]
11100111	[74, 75, 76]
11010111	[77, 78, 79, 80, 81, 82, 83, 84, 85, 86]
11001010	[87, 88, 89, 90, 91]
01000001	[92, 93, 94, 95, 96, 97, 98]
00010110	[99, 100]
01010111	[101, 102, 103, 104, 105]
01011001	[106, 107, 108, 109, 110, 111]
01100001	[112, 113]
01010001	[114, 115, 116, 117]
00100110	[118, 119, 120, 121, 122]
01100000	[123, 124, 125, 126, 127, 128, 129]
01010000	[130, 131, 132, 133]
01010110	[134, 135]
00110110	[136, 137, 138, 139, 140, 141, 142, 143, 144, 145]
01011111	[146, 147, 148]
11111000	[149, 150, 151, 152, 153, 154, 155]
01001111	[156, 157, 158, 159, 160, 161]
01011100	[162, 163, 164, 165, 166]
01001100	[167, 168, 169, 170, 171, 172, 173]
01101110	[174, 175, 176, 177]
01001101	[178, 179]
01101101	[180, 181, 182, 183, 184, 185, 186, 187]
01101111	[188, 189, 190, 191, 192]
00111110	[193, 194]
11111110	[195, 196]
00111101	[197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212]
00011101	[213, 214, 215, 216, 217, 218, 219, 220]
10111110	[221, 222, 223, 224]
00001110	[225, 226, 227, 228, 229, 230, 231]
00111111	[232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245]
10101100	[246, 247, 248, 249, 250, 251]
10001000	[252]

10011111	2	True
10011000	10	True
10001010	9	False
11101000	17	True
10100100	24	False
10011001	30	False
10101001	36	True
10010101	45	False
10000110	51	True
00110001	58	True
11110011	71	False
10010011	67	True
11100011	80	False
11100111	74	True
11010111	84	True
11001010	81	False
01000001	94	True
00010110	95	False
01010111	105	True
01011001	109	True
01100001	107	False
01010001	111	False
00100110	122	True
01100000	122	False
01010000	130	True
01010110	132	False
00110110	147	False
01011111	145	False
11111000	139	False
01001111	158	True
01011100	167	False
01001100	171	True
01101110	177	True
01001101	177	False
01101101	175	False
01101111	194	False
00111110	195	False
11111110	197	False
00111101	203	True
00011101	213	True
10111110	225	False
00001110	231	True
00111111	235	True
10101100	248	True
10001000	1	False

Number of codes used=45

Iteration=    120000 training nets give:
alice_loss.item()=0.009363694116473198	bob_loss.item()=0.2766038477420807

10011110	[0, 1, 2, 250, 251, 252, 253, 254, 255]
00001001	[3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
00111011	[13]
00011010	[14, 15, 16, 17]
00001010	[18, 19, 20]
11101000	[21, 22, 23]
10011001	[24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
10000101	[36, 37, 38, 39, 40, 41, 42, 43, 44]
10010001	[45, 46, 47, 48, 49, 50, 51, 52, 53]
11011001	[54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70]
00000111	[71, 72, 73, 74, 75, 76]
00010011	[77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87]
11110111	[88, 89]
11000101	[90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]
11010110	[101, 102]
01010111	[103, 104, 105, 106]
11100101	[107, 108, 109, 110, 111, 112]
01010001	[113, 114, 115, 116, 117]
01100111	[118, 119, 120, 121, 122, 123]
00100110	[124, 125]
11110110	[126, 127, 128, 129, 130, 131, 132, 133, 134]
01011000	[135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146]
01100101	[147, 148, 149, 150, 151]
00110100	[152, 153, 154]
11110100	[155]
01001110	[156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166]
01011100	[167, 168, 169]
01111110	[170, 171, 172, 173, 174, 175, 176, 177]
01101101	[178, 179, 180, 181, 182, 183, 184, 185, 186]
01111101	[187, 188, 189, 190, 191, 192]
00111110	[193]
11111110	[194, 195, 196, 197, 198]
11111100	[199, 200, 201, 202, 203]
11111101	[204, 205, 206, 207, 208]
11101101	[209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219]
10111110	[220, 221, 222, 223, 224]
00101100	[225, 226]
00101110	[227, 228, 229, 230]
11101100	[231, 232, 234]
00011110	[233]
00111111	[235, 236]
10011101	[237, 238, 239, 240, 241, 242]
00001100	[243, 244]
00111001	[245, 246, 247, 248, 249]

10011110	245	False
00001001	6	True
00111011	16	False
00011010	11	False
00001010	17	False
11101000	25	False
10011001	28	True
10000101	52	False
10010001	53	True
11011001	69	True
00000111	67	False
00010011	87	True
11110111	86	False
11000101	98	True
11010110	103	False
01010111	107	False
11100101	108	True
01010001	106	False
01100111	120	True
00100110	125	True
11110110	133	True
01011000	143	True
01100101	138	False
00110100	145	False
11110100	157	False
01001110	162	True
01011100	168	True
01111110	174	True
01101101	184	True
01111101	191	True
00111110	190	False
11111110	205	False
11111100	203	True
11111101	204	True
11101101	213	True
10111110	223	True
00101100	228	False
00101110	222	False
11101100	234	True
00011110	245	False
00111111	234	False
10011101	241	True
00001100	248	False
00111001	7	False

Number of codes used=44

Iteration=    130000 training nets give:
alice_loss.item()=0.0029231319203972816	bob_loss.item()=0.356677770614624

00011111	[0, 1, 2, 3, 4, 5, 6, 251, 252, 253, 254, 255]
00001011	[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]
10010100	[19, 20, 21, 22, 23, 24]
10100100	[25, 26]
10101010	[27, 28, 29, 30, 31, 32, 33, 34, 35]
10101011	[36]
10010101	[37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]
10000111	[51, 52, 53, 54]
10010110	[55]
10000000	[56, 57]
10110111	[58, 59, 60, 61, 62]
11011001	[63, 64, 65, 66, 67, 68, 69, 70, 71]
00110011	[72, 73, 74, 75, 76]
00000010	[77, 78, 79]
11010001	[80, 81]
11010111	[82, 83, 84, 85, 86, 87]
11000000	[88, 89, 90, 91, 92]
01000111	[93, 94, 95, 96, 97, 98, 99, 100]
01001010	[101, 102]
00010000	[103, 104, 105, 106, 107, 108, 109, 110]
01010001	[111, 112]
11011000	[113]
11100000	[114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
11010100	[128, 129, 130]
01010000	[131, 132, 133]
11110000	[134]
00100100	[135, 136]
00110110	[137, 138, 139, 140, 141, 142, 143, 144]
01011000	[145]
01110100	[146, 147, 148, 149]
01110101	[150, 151, 152, 153]
00110101	[154, 155]
01101000	[156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168]
01011100	[169]
01111110	[170, 171, 172, 173, 174, 175]
01001101	[176, 177]
01111100	[178, 179, 180, 181, 182, 183, 184, 185, 186]
01101111	[187, 188, 189, 190, 191]
01111101	[192, 193, 194, 195]
00111100	[196, 197, 198, 199]
11111100	[200, 201, 202, 203, 204, 205, 206, 207]
11111111	[208, 209, 210, 211, 212, 213]
00101101	[214, 215, 216, 217, 218]
10111110	[219, 220, 221, 222]
00101100	[223, 224, 225, 226, 227, 228, 229]
10111100	[230]
11101100	[231]
00111111	[232, 233, 234, 235, 236]
00001110	[237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248]
11001110	[249, 250]

00011111	3	True
00001011	7	True
10010100	19	True
10100100	23	False
10101010	37	False
10101011	36	True
10010101	42	True
10000111	54	True
10010110	56	False
10000000	58	False
10110111	62	True
11011001	70	True
00110011	82	False
00000010	79	True
11010001	79	False
11010111	79	False
11000000	92	True
01000111	93	True
01001010	100	False
00010000	107	True
01010001	111	True
11011000	114	False
11100000	117	True
11010100	127	False
01010000	132	True
11110000	133	False
00100100	137	False
00110110	139	True
01011000	145	True
01110100	152	False
01110101	152	True
00110101	149	False
01101000	167	True
01011100	169	True
01111110	172	True
01001101	181	False
01111100	176	False
01101111	193	False
01111101	188	False
00111100	195	False
11111100	205	True
11111111	213	True
00101101	215	True
10111110	223	False
00101100	225	True
10111100	222	False
11101100	231	True
00111111	236	True
00001110	241	True
11001110	244	False

Number of codes used=50

Iteration=    140000 training nets give:
alice_loss.item()=0.005659644957631826	bob_loss.item()=0.1924455314874649

00001000	[0, 1, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00011000	[2, 3]
10111001	[4, 5, 6, 7]
00011011	[8, 9, 10, 11, 12, 13, 14, 15, 16]
00101001	[17, 18, 19, 20, 21, 22, 23]
10011011	[24, 25, 26, 27]
00101011	[28, 29, 30, 31]
10000100	[32, 33]
10101011	[34, 35, 36]
10101001	[37, 38, 39, 40, 41]
11011111	[42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
10000110	[52, 53, 54, 55]
10010000	[56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67]
00110001	[68, 69, 70, 71, 72, 73]
00000101	[74, 75]
11110011	[76, 77, 78, 83, 84, 85, 86, 87]
11010010	[79, 80, 81, 82]
11000001	[88]
11110111	[89]
01010011	[90, 91, 92, 93, 94, 95, 96, 97, 98]
11010110	[99, 100, 101, 102]
01010001	[103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113]
01000101	[114, 115, 116]
01100111	[117, 118, 119, 120, 121, 122, 123, 124]
01110001	[125, 126, 127]
11010100	[128, 129, 130]
00100000	[131, 132, 133, 134, 135]
00110110	[136, 137, 138, 139, 140, 141]
01010100	[142, 143, 144, 145, 146, 147, 148, 149]
01111010	[150]
01110101	[151, 152, 153]
01001000	[154, 155, 156, 157, 158, 159, 160]
01101000	[161, 162, 163]
01101001	[164, 165, 166, 167, 168]
01011100	[169, 170, 171, 172, 173, 174, 175]
01101101	[176, 177, 178, 179]
01111111	[180, 181, 182, 183, 184]
00111110	[185, 186, 187, 188, 189, 190, 191, 192, 193, 194]
00111100	[195, 196]
00111101	[197, 198, 199, 200]
11111100	[201, 202, 203, 204, 205, 206, 207, 208, 209]
11101101	[210, 211, 212, 213]
10111101	[214, 215, 216, 217, 218]
11011101	[219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231]
10011101	[232, 233, 234, 235, 236, 237, 238, 239, 240, 241]
00001110	[242, 243, 244, 245, 246]

00001000	253	True
00011000	5	False
10111001	2	False
00011011	12	True
00101001	22	True
10011011	25	True
00101011	30	True
10000100	23	False
10101011	34	True
10101001	43	False
11011111	2	False
10000110	50	False
10010000	67	True
00110001	66	False
00000101	77	False
11110011	81	False
11010010	83	False
11000001	83	False
11110111	85	False
01010011	95	True
11010110	101	True
01010001	100	False
01000101	115	True
01100111	124	True
01110001	121	False
11010100	134	False
00100000	133	True
00110110	141	True
01010100	143	True
01111010	151	False
01110101	147	False
01001000	153	False
01101000	164	False
01101001	165	True
01011100	173	True
01101101	183	False
01111111	182	True
00111110	191	True
00111100	197	False
00111101	199	True
11111100	211	False
11101101	208	False
10111101	215	True
11011101	229	True
10011101	234	True
00001110	240	False

Number of codes used=46

Iteration=    150000 training nets give:
alice_loss.item()=0.006195216905325651	bob_loss.item()=0.2716028094291687

10011111	[0, 1, 2, 3, 4, 250, 251, 252, 253, 254, 255]
00111000	[5]
00001001	[6]
00111010	[7, 8, 9, 10, 11, 12, 13, 14]
11101001	[15, 16, 17]
00011010	[18, 19, 20, 21, 22, 23]
10001001	[24, 25, 26, 27, 28, 29, 30, 31, 32, 33]
11101011	[34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]
10100011	[49, 50]
10000010	[51, 52, 53, 54, 55, 56]
10110000	[57, 58, 59, 60, 61, 62, 63]
10110111	[64]
00100011	[65, 66, 67]
11011011	[68, 69, 70, 71, 72, 73]
11100011	[74]
11010001	[75, 76, 77, 78, 79, 80, 81]
00110011	[82, 83, 84]
11010010	[85, 86, 87, 88]
11000110	[89, 90, 91, 92, 93, 94]
01110011	[95, 96, 97, 98, 99]
11010110	[100, 101, 102]
01010111	[103]
11001000	[104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115]
01100010	[116, 117, 118]
11010101	[119, 120, 121, 122]
01000110	[123]
01010000	[124, 125, 126, 127, 128, 129, 130]
11010100	[131]
01010101	[132, 133, 134, 135, 136, 137, 138, 139]
01101010	[140, 141, 142, 143]
01010100	[144, 145, 146, 147]
11110100	[148, 149, 150, 151, 152, 153, 154, 155, 156]
01001000	[157, 158]
01011110	[159, 160, 161, 162, 163, 164, 165, 166, 167]
01101110	[168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184]
01011101	[185, 186]
00111111	[187, 188, 189, 190, 191, 192]
00111101	[193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204]
11111100	[205]
11011110	[206, 207, 208, 209, 210, 211]
00011101	[212, 213, 214, 215, 216, 217]
00101110	[218, 219, 220, 221, 222]
10111110	[223]
10111100	[224, 225, 226]
11101111	[227, 228, 229, 230]
10101111	[231, 232, 233, 234, 235]
10101110	[236, 237, 238]
00001110	[239, 240, 241, 242, 243, 244]
10101100	[245, 246, 247, 248, 249]

10011111	252	True
00111000	8	False
00001001	12	False
00111010	18	False
11101001	11	False
00011010	23	True
10001001	30	True
11101011	35	True
10100011	50	True
10000010	53	True
10110000	59	True
10110111	62	False
00100011	68	False
11011011	69	True
11100011	90	False
11010001	78	True
00110011	84	True
11010010	82	False
11000110	95	False
01110011	94	False
11010110	99	False
01010111	103	True
11001000	108	True
01100010	122	False
11010101	116	False
01000110	121	False
01010000	124	True
11010100	133	False
01010101	143	False
01101010	139	False
01010100	141	False
11110100	142	False
01001000	160	False
01011110	165	True
01101110	170	True
01011101	181	False
00111111	186	False
00111101	200	True
11111100	211	False
11011110	209	True
00011101	214	True
00101110	218	True
10111110	215	False
10111100	221	False
11101111	228	True
10101111	235	True
10101110	237	True
00001110	239	True
10101100	250	False

Number of codes used=49

Iteration=    160000 training nets give:
alice_loss.item()=0.009831059724092484	bob_loss.item()=0.21810996532440186

00011111	[0, 1, 2, 254, 255]
10111001	[3, 4, 5, 6, 7, 8, 9, 10, 11]
10010100	[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]
10111010	[25, 26, 27, 28]
10101000	[29, 30, 31, 32, 33, 34]
10010101	[35]
11111011	[36, 37, 38, 39, 40]
10101001	[41, 42, 43, 44]
10010001	[45, 46]
10100001	[47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
10110000	[62, 63]
10110011	[64, 66, 67, 68, 69]
00100001	[65]
10010010	[70, 71]
00000011	[72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82]
11010011	[83, 84, 85, 86, 87, 88, 89, 90]
01110011	[91, 92, 93, 94, 95]
01010011	[96, 97, 98]
00000110	[99, 100, 101, 102]
00010110	[103]
11011000	[104, 105, 106, 107, 108, 109, 110]
01001011	[111, 112]
01000101	[113]
11010101	[114, 115, 116]
01011010	[117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128]
11010100	[129, 130]
11110110	[131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142]
01101010	[143, 144, 145, 146, 147, 148, 149, 150]
01111010	[151]
01111001	[152, 153, 154, 155, 156]
00100101	[157, 158, 159, 160]
01101001	[161, 162, 163, 164, 165]
01011110	[166]
01101110	[167, 168, 169, 170, 171]
01011100	[172, 173, 174, 175, 176, 177]
01101101	[178, 179, 180, 181, 182, 183, 184]
01111100	[185, 186, 187]
01111101	[188, 189, 190, 191, 192, 193, 194]
00111100	[195]
00111101	[196, 197, 198, 199, 200, 201]
11011110	[202, 203, 204, 205, 206, 207, 208, 209, 210]
11111111	[211, 212, 213, 214, 215, 216]
00101101	[217]
10111110	[218, 219, 220, 221, 222, 223, 224, 225]
11101111	[226, 227, 228]
10011101	[229, 230, 231, 232, 233, 234, 235]
11011100	[236]
10101110	[237, 238, 239, 240, 241, 242]
10011110	[243, 244, 245]
10001111	[246, 247]
10101100	[248, 249, 250, 251, 252, 253]

00011111	3	False
10111001	6	True
10010100	21	True
10111010	24	False
10101000	25	False
10010101	33	False
11111011	41	False
10101001	39	False
10010001	49	False
10100001	41	False
10110000	59	False
10110011	70	False
00100001	72	False
10010010	71	True
00000011	72	True
11010011	84	True
01110011	96	False
01010011	95	False
00000110	99	True
00010110	119	False
11011000	107	True
01001011	106	False
01000101	116	False
11010101	117	False
01011010	124	True
11010100	130	True
11110110	142	True
01101010	148	True
01111010	151	True
01111001	156	True
00100101	152	False
01101001	165	True
01011110	170	False
01101110	175	False
01011100	178	False
01101101	183	True
01111100	188	False
01111101	185	False
00111100	198	False
00111101	201	True
11011110	206	True
11111111	212	True
00101101	219	False
10111110	219	True
11101111	221	False
10011101	228	False
11011100	234	False
10101110	239	True
10011110	250	False
10001111	246	True
10101100	249	True

Number of codes used=51

Iteration=    170000 training nets give:
alice_loss.item()=0.004457531031221151	bob_loss.item()=0.24797168374061584

10001110	[0, 250, 251, 252, 253, 254, 255]
00011000	[1, 2, 3, 4, 5]
00111000	[6, 7, 8, 9, 10, 11, 12, 13]
11111001	[14, 15, 16, 17, 18, 19, 20, 21]
10111010	[22, 23, 24, 25, 26, 27, 28, 29, 30]
11101011	[31, 32, 33, 34, 35, 36]
11111011	[37, 38, 39, 40, 41, 42, 43, 44]
10010001	[45, 46, 47, 48]
10100011	[49, 50, 51, 52, 53, 54]
10000111	[55]
10010110	[56, 57, 58]
00010001	[59, 60]
10110010	[61]
10110111	[62, 63, 64, 65, 66, 67, 68]
10110011	[69]
10010010	[70, 71, 72, 73, 74, 75]
11000011	[76]
11011010	[77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87]
11110111	[88, 89]
11000111	[90, 91, 92, 93, 94, 95, 96]
01010010	[97, 98, 99, 100, 101, 102]
01001010	[103, 104, 105]
11011000	[106]
01011011	[107, 108, 109, 110, 111, 112, 113, 114]
01100001	[115, 116, 117]
01110001	[118]
11100110	[119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129]
00100000	[130, 131, 132, 133]
01100110	[134]
00010100	[135, 136, 137, 138, 139]
11110100	[140, 141, 142, 143, 144, 145, 146, 147]
01101000	[148, 149, 150, 151, 152, 153, 154]
01111001	[155, 156, 157, 158, 159, 160]
01111000	[161, 162, 163]
01101100	[164, 165, 166, 167, 168, 169, 170, 171]
01011111	[172, 173, 174, 175]
01111111	[176, 177, 178]
01011100	[179, 180, 181, 182, 183, 184]
01101101	[185]
00111111	[186]
01111101	[187, 188, 189, 190]
00111110	[191, 192]
00111100	[193, 194, 195]
00111101	[196, 197, 198, 199, 200, 201, 202, 203]
00011100	[204, 205, 206, 207]
00011110	[208, 209]
11111100	[210, 211]
10111101	[212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222]
11101100	[223, 224, 225]
10011101	[226, 227, 228, 229]
10001101	[230, 231, 232, 233, 234, 235, 236, 237]
10001100	[238, 239, 240, 241, 242, 243]
10011110	[244, 245, 246]
10101100	[247, 248, 249]

10001110	253	True
00011000	0	False
00111000	7	True
11111001	22	False
10111010	27	True
11101011	26	False
11111011	41	True
10010001	48	True
10100011	54	True
10000111	49	False
10010110	56	True
00010001	59	True
10110010	62	False
10110111	66	True
10110011	70	False
10010010	75	True
11000011	81	False
11011010	83	True
11110111	88	True
11000111	94	True
01010010	98	True
01001010	105	True
11011000	107	False
01011011	114	True
01100001	115	True
01110001	118	True
11100110	125	True
00100000	134	False
01100110	132	False
00010100	136	True
11110100	145	True
01101000	164	False
01111001	158	True
01111000	165	False
01101100	168	True
01011111	178	False
01111111	180	False
01011100	186	False
01101101	174	False
00111111	190	False
01111101	187	True
00111110	191	True
00111100	196	False
00111101	204	False
00011100	206	True
00011110	208	True
11111100	208	False
10111101	219	True
11101100	224	True
10011101	227	True
10001101	229	False
10001100	243	True
10011110	249	False
10101100	248	True

Number of codes used=54

Iteration=    180000 training nets give:
alice_loss.item()=0.04325651749968529	bob_loss.item()=0.30427175760269165

00011001	[0, 1, 2, 3, 252, 253, 254, 255]
10111001	[4, 5]
11101001	[6, 7, 8, 9, 10, 11, 12, 13, 14]
00101000	[15, 16]
11101010	[17, 18, 19]
10101000	[20, 21, 22, 23, 24, 25]
11101011	[26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]
10101001	[38, 39, 40]
10010101	[41, 42, 43, 44, 45]
10010001	[46, 47]
10000110	[48, 49, 50, 51, 52, 53, 54]
10010110	[55, 56, 57, 58, 59]
10110010	[60, 61]
10110111	[62, 63, 64, 65]
10010011	[66, 67, 68, 69, 70, 71]
11011011	[72, 73, 74, 75, 76]
11010001	[77, 78, 79, 80, 81, 82, 83, 84, 85, 86]
11010011	[87]
11000101	[88, 89, 90, 91]
11100111	[92, 93, 94, 95, 96]
11001010	[97, 98, 99, 100, 101, 102, 103, 104]
01001010	[105, 106]
00010000	[107, 108, 109, 110, 111, 112]
01100001	[113, 114, 115, 116]
01011001	[117, 118]
01000110	[119, 120, 121]
00100110	[122, 123]
11010100	[124, 125, 126, 127, 128, 129, 130, 131]
11110000	[132, 133, 134, 135, 136, 137, 138, 139, 140]
01100101	[141]
11110110	[142]
11110100	[143]
00100100	[144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161]
01111000	[162, 163, 164]
01101001	[165, 166, 167]
01101110	[168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180]
01101101	[181, 182, 183, 184, 185]
01111101	[186, 187]
00111111	[188, 189, 190, 191, 192]
00111100	[193, 194, 195, 196, 197, 198, 199, 200]
00011100	[201, 202, 203, 204, 205, 206, 207, 208, 209]
11111100	[210]
00101100	[211, 212, 213, 214, 215, 216, 217]
10111101	[218, 219, 220]
11101110	[221, 222, 223, 224]
00101111	[225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235]
10011100	[236, 237, 238, 239, 240, 241]
10011110	[242, 243, 244, 245, 246, 247]
10101100	[248, 249, 250, 251]

00011001	255	True
10111001	3	False
11101001	7	True
00101000	16	True
11101010	18	True
10101000	24	True
11101011	28	True
10101001	31	False
10010101	48	False
10010001	51	False
10000110	54	True
10010110	64	False
10110010	63	False
10110111	65	True
10010011	69	True
11011011	78	False
11010001	79	True
11010011	90	False
11000101	87	False
11100111	95	True
11001010	100	True
01001010	105	True
00010000	111	True
01100001	116	True
01011001	115	False
01000110	123	False
00100110	122	True
11010100	128	True
11110000	133	True
01100101	135	False
11110110	137	False
11110100	145	False
00100100	156	True
01111000	166	False
01101001	168	False
01101110	170	True
01101101	176	False
01111101	186	True
00111111	193	False
00111100	195	True
00011100	208	True
11111100	208	False
00101100	213	True
10111101	216	False
11101110	227	False
00101111	233	True
10011100	236	True
10011110	242	True
10101100	246	False

Number of codes used=49

Iteration=    190000 training nets give:
alice_loss.item()=0.00041457131737843156	bob_loss.item()=0.007164605427533388

00011001	[0, 1, 2, 253, 254, 255]
10011000	[3]
10111001	[4, 5, 6, 7, 8, 9]
11101001	[10, 11, 12, 13]
00011011	[14, 15]
10011010	[16, 17, 18, 19, 20]
11111001	[21, 22, 23, 24]
10001010	[25]
11111010	[26, 27, 28]
10001001	[29, 30]
10101011	[31, 32]
10001011	[33, 34, 35]
10111011	[36, 37, 38]
11111011	[39, 40, 41, 42, 43]
01101011	[44, 45]
10010001	[46, 47, 48, 49]
10000110	[50, 51, 52]
10100011	[53, 54, 55]
10100010	[56, 57, 58]
10100111	[59]
00010001	[60, 61]
00110001	[62, 63, 64, 65]
10010011	[66, 67, 68, 69, 70, 71, 72]
11011011	[73, 74, 75, 76, 77]
11000011	[78, 79, 80, 81]
11000010	[82, 83, 84]
11010111	[85]
01000011	[86, 87, 88]
11110111	[89, 90, 91, 92, 93, 94]
01110011	[95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108]
01001011	[109, 110]
01100001	[111, 112, 113, 114, 115]
11010110	[116, 117]
01011001	[118, 119, 120]
00100110	[121, 122, 123]
11100000	[124, 125, 126, 127]
01010101	[128]
00100000	[129, 130, 131, 132]
01010100	[133, 134, 135, 136, 137]
00110111	[138, 139, 140]
00110110	[141]
11000100	[142, 143, 144, 145]
01110101	[146, 147, 148, 149]
00110100	[150]
01001110	[151, 152, 153, 154]
01111001	[155, 156]
11111000	[157, 158, 159, 160, 161]
01111000	[162, 163]
01101001	[164, 165, 166, 167, 168, 169, 170, 171]
01011101	[172, 173]
01011100	[174, 175, 176, 177, 178, 179, 180, 181, 182, 183]
01111101	[184, 185, 186, 187, 188]
00111110	[189, 190, 191, 192, 193]
00111100	[194, 195, 196, 197, 198]
01101111	[199, 200, 201]
00011100	[202, 203, 204, 205, 206]
00011110	[207, 208]
11111110	[209, 210, 211]
10111111	[212]
11111111	[213, 214]
10111110	[215, 216, 217, 218, 219, 220, 221]
11001111	[222, 223, 224, 225, 226, 227]
10011101	[228]
00101111	[229, 230, 231, 232, 233]
10101101	[234]
11001101	[235, 236, 237]
10101110	[238, 239, 240, 241, 242]
10011111	[243, 244, 245, 246, 247, 248]
11001100	[249, 250, 251]
00011000	[252]

00011001	254	True
10011000	255	False
10111001	1	False
11101001	12	True
00011011	18	False
10011010	17	True
11111001	23	True
10001010	25	True
11111010	29	False
10001001	30	True
10101011	31	True
10001011	38	False
10111011	34	False
11111011	42	True
01101011	42	False
10010001	52	False
10000110	55	False
10100011	55	True
10100010	54	False
10100111	57	False
00010001	67	False
00110001	65	True
10010011	69	True
11011011	80	False
11000011	82	False
11000010	84	True
11010111	86	False
01000011	91	False
11110111	90	True
01110011	96	True
01001011	106	False
01100001	115	True
11010110	120	False
01011001	116	False
00100110	125	False
11100000	130	False
01010101	135	False
00100000	130	True
01010100	136	True
00110111	141	False
00110110	146	False
11000100	143	True
01110101	149	True
00110100	151	False
01001110	157	False
01111001	155	True
11111000	160	True
01111000	161	False
01101001	168	True
01011101	174	False
01011100	181	True
01111101	183	False
00111110	191	True
00111100	193	False
01101111	197	False
00011100	204	True
00011110	207	True
11111110	206	False
10111111	208	False
11111111	215	False
10111110	219	True
11001111	223	True
10011101	211	False
00101111	232	True
10101101	228	False
11001101	229	False
10101110	245	False
10011111	219	False
11001100	242	False
00011000	253	False

Number of codes used=70

Iteration=    200000 training nets give:
alice_loss.item()=0.00015720725059509277	bob_loss.item()=0.0002610915689729154

00011001	[0, 1, 252, 253, 254, 255]
10011000	[2, 3, 4]
00111000	[5, 6, 7]
11101001	[8, 9, 10, 11, 12, 13]
10011011	[14, 15, 16, 17]
10011010	[18, 19, 20, 21]
10001010	[22, 23, 24, 25, 26]
11101011	[27, 28, 29, 30, 31, 32, 33]
10101001	[34]
10111011	[35, 36]
10101010	[37, 38]
11111011	[39, 40]
01101011	[41, 42, 43, 44, 45, 46]
10100000	[47, 48, 49, 50, 51]
10100011	[52]
10000001	[53, 54, 55, 56]
10100010	[57, 58]
10000010	[59, 60, 61]
00000001	[62, 63, 64]
10110111	[65, 66]
10010011	[67, 68, 69, 70, 71, 72]
11100001	[73, 74]
11000011	[75, 76, 77, 78, 79, 80]
11000010	[81, 82, 83]
11010111	[84, 85, 86, 87]
11000101	[88, 89, 90]
11100010	[91, 92, 93]
01010011	[94, 95, 96, 97]
11100111	[98, 99]
11001010	[100]
01010111	[101, 102, 103, 104, 105, 106]
01001010	[107, 108]
00010000	[109, 110]
01100001	[111, 112, 113, 114, 115]
00100010	[116, 117]
11010110	[118, 119, 120, 121]
00100110	[122, 123, 124, 125]
01110110	[126, 127]
00100000	[128, 129, 130, 131, 132]
01010100	[133, 134]
01100100	[135, 136, 137]
11110110	[138, 139]
11000100	[140, 141, 142, 143, 144]
00010101	[145, 146]
01110101	[147, 148, 149]
01110100	[150, 151, 152, 153]
01111001	[154, 155, 156, 157]
11111000	[158, 159, 160, 161]
01111000	[162, 163, 164, 165, 166]
01101001	[167, 168, 169, 170, 171]
01011101	[172, 173, 174]
01011110	[175, 176, 177, 178, 179]
01111110	[180, 181, 182, 183, 184]
01111101	[185, 186, 187, 188]
00111110	[189, 190, 191, 192]
00111100	[193, 194, 195, 196]
01101111	[197, 198, 199, 200, 201, 202]
00011110	[203, 204, 205, 206, 207, 208]
10111111	[209, 210, 211, 212]
11111111	[213, 214, 215, 216]
10111110	[217, 218, 219, 220]
11001111	[221, 222, 223, 224, 225, 226, 227, 228]
00101111	[229, 230, 231, 232, 233, 234, 235]
10011100	[236]
00001101	[237, 238, 239, 240, 241, 242]
00001100	[243, 244, 245, 246, 247, 248]
10001110	[249, 250]
00011000	[251]

00011001	254	True
10011000	0	False
00111000	8	False
11101001	10	True
10011011	15	True
10011010	21	True
10001010	25	True
11101011	31	True
10101001	31	False
10111011	40	False
10101010	38	True
11111011	41	False
01101011	41	True
10100000	51	True
10100011	56	False
10000001	59	False
10100010	52	False
10000010	54	False
00000001	68	False
10110111	64	False
10010011	70	True
11100001	69	False
11000011	81	False
11000010	83	True
11010111	87	True
11000101	96	False
11100010	93	True
01010011	97	True
11100111	99	True
11001010	107	False
01010111	103	True
01001010	107	True
00010000	113	False
01100001	116	False
00100010	118	False
11010110	118	True
00100110	124	True
01110110	132	False
00100000	133	False
01010100	137	False
01100100	138	False
11110110	139	True
11000100	141	True
00010101	143	False
01110101	151	False
01110100	154	False
01111001	155	True
11111000	158	True
01111000	162	True
01101001	169	True
01011101	179	False
01011110	180	False
01111110	183	True
01111101	185	True
00111110	186	False
00111100	196	True
01101111	197	True
00011110	206	True
10111111	209	True
11111111	217	False
10111110	220	True
11001111	223	True
00101111	228	False
10011100	234	False
00001101	241	True
00001100	245	True
10001110	249	True
00011000	249	False

Number of codes used=68


End of hp run 1.  Result of run:
[(-0.9952320869258041, 200000), ('21-05-13_09:05:41BST_NLearn_model_1_Alice_iter200000', '21-05-13_09:05:41BST_NLearn_model_1_Bob_iter200000')]
(-0.9952320869258041, 200000)



Time taken over all 1 given sets of hyperparameters=4:19:33, averaging 4:19:33 per run


 ---- Table of results ----

 code  hp_run  result
    0       1  (-0.995, 200000)
 --------------------------

++++ Best result was (-0.995, 200000) on hp_run=1 with
hyperparameters = {
	'N_ITERATIONS': 200000,
	'RANDOM_SEEDS': (714844, 936892, 888616, 165835),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
	'n_rng': Generator(PCG64)
	'ne_rng': Generator(PCG64)
	't_rng': <torch._C.Generator object at 0x7f0200cc1cf0>
	'te_rng': <torch._C.Generator object at 0x7f0200cc1970>
}


End closed log for run 21-05-13_09:05:41BST
