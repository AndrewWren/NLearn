The closed log for run 21-05-13_14:17:04BST

SMOOTHING_LENGTH = 10000
SAVE_PERIOD = 100000
CODE_BOOK_PERIOD = 10000
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # Used: "cuda"
MODEL_FOLDER = 'models'
CONFIGS_FOLDER = 'configs'
LOGS_FOLDER = 'logs'

hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': [(714844, 936892, 888616, 165835)],
	'ALICE_NET': ['FFs(3, 50)', 'MaxNet("In", 3, 50, relu=True)', 'MaxNet("In", 3, 50, bias_included=True, relu=True)'],
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}



>>>> hp_run=1 of 3
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (714844, 936892, 888616, 165835),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=     20000 training nets give:
alice_loss.item()=0.48465365171432495	bob_loss.item()=0.4868663549423218

10110111	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]

10110111	51	True

Number of codes used=1

Iteration=     30000 training nets give:
alice_loss.item()=0.08064199239015579	bob_loss.item()=0.44837111234664917

11000011	[0, 1, 2, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10110111	[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75]
10010001	[76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153]
01111001	[154, 155]

11000011	205	True
10110111	73	True
10010001	97	True
01111001	88	False

Number of codes used=4

Iteration=     40000 training nets give:
alice_loss.item()=0.12230836600065231	bob_loss.item()=0.20870615541934967

01001011	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
10010110	[13, 14, 15, 16, 17, 18]
10110111	[19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]
00110111	[75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]
10110110	[90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104]
10010001	[105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129]
11101011	[130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152]
11100011	[153, 154, 155]
11001011	[156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189]
11000010	[190, 191, 192, 193, 194, 195, 196, 197, 198, 199]
11000011	[200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251]
11001010	[252, 253, 254, 255]

01001011	187	False
10010110	75	False
10110111	66	True
00110111	77	True
10110110	81	False
10010001	88	False
11101011	198	False
11100011	185	False
11001011	191	False
11000010	213	False
11000011	203	True
11001010	205	False

Number of codes used=12

Iteration=     50000 training nets give:
alice_loss.item()=0.05358793959021568	bob_loss.item()=0.19303813576698303

11100010	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01001111	[22]
11110100	[23, 24]
10110111	[25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68]
00110111	[69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]
11010000	[80, 81, 82, 83, 84, 85, 86, 87, 88, 89]
01111000	[90, 91, 92, 93, 94, 95, 96, 97]
10011011	[98, 99, 100, 101]
11010001	[102, 103, 104, 105]
10001001	[106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
00001001	[144, 145, 146]
10001011	[147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180]
11010011	[181]
11100011	[182, 183, 184, 185, 186, 187, 188, 189]
11101011	[190, 191, 192, 193, 194, 195, 196, 197, 198, 199]
11101001	[200, 201, 202, 203, 204, 205]
11000010	[206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228]

11100010	230	True
01001111	209	False
11110100	65	False
10110111	51	True
00110111	60	False
11010000	83	True
01111000	85	False
10011011	99	True
11010001	97	False
10001001	113	True
00001001	113	False
10001011	179	True
11010011	216	False
11100011	218	False
11101011	194	True
11101001	195	False
11000010	240	False

Number of codes used=17

Iteration=     60000 training nets give:
alice_loss.item()=0.0111883245408535	bob_loss.item()=0.09047955274581909

01000100	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10110111	[25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
00110101	[64, 65, 66, 67, 68, 69, 70, 71]
10010111	[72, 73, 74, 75, 76, 77, 78, 79]
10111111	[80, 81, 82, 83, 84, 85, 86]
00011101	[87, 88, 89, 90, 91, 92]
10000101	[93, 94]
00010001	[95, 96, 97, 98]
10011011	[99, 100, 101, 102]
10101001	[103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
10001001	[115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146]
10001011	[147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189]
11001011	[190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200]
11000011	[201, 202, 203, 204, 205]
11001111	[206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216]
11001110	[217, 218, 219]
11100010	[220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242]

01000100	248	True
10110111	56	True
00110101	69	True
10010111	69	False
10111111	75	False
00011101	91	True
10000101	87	False
00010001	107	False
10011011	109	False
10101001	107	True
10001001	121	True
10001011	174	True
11001011	191	True
11000011	215	False
11001111	209	True
11001110	229	False
11100010	235	True

Number of codes used=17

Iteration=     70000 training nets give:
alice_loss.item()=0.005926280282437801	bob_loss.item()=0.027784548699855804

01000100	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10110111	[24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60]
00110101	[61, 62, 63, 64, 65, 66, 67, 68]
10110100	[69, 70, 71, 72, 73]
10110101	[74]
11110101	[75, 76, 77, 78, 79, 80]
10110010	[81, 82, 83]
00011101	[84, 85, 86, 87, 88, 89]
00110001	[90, 91, 92, 93, 94, 95, 96]
10011000	[97]
01010001	[98, 99, 100, 101, 102, 103]
10101001	[104, 105, 106, 107, 108, 109, 110, 111, 112, 113]
10001001	[114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147]
10001011	[148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183]
11001011	[184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196]
11001001	[197, 198, 199, 200, 201, 202, 203]
01101011	[204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217]
10000010	[218, 219, 220, 221, 222, 223, 224, 225, 226]
11100010	[227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240]

01000100	247	True
10110111	53	True
00110101	69	False
10110100	69	True
10110101	69	False
11110101	73	False
10110010	74	False
00011101	92	False
00110001	89	False
10011000	101	False
01010001	103	True
10101001	103	False
10001001	123	True
10001011	170	True
11001011	188	True
11001001	196	False
01101011	213	True
10000010	217	False
11100010	236	True

Number of codes used=19


End of hp run 1.  Result of run:
[(-0.9579113567032395, 70000), ('21-05-13_14:17:04BST_NLearn_model_1_Alice_iter70000', '21-05-13_14:17:04BST_NLearn_model_1_Bob_iter70000')]
(-0.9579113567032395, 70000)


>>>> hp_run=2 of 3, time elapsed 1:24:58 of estimated 4:14:54, 
implying ending at 18:31:57BST on Thursday 13 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (714844, 936892, 888616, 165835),
	'ALICE_NET': 'MaxNet("In", 3, 50, relu=True)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=     20000 training nets give:
alice_loss.item()=0.6909523010253906	bob_loss.item()=0.593999981880188

11101010	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 218, 219, 220, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10110000	[86, 87, 88, 89, 90, 91]
01111011	[216, 217, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244]

11101010	104	True
10110000	87	True
01111011	13	False

Number of codes used=3

Iteration=     30000 training nets give:
alice_loss.item()=0.2144765555858612	bob_loss.item()=0.3701898455619812

01110000	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01100000	[13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
01100011	[62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91]
11101010	[92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142]
10100010	[143, 144, 145, 146, 147, 148, 149, 150]
00001010	[151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178]
10110000	[153]
00000110	[179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]
00010000	[230, 231, 232, 233, 234, 235, 236, 237, 238, 239]

01110000	255	True
01100000	37	True
01100011	76	True
11101010	111	True
10100010	195	False
00001010	204	False
10110000	239	False
00000110	201	True
00010000	225	False

Number of codes used=9

Iteration=     40000 training nets give:
alice_loss.item()=0.11403392255306244	bob_loss.item()=0.16001296043395996

01110000	[0, 1, 237, 238, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 254, 255]
01110100	[2, 10, 11, 19]
00110000	[3, 4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 252, 253]
01100000	[20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
01111011	[32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]
01100001	[49, 50, 51, 52, 53, 54]
01100010	[55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76]
01001010	[77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93]
11111010	[94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106]
11101010	[107, 108, 109, 110, 111]
11001010	[112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128]
10001110	[129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 155]
10001100	[148, 149, 150, 151, 152, 153, 154]
00101110	[156, 157, 158, 161, 162, 163, 164, 165, 166, 167]
10001010	[159, 160]
00001110	[168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192]
00000100	[193]
00000110	[194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212]
00010001	[213]
00010000	[214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 236]
00010100	[235, 239, 240, 241]

01110000	3	False
01110100	7	False
00110000	1	False
01100000	38	False
01111011	56	False
01100001	45	False
01100010	67	True
01001010	98	False
11111010	109	False
11101010	117	False
11001010	124	True
10001110	153	False
10001100	166	False
00101110	166	True
10001010	147	False
00001110	174	True
00000100	191	False
00000110	182	False
00010001	241	False
00010000	217	True
00010100	221	False

Number of codes used=21

Iteration=     50000 training nets give:
alice_loss.item()=0.11196593940258026	bob_loss.item()=0.10665354132652283

01010000	[0, 1, 2, 4, 5, 6, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00110010	[3]
01110001	[7, 8, 9]
00111000	[10, 11, 12, 13, 14, 23, 24]
01110010	[15, 16, 21, 22, 26]
01110011	[17, 18, 19, 20]
01111000	[25, 27, 28, 29, 30, 31]
01100000	[32, 33, 34, 35, 36, 37, 38]
01100001	[39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52]
01100101	[53, 54, 55, 56, 57, 58, 59, 60, 61]
01100010	[62, 63, 64, 65, 66, 68, 69, 70, 71, 72]
01100011	[67, 73, 74, 75, 76, 77]
01101011	[78, 79, 80, 81, 82, 83]
11110010	[84, 85]
11100010	[86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 108, 109, 110, 111]
01001010	[101, 102, 103, 104, 105, 106, 107]
11101010	[112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123]
11101011	[124, 125]
11001011	[126, 127, 128, 129, 130, 131, 132, 133]
10001000	[134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146]
10001100	[147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166]
01001100	[167, 168, 169]
00101111	[170, 171, 172, 173, 174, 175, 176]
00001110	[177, 178, 179, 180, 181, 182]
00001111	[183, 184, 185, 186, 187, 188]
00001100	[189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200]
10010000	[201, 202, 203, 204]
00011000	[205, 210, 211]
00000010	[206]
00010100	[207, 208, 209]
00010000	[212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]
00011001	[230, 231, 232, 233]
00110110	[234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245]

01010000	250	True
00110010	250	False
01110001	4	False
00111000	14	True
01110010	23	False
01110011	22	False
01111000	13	False
01100000	40	False
01100001	47	True
01100101	46	False
01100010	57	False
01100011	60	False
01101011	86	False
11110010	81	False
11100010	98	True
01001010	96	False
11101010	112	True
11101011	114	False
11001011	120	False
10001000	133	False
10001100	167	False
01001100	177	False
00101111	166	False
00001110	171	False
00001111	177	False
00001100	185	False
10010000	218	False
00011000	222	False
00000010	190	False
00010100	232	False
00010000	229	True
00011001	222	False
00110110	246	False

Number of codes used=33

Iteration=     60000 training nets give:
alice_loss.item()=0.010822176933288574	bob_loss.item()=0.09656653553247452

00110001	[0, 250, 251, 252]
00110100	[1, 2, 3, 253, 254, 255]
00110000	[4, 5, 6, 7, 8]
00111001	[9]
00111000	[10, 12, 13, 14, 15]
01110011	[11, 16, 17, 19, 20]
01110010	[18, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]
01000001	[33, 34, 35]
00100000	[36, 37, 38]
01100000	[39]
00100001	[40]
00101001	[41, 42, 43, 44, 45, 48, 49, 50, 51, 52, 53, 54]
01100101	[46]
01100001	[47]
01101000	[55, 56, 57]
01100111	[58, 59]
01000011	[60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73]
11100001	[74, 75, 76, 77, 78, 79, 83, 84]
01101011	[80, 81, 82, 85]
01101010	[86, 87, 88, 89, 90]
01001010	[91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101]
11100010	[102, 103, 104]
11011010	[105, 106, 107]
11101010	[108, 109, 113]
11101011	[110, 111, 112, 114, 115, 116, 117, 118, 120]
10111110	[119]
11001010	[121, 122, 123, 124]
10101011	[125, 126, 127]
11101111	[128, 129, 130, 131, 132, 133, 134]
10001000	[135, 136, 137, 138]
11001110	[139, 141, 142, 143]
11001111	[140]
10001010	[144, 145, 146, 147]
10011010	[148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161]
10001111	[162, 163, 164, 165, 172, 173, 174]
00101111	[166, 167, 168, 169]
00001110	[170, 171]
01001100	[175, 176, 177, 178, 179, 180]
00001100	[181, 182, 183, 184, 185, 187]
00001000	[186]
00001010	[188, 189, 190, 191, 192]
00000100	[193, 194]
00000010	[195, 196, 197, 198]
00000011	[199, 200, 201, 202, 204]
10010100	[203, 205, 206, 207]
00011000	[208, 212, 213, 214, 215, 216, 217, 218, 219]
01011100	[209, 210, 211]
00010011	[220, 221, 222, 223]
00011001	[224, 225, 226, 227, 228, 229]
00010001	[230, 231, 232, 233, 234, 235, 236]
00110110	[237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249]

00110001	251	True
00110100	247	False
00110000	254	False
00111001	3	False
00111000	10	True
01110011	20	True
01110010	19	False
01000001	35	True
00100000	31	False
01100000	38	False
00100001	43	False
00101001	55	False
01100101	44	False
01100001	45	False
01101000	48	False
01100111	65	False
01000011	65	True
11100001	79	True
01101011	82	True
01101010	85	False
01001010	93	True
11100010	102	True
11011010	104	False
11101010	113	True
11101011	117	True
10111110	135	False
11001010	125	False
10101011	121	False
11101111	126	False
10001000	138	True
11001110	143	True
11001111	132	False
10001010	147	True
10011010	153	True
10001111	167	False
00101111	169	True
00001110	173	False
01001100	181	False
00001100	198	False
00001000	179	False
00001010	187	False
00000100	195	False
00000010	193	False
00000011	205	False
10010100	212	False
00011000	210	False
01011100	201	False
00010011	219	False
00011001	223	False
00010001	226	False
00110110	243	True

Number of codes used=51

Iteration=     70000 training nets give:
alice_loss.item()=0.0009728693403303623	bob_loss.item()=0.012220589444041252

00110001	[0, 1, 2, 252, 253, 254, 255]
01110001	[3, 4, 5, 6, 7]
01110101	[8, 9]
00111000	[10]
01110010	[11, 12, 13, 14]
01110011	[15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]
01000001	[28, 29, 30, 31, 32, 33]
00100001	[34, 35]
01111001	[36]
01100000	[37, 38, 39, 40, 41]
01100101	[42, 43, 45]
01001001	[44]
01100001	[46, 47, 48, 49, 50, 51, 52, 53, 54]
00101001	[55, 56, 57, 58, 59, 60]
01000011	[61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]
00100011	[73, 74]
11100001	[75, 76, 77, 78, 79, 80, 81]
01101011	[82, 83, 84, 85, 86]
11111010	[87, 88, 89, 90]
01001010	[91, 92, 93, 94, 95, 96, 97, 98]
11100010	[99, 100, 101, 102, 103]
11011010	[104, 105, 106, 107, 108]
11101010	[109, 110, 114]
10101010	[111, 112, 113]
11101011	[115, 116, 117, 119]
11001010	[118, 120, 121, 122, 123, 124, 125, 126, 127, 128]
11101111	[129]
11001111	[130, 131, 132, 133, 134]
10001000	[135, 136, 137, 138, 139]
11001110	[140, 141, 142, 143, 144, 145]
10011010	[146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157]
10001110	[158, 159]
10001111	[160, 161, 162, 163, 164, 166, 167, 168, 169]
00001110	[165, 170, 171, 172, 173]
00001111	[174, 175, 176, 178, 179, 180, 181]
01001100	[177]
00001010	[182, 183, 186, 187, 188]
00001000	[184, 185]
10000110	[189]
00000010	[190]
00000110	[191, 192, 193, 194, 195]
00000111	[196, 197, 198, 199, 200, 201, 202, 203]
10010100	[204, 205]
00010110	[206, 207, 208, 209, 210, 211, 212]
00010011	[213, 214, 215, 216, 217, 218, 219, 220, 221, 222]
00010100	[223, 224, 225]
00010000	[226, 227, 228, 229]
00010001	[230, 231, 232, 233, 234, 235, 236]
00110110	[237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248]
00110100	[249, 250, 251]

00110001	251	False
01110001	1	False
01110101	3	False
00111000	11	False
01110010	29	False
01110011	22	True
01000001	33	True
00100001	38	False
01111001	39	False
01100000	39	True
01100101	44	False
01001001	42	False
01100001	49	True
00101001	53	False
01000011	67	True
00100011	70	False
11100001	75	True
01101011	79	False
11111010	95	False
01001010	95	True
11100010	106	False
11011010	104	True
11101010	114	True
10101010	116	False
11101011	119	True
11001010	125	True
11101111	125	False
11001111	134	True
10001000	137	True
11001110	142	True
10011010	152	True
10001110	162	False
10001111	166	True
00001110	172	True
00001111	173	False
01001100	181	False
00001010	186	True
00001000	181	False
10000110	187	False
00000010	191	False
00000110	191	True
00000111	197	True
10010100	213	False
00010110	203	False
00010011	219	True
00010100	225	True
00010000	225	False
00010001	231	True
00110110	242	True
00110100	249	True

Number of codes used=50


End of hp run 2.  Result of run:
[(-0.9905862850905224, 70000), ('21-05-13_14:17:04BST_NLearn_model_2_Alice_iter70000', '21-05-13_14:17:04BST_NLearn_model_2_Bob_iter70000')]
(-0.9905862850905224, 70000)


>>>> hp_run=3 of 3, time elapsed 3:26:24 of estimated 5:09:35, 
implying ending at 19:26:39BST on Thursday 13 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (714844, 936892, 888616, 165835),
	'ALICE_NET': 'MaxNet("In", 3, 50, bias_included=True, relu=True)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=     20000 training nets give:
alice_loss.item()=0.6501668691635132	bob_loss.item()=0.6306729912757874

00111000	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 48, 49, 50, 51, 52, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10111101	[41, 42]
11000111	[43, 44, 45, 46, 47, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78]
10110010	[79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111]

00111000	192	True
10111101	185	False
11000111	31	False
10110010	189	False

Number of codes used=4

Iteration=     30000 training nets give:
alice_loss.item()=0.14300449192523956	bob_loss.item()=0.262024849653244

11111101	[0, 1, 2, 3, 4, 5, 6, 7, 8, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11000111	[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
01000010	[48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
01100110	[84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138]
00110110	[139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186]
00111000	[187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]

11111101	233	True
11000111	53	False
01000010	78	True
01100110	108	True
00110110	166	True
00111000	208	True

Number of codes used=6

Iteration=     40000 training nets give:
alice_loss.item()=0.09384698420763016	bob_loss.item()=0.13210678100585938

11011101	[0, 1, 2, 3, 4, 5, 6, 7, 9, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 251, 252, 253, 254, 255]
01111000	[8, 10]
11010101	[11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43]
11000111	[44, 45]
01001011	[46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]
01000011	[75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103]
01100110	[104, 105, 106, 107]
01010110	[108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126]
01110110	[127, 128, 129, 130, 149]
01000110	[131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154]
01100100	[155, 156, 157]
00011000	[158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 173, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 207, 210, 211]
00110110	[170, 171, 172, 174]
00111000	[195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206]
01111101	[208, 209, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224]
11111101	[225, 226, 227, 228, 229, 230, 231, 232]
10011101	[249, 250]

11011101	251	True
01111000	232	False
11010101	2	False
11000111	40	False
01001011	73	True
01000011	72	False
01100110	112	False
01010110	134	False
01110110	134	False
01000110	114	False
01100100	132	False
00011000	210	True
00110110	160	False
00111000	204	True
01111101	229	False
11111101	233	False
10011101	245	False

Number of codes used=17

Iteration=     50000 training nets give:
alice_loss.item()=0.10986578464508057	bob_loss.item()=0.12568336725234985

11010101	[0, 1, 2, 3, 4, 5]
11010001	[6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 18, 19]
11001001	[16, 17, 20, 21]
10001111	[22]
11011011	[23, 24, 25]
11011111	[26, 27]
11001011	[28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]
11000111	[41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52]
11000011	[53, 55, 56, 57, 58]
10001010	[54]
11100010	[59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]
11100111	[70, 71, 72, 73, 74, 75, 76]
01011010	[77]
11100110	[78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97]
11110110	[98, 99, 100, 101, 102, 103, 104, 105]
01100000	[106, 107, 108, 109, 110, 111, 112]
01110111	[113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125]
01110110	[126, 127, 128, 134, 135, 136, 137, 138, 139, 140, 141, 148, 149, 150, 151]
00100111	[129, 130, 131, 132, 133]
01110100	[142, 143, 144, 145, 146]
00100110	[147]
00110110	[152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 167, 168]
00110100	[165, 166, 169, 170, 171, 172, 173]
00100000	[174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 192]
00100100	[184, 185, 186, 187, 188, 189, 190, 191]
00111000	[193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208]
00001000	[209, 210, 211, 212, 213]
00101000	[214, 215]
00111111	[216, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230]
01111100	[217, 218]
01111101	[231, 232]
10011101	[233, 234, 235, 236]
10111111	[237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247]
10111101	[248, 249, 250, 251]
11011101	[252, 253, 254, 255]

11010101	12	False
11010001	7	True
11001001	27	False
10001111	32	False
11011011	25	True
11011111	23	False
11001011	39	True
11000111	40	False
11000011	50	False
10001010	67	False
11100010	66	True
11100111	39	False
01011010	78	False
11100110	93	True
11110110	110	False
01100000	105	False
01110111	126	False
01110110	131	False
00100111	120	False
01110100	140	False
00100110	136	False
00110110	162	True
00110100	163	False
00100000	172	False
00100100	172	False
00111000	193	True
00001000	209	True
00101000	198	False
00111111	218	False
01111100	216	False
01111101	224	False
10011101	246	False
10111111	247	True
10111101	241	False
11011101	7	False

Number of codes used=35

Iteration=     60000 training nets give:
alice_loss.item()=0.012761803343892097	bob_loss.item()=0.08658051490783691

11101100	[0, 1, 255]
11010101	[2, 3, 4]
11010001	[5, 6, 7, 8, 9, 10, 11, 12, 13]
11011111	[14, 15, 16, 17, 18, 19, 21]
11001001	[20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]
11001011	[34, 35, 36, 37, 38, 41, 45, 46]
11000111	[39, 40, 42, 43, 44]
01001111	[47, 48, 52]
11010000	[49, 50, 51, 53, 54, 55, 56]
11001010	[57, 58, 59, 61]
11100011	[60]
11100010	[62, 63, 64, 65, 66]
01010011	[67, 68, 69, 70, 71, 72, 73]
01001011	[74]
01011010	[75, 76, 77, 78, 79, 80]
01100011	[81, 82]
01000111	[83, 84, 85, 86]
01110011	[87, 88, 89, 90, 91, 92]
00100011	[93, 94, 95, 96, 97, 100, 101]
00001010	[98, 99, 102, 103, 104, 105, 106, 107, 108, 110]
01101110	[109, 111, 112]
01100110	[113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123]
01110111	[124, 125, 126]
01111110	[127, 128, 129, 130, 131, 132, 134, 135]
01110110	[133, 138, 139, 140, 141, 142, 143, 144]
00100110	[136, 137, 145, 146]
00111110	[147]
00010110	[148, 149, 150, 151, 152, 153, 154, 155, 156]
00110110	[157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 169, 170, 171]
00100000	[167, 168, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186]
00010100	[172, 173]
00000100	[187]
00010000	[188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]
00111000	[200, 201, 202, 203, 204, 205]
00011100	[206, 207, 208]
00100001	[209, 210, 211, 212, 213, 214, 219]
00101101	[215, 216, 217]
00111101	[218]
01011101	[220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242]
10111111	[243, 244, 245, 246, 247, 249]
10101001	[248, 250, 251, 252, 253, 254]

11101100	243	False
11010101	11	False
11010001	17	False
11011111	21	True
11001001	27	True
11001011	43	False
11000111	43	True
01001111	45	False
11010000	56	True
11001010	62	False
11100011	67	False
11100010	70	False
01010011	68	True
01001011	78	False
01011010	74	False
01100011	82	True
01000111	71	False
01110011	89	True
00100011	97	True
00001010	100	False
01101110	96	False
01100110	108	False
01110111	128	False
01111110	136	False
01110110	129	False
00100110	137	True
00111110	158	False
00010110	157	False
00110110	162	True
00100000	177	True
00010100	169	False
00000100	173	False
00010000	196	True
00111000	199	False
00011100	211	False
00100001	193	False
00101101	219	False
00111101	214	False
01011101	240	True
10111111	246	True
10101001	249	False

Number of codes used=41

Iteration=     70000 training nets give:
alice_loss.item()=0.0004352495016064495	bob_loss.item()=0.0007224960718303919

11001100	[0, 1, 2, 3, 249, 250, 251, 252, 253, 254, 255]
11010101	[4, 5, 6]
11010001	[7, 8, 9, 12, 13, 14, 16]
11100001	[10, 11, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]
11011111	[15, 17, 18, 19, 20, 21, 22, 23, 24]
11001001	[25]
11000111	[38, 39, 40, 43, 44]
10001111	[41, 42]
01001111	[45, 46, 47, 48, 49]
11000011	[50, 51, 52, 53, 54, 55]
11001010	[56, 57, 58, 59, 60, 61, 62, 63]
11100010	[64, 65, 66, 68, 69, 72]
01000010	[67, 70, 71]
00000011	[73, 74, 75, 76, 77]
01101011	[78]
01011010	[79, 80, 81, 82, 83, 84]
01110011	[85, 86, 87, 88, 89]
11100110	[90, 92, 93]
00000010	[91]
00100011	[94, 95, 96, 97, 98, 99]
00001010	[100, 101]
01100000	[102, 103, 104, 105]
01100010	[106, 107, 108, 109, 110, 111, 112, 113, 114, 115]
10100110	[116, 117, 118, 119]
01110111	[120, 121]
01011110	[122, 123, 124, 125, 126, 127, 128]
01110110	[129, 130, 131, 132, 133, 134, 135, 136]
00100110	[137]
01110100	[138, 139, 140, 141, 142, 143, 144, 145, 146, 147]
00010110	[148, 150, 151, 152]
00111110	[149]
00110100	[153, 154, 155, 156, 157, 158, 159, 160, 161, 162]
00110110	[163, 164, 165, 166, 167]
00010100	[168, 169, 170, 171, 172, 173]
00100000	[174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187]
00111000	[188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198]
10110100	[199, 200, 202]
00110000	[201]
00111010	[203, 204, 205, 206]
00010001	[207, 208, 209]
00111101	[210, 211, 212]
00111111	[213, 214, 216, 217]
00011001	[215, 223, 224, 225]
01011000	[218, 219]
01110101	[220, 221, 222]
10101100	[226]
00011101	[227, 235, 236, 237, 238, 239]
01011001	[228, 229, 230, 231, 232, 233, 234]
11011100	[240, 241]
10111111	[242]
11101101	[243, 244, 245, 246, 247]
10101001	[248]

11001100	252	True
11010101	6	True
11010001	9	True
11100001	23	False
11011111	22	True
11001001	28	False
11000111	44	True
10001111	27	False
01001111	45	True
11000011	51	True
11001010	62	True
11100010	72	True
01000010	70	True
00000011	72	False
01101011	75	False
01011010	78	False
01110011	92	False
11100110	96	False
00000010	91	True
00100011	97	True
00001010	93	False
01100000	109	False
01100010	110	True
10100110	117	True
01110111	124	False
01011110	123	True
01110110	137	False
00100110	138	False
01110100	145	True
00010110	158	False
00111110	157	False
00110100	161	True
00110110	161	False
00010100	172	True
00100000	183	True
00111000	198	True
10110100	198	False
00110000	200	False
00111010	204	True
00010001	209	True
00111101	216	False
00111111	216	True
00011001	218	False
01011000	222	False
01110101	220	True
10101100	222	False
00011101	229	False
01011001	234	True
11011100	252	False
10111111	251	False
11101101	246	True
10101001	252	False

Number of codes used=52


End of hp run 3.  Result of run:
[(-0.9885387314889161, 70000), ('21-05-13_14:17:04BST_NLearn_model_3_Alice_iter70000', '21-05-13_14:17:04BST_NLearn_model_3_Bob_iter70000')]
(-0.9885387314889161, 70000)



Time taken over all 3 given sets of hyperparameters=5:30:02, averaging 1:50:01 per run


 ---- Table of results ----

 code  hp_run  result
   00       1  (-0.958, 70000)
   01       2  (-0.991, 70000)
   02       3  (-0.989, 70000)
 --------------------------

++++ Best result was (-0.991, 70000) on hp_run=2 with
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (714844, 936892, 888616, 165835),
	'ALICE_NET': 'MaxNet("In", 3, 50, relu=True)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
	'n_rng': Generator(PCG64)
	'ne_rng': Generator(PCG64)
	't_rng': <torch._C.Generator object at 0x7f87c9e06b90>
	'te_rng': <torch._C.Generator object at 0x7f87c9e06c70>
}


End closed log for run 21-05-13_14:17:04BST
