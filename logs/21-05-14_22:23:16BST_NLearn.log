The closed log for run 21-05-14_22:23:16BST

SMOOTHING_LENGTH = 10000
SAVE_PERIOD = 100000
CODE_BOOK_PERIOD = 10000
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # Used: "cuda"
MODEL_FOLDER = 'models'
CONFIGS_FOLDER = 'configs'
LOGS_FOLDER = 'logs'

hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': [(714844, 936892, 888616, 165835)],
	'ALICE_NET': ['MaxNet("In", 3, 50, dropout=0.7)', 'MaxNet("In", 3, 50, dropout=0.5)', 'MaxNet("In", 3, 50, dropout=0.3)', 'MaxNet("In", 3, 50, dropout=0.1)'],
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}



>>>> hp_run=1 of 4
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (714844, 936892, 888616, 165835),
	'ALICE_NET': 'MaxNet("In", 3, 50, dropout=0.7)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=     20000 training nets give:
alice_loss.item()=1068.5738525390625	bob_loss.item()=0.5856112241744995

01111111	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01010101	[94]
11100000	[95, 96, 223, 224]
00010101	[97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222]

01111111	224	False
01010101	213	False
11100000	190	False
00010101	4	False

Number of codes used=4

Iteration=     30000 training nets give:
alice_loss.item()=33.69224548339844	bob_loss.item()=0.6372160911560059

11011110	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 248, 249, 250, 251, 252, 253, 254, 255]
11000111	[96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150]
00101111	[129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 170, 171, 172, 173, 174, 175, 176]
01100100	[151, 152]
00101110	[153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169]
01111110	[177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197]
00010001	[198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222]
01111101	[223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243]
11101001	[244, 245, 246, 247]

11011110	52	True
11000111	249	False
00101111	117	False
01100100	132	False
00101110	133	False
01111110	68	False
00010001	146	False
01111101	66	False
11101001	159	False

Number of codes used=9

Iteration=     40000 training nets give:
alice_loss.item()=1.330216646194458	bob_loss.item()=0.38919252157211304

00110011	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01111111	[15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]
11011001	[101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164]
00011000	[165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233]

00110011	72	False
01111111	62	True
11011001	203	False
00011000	197	True

Number of codes used=4

Iteration=     50000 training nets give:
alice_loss.item()=0.7320401668548584	bob_loss.item()=0.3836510181427002

01010011	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 181, 183, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01010110	[10, 13, 15, 17, 19, 22, 25, 26, 27, 39, 40, 43, 44]
10111110	[11, 12, 14, 16, 18, 20, 21, 23, 24]
00110011	[28, 29, 30, 33, 36, 37, 38]
01111111	[31, 32, 34, 35]
11011101	[41, 42, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 67, 69, 71]
10011010	[65, 66, 68, 70, 72, 74, 75, 76, 78, 80, 81, 83, 85, 87, 88, 89, 91, 93, 94, 96, 97, 99, 100, 101, 103, 104, 105, 106, 107, 110, 111, 112, 113, 114, 115, 116, 119, 120, 122]
00111100	[73, 77]
00100011	[79, 82, 84, 86, 90, 92, 95, 98, 102, 108, 109, 117, 118, 124, 127, 130, 135, 139, 142]
01011001	[121, 123, 126, 129, 132, 136, 137, 143, 144, 145, 152, 153, 154, 160, 161, 165, 169, 172, 175, 177, 180, 182, 184]
11111100	[125, 128, 131, 133, 134, 138, 140, 141, 149, 150, 151, 155, 156, 157, 158, 159]
10100010	[146, 147, 148, 162, 166, 168, 171, 173]
10101000	[163]
11110010	[164, 167, 170]
00100101	[174, 176, 178, 179]
11011001	[217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233]

01010011	245	True
01010110	43	True
10111110	52	False
00110011	62	False
01111111	60	False
11011101	212	False
10011010	67	False
00111100	86	False
00100011	115	False
01011001	194	False
11111100	99	False
10100010	163	False
10101000	180	False
11110010	204	False
00100101	77	False
11011001	213	False

Number of codes used=16

Iteration=     60000 training nets give:
alice_loss.item()=0.34045499563217163	bob_loss.item()=0.20667670667171478

11011001	[0, 1, 2, 3, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01010001	[4, 5, 6]
01111111	[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86]
00110011	[87, 88, 90, 91]
11111100	[89, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 112, 113, 114, 115, 116, 122, 123, 124, 125, 126, 130, 132, 134, 136, 138, 139]
01101111	[95]
00100011	[110, 111, 117, 118, 119, 120, 121, 127, 128, 129]
00000011	[131, 133, 135, 137, 140, 144, 150, 151, 152, 153, 154, 155, 161, 165, 167, 168, 170, 172]
10101000	[141, 142, 143, 145, 146, 147, 148, 149, 156, 157, 158, 159, 160, 162, 163, 164, 166]
10000100	[169, 171, 174, 175, 178, 179, 180, 181, 182, 183, 184, 185, 186, 188, 189, 191, 192, 193, 194, 195, 196, 197, 198, 200]
11110010	[173, 176, 177]
11011000	[187]
11100101	[190]
00011001	[199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219]

11011001	214	False
01010001	2	False
01111111	64	True
00110011	59	False
11111100	94	True
01101111	77	False
00100011	121	True
00000011	139	False
10101000	164	True
10000100	222	False
11110010	208	False
11011000	180	False
11100101	215	False
00011001	198	False

Number of codes used=14

Iteration=     70000 training nets give:
alice_loss.item()=0.29142799973487854	bob_loss.item()=0.07298917323350906

01010011	[0, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01010001	[1, 2, 3]
01010110	[4, 5, 6, 7, 8, 9, 10, 11]
01011011	[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 49, 50, 51, 52, 54, 55, 56]
01101111	[47, 48, 53, 84, 87, 88, 89, 92, 94, 95, 99, 100, 104, 108, 109, 110, 111, 113]
00110101	[57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73]
00111100	[74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
10011010	[85, 86, 90, 96, 97]
00110011	[91, 93]
00100011	[98, 101, 102, 103, 105, 106, 107]
01000011	[112, 114, 115, 116, 117, 118, 119, 120, 121, 122]
00000011	[123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149]
00011000	[150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204]
11011001	[205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227]

01010011	238	True
01010001	2	True
01010110	34	False
01011011	37	True
01101111	78	False
00110101	48	False
00111100	70	False
10011010	80	False
00110011	53	False
00100011	124	False
01000011	127	False
00000011	144	True
00011000	201	True
11011001	217	True

Number of codes used=14


End of hp run 1.  Result of run:
[(-0.8731234077547196, 70000), ('21-05-14_22:23:16BST_NLearn_model_1_Alice_iter70000', '21-05-14_22:23:16BST_NLearn_model_1_Bob_iter70000')]
(-0.8731234077547196, 70000)


>>>> hp_run=2 of 4, time elapsed 2:07:51 of estimated 8:31:26, 
implying ending at 06:54:42BST on Saturday 15 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (714844, 936892, 888616, 165835),
	'ALICE_NET': 'MaxNet("In", 3, 50, dropout=0.5)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=     20000 training nets give:
alice_loss.item()=1166.537109375	bob_loss.item()=0.6255846619606018

01111001	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01100100	[93, 94, 95, 224, 225, 226]
10000110	[96, 223]
11010001	[97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222]
10110101	[141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177]

01111001	63	True
01100100	255	False
10000110	199	False
11010001	254	False
10110101	238	False

Number of codes used=5

Iteration=     30000 training nets give:
alice_loss.item()=62.89057922363281	bob_loss.item()=0.4349864721298218

10111100	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 39, 40, 41, 42, 43, 44, 52, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11000011	[28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 57, 58, 59]
10101011	[56, 60, 76, 77, 78]
00000000	[61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75]
00101010	[79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108]
00110110	[109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155]
01100110	[124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135]
00011101	[156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173]
10110000	[174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215]

10111100	224	True
11000011	95	False
10101011	90	False
00000000	73	True
00101010	58	False
00110110	190	False
01100110	185	False
00011101	222	False
10110000	185	True

Number of codes used=9

Iteration=     40000 training nets give:
alice_loss.item()=2.951754093170166	bob_loss.item()=0.3809284567832947

01010111	[0, 1, 8]
10000111	[2, 3, 4, 5, 6, 7, 9]
11000000	[10, 11, 12, 13, 14, 15, 16, 17, 18]
01111100	[19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]
01011000	[34]
00010011	[35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98]
00110100	[99, 100, 101, 102, 103]
00011101	[104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167]
11011001	[168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240]
01101101	[241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]

01010111	112	False
10000111	131	False
11000000	239	False
01111100	54	False
01011000	45	False
00010011	76	True
00110100	143	False
00011101	102	False
11011001	235	True
01101101	185	False

Number of codes used=10

Iteration=     50000 training nets give:
alice_loss.item()=0.36450883746147156	bob_loss.item()=0.26384949684143066

11000100	[0, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 249, 250, 252, 254]
00000010	[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 244, 245, 246, 247, 248, 251, 253, 255]
11000001	[62, 63, 64, 65, 66, 67, 68, 69, 70, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89]
00000000	[71, 72, 73, 74, 75, 76, 77, 78, 88, 91, 106, 110, 111]
00000111	[90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 107, 108]
11000011	[109, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124]
00110100	[125, 126]
10000101	[127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149]
01100110	[150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174]
10110000	[175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191]
10100000	[192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208]

11000100	249	True
00000010	33	True
11000001	79	True
00000000	63	False
00000111	87	False
11000011	72	False
00110100	120	False
10000101	118	False
01100110	169	True
10110000	176	True
10100000	216	False

Number of codes used=11

Iteration=     60000 training nets give:
alice_loss.item()=0.3569883704185486	bob_loss.item()=0.19245021045207977

11000100	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01100100	[30, 31, 32, 33]
01111001	[34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]
00000111	[73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113]
01111101	[114, 115, 116, 118, 119, 121, 123]
10110111	[117, 120, 122, 124, 125, 126, 127, 128, 186, 187, 188]
11100111	[129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 177, 179, 182, 183, 185]
10100000	[175, 176, 178, 180, 181, 184, 196, 200, 207, 213, 217]
11100000	[189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 201, 203, 204, 208, 209, 210, 214, 215, 216, 218, 219, 220, 221, 222, 223, 224, 225, 226, 231]
11100001	[202, 205, 206]
10111110	[211, 212, 227, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243]

11000100	242	False
01100100	22	False
01111001	67	True
00000111	91	True
01111101	84	False
10110111	122	True
11100111	161	True
10100000	203	False
11100000	223	True
11100001	223	False
10111110	223	False

Number of codes used=11

Iteration=     70000 training nets give:
alice_loss.item()=0.4290446937084198	bob_loss.item()=0.03994344174861908

11000100	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 234, 235, 237, 238, 240, 242, 243, 244, 245, 246, 248, 250, 251, 253, 255]
01100100	[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
01111001	[31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
10101011	[72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85]
11010001	[86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106]
01111111	[107]
11000001	[108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123]
10000111	[124, 125, 126]
10100111	[127, 128, 129, 130, 131, 132, 133, 134]
11100111	[135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188]
01100110	[145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172]
10100000	[189, 190, 191, 192, 194, 195, 197, 199, 201, 202, 203, 204, 205, 206, 207, 208]
00101001	[193, 196, 198, 200, 212, 217, 221, 226, 229, 231, 232]
11100001	[209, 210, 211, 213, 214, 215, 216]
10101100	[218, 219, 220, 222, 223]
11100000	[224, 225, 227, 230]
01111110	[228, 233, 236, 239, 241, 247, 249, 252, 254]

11000100	244	True
01100100	20	True
01111001	62	True
10101011	92	False
11010001	93	True
01111111	108	False
11000001	80	False
10000111	125	True
10100111	141	False
11100111	155	False
01100110	152	True
10100000	205	True
00101001	214	False
11100001	214	True
10101100	188	False
11100000	221	False
01111110	255	False

Number of codes used=17


End of hp run 2.  Result of run:
[(-0.9193476074036011, 70000), ('21-05-14_22:23:16BST_NLearn_model_2_Alice_iter70000', '21-05-14_22:23:16BST_NLearn_model_2_Bob_iter70000')]
(-0.9193476074036011, 70000)


>>>> hp_run=3 of 4, time elapsed 4:05:57 of estimated 8:11:54, 
implying ending at 06:35:10BST on Saturday 15 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (714844, 936892, 888616, 165835),
	'ALICE_NET': 'MaxNet("In", 3, 50, dropout=0.3)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=     20000 training nets give:
alice_loss.item()=1115.21875	bob_loss.item()=0.4916209578514099

01110011	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01110111	[101, 102, 103, 104, 105, 106, 107, 108, 212, 213, 214, 215, 216, 217, 218, 219]
11110000	[109, 110, 111, 112, 113, 114, 115, 206, 207, 208, 209, 210, 211]
11000110	[116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205]
11111010	[220]

01110011	23	True
01110111	20	False
11110000	43	False
11000110	251	False
11111010	55	False

Number of codes used=5

Iteration=     30000 training nets give:
alice_loss.item()=45.7359619140625	bob_loss.item()=0.6345285773277283

01110011	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00111100	[90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226]
10100100	[227, 228, 229, 230, 231, 232, 233, 234, 235, 236]

01110011	10	True
00111100	170	True
10100100	177	False

Number of codes used=3

Iteration=     40000 training nets give:
alice_loss.item()=6.458317756652832	bob_loss.item()=0.4844752848148346

00000010	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01110011	[17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]
01010101	[45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107]
10100100	[108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203]
00111100	[126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 155, 156, 157, 158, 159, 160, 161]

00000010	215	True
01110011	250	False
01010101	94	True
10100100	185	True
00111100	184	False

Number of codes used=5

Iteration=     50000 training nets give:
alice_loss.item()=0.4710434675216675	bob_loss.item()=0.42342233657836914

01100111	[0, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11011011	[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 19, 20]
10010000	[16, 17, 18, 21, 22, 23, 24, 25, 26, 27, 28]
01011001	[29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73]
10110100	[74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 112, 113, 114, 115, 116, 117, 118, 119]
01010101	[93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111]
11000111	[120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 142, 144, 146, 148, 149, 151, 152, 153, 155, 156, 157, 158]
10111100	[141, 143, 145, 147, 150, 154, 159, 160, 161, 167, 168, 169, 174, 177, 180, 185, 190, 192, 193, 195, 197]
11110001	[162, 163, 164, 165, 166, 170, 171, 172, 173, 175, 176, 178, 179, 181, 182, 183, 184, 186, 187, 188, 189, 191, 194, 196]
01001010	[198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231]

01100111	234	True
11011011	241	False
10010000	78	False
01011001	73	True
10110100	98	False
01010101	86	False
11000111	107	False
10111100	164	False
11110001	214	False
01001010	230	True

Number of codes used=10

Iteration=     60000 training nets give:
alice_loss.item()=0.4027603566646576	bob_loss.item()=0.15308615565299988

01100001	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 58, 59, 248, 249, 250, 251, 252, 253, 254, 255]
01000011	[57, 62, 68, 75, 78, 86]
11110000	[60, 61, 63]
01011001	[64, 65, 66, 67, 69, 70, 71, 72, 73, 74]
01001101	[76, 77, 79, 80, 81]
01010101	[82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133]
00111100	[134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180]
10100100	[181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192]
00000010	[193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213]
00001010	[214, 215, 216, 217]
01100111	[218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244]
00100010	[245, 246, 247]

01100001	4	True
01000011	69	False
11110000	45	False
01011001	73	True
01001101	72	False
01010101	92	True
00111100	159	True
10100100	176	False
00000010	221	False
00001010	228	False
01100111	234	True
00100010	224	False

Number of codes used=12

Iteration=     70000 training nets give:
alice_loss.item()=0.1997961699962616	bob_loss.item()=0.10197406262159348

01110011	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01010101	[46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 105, 106, 107]
01100100	[91, 98, 104, 108, 109, 113, 116, 117, 120, 121]
11100100	[110, 111, 112, 114, 115]
11000111	[118, 119, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132]
10111100	[133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 151, 154, 175]
00111100	[150, 152, 155, 159]
10101010	[153, 156, 157, 158, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 196, 197, 198, 199]
00010010	[195, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241]
00000010	[200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 244, 245]
11101001	[242, 243]

01110011	17	True
01010101	98	False
01100100	94	False
11100100	116	False
11000111	113	False
10111100	153	False
00111100	158	False
10101010	183	True
00010010	225	False
00000010	218	True
11101001	242	True

Number of codes used=11


End of hp run 3.  Result of run:
[(-0.8860475051780213, 70000), ('21-05-14_22:23:16BST_NLearn_model_3_Alice_iter70000', '21-05-14_22:23:16BST_NLearn_model_3_Bob_iter70000')]
(-0.8860475051780213, 70000)


>>>> hp_run=4 of 4, time elapsed 6:03:10 of estimated 8:04:13, 
implying ending at 06:27:30BST on Saturday 15 May 2021
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (714844, 936892, 888616, 165835),
	'ALICE_NET': 'MaxNet("In", 3, 50, dropout=0.1)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=     20000 training nets give:
alice_loss.item()=1131.213623046875	bob_loss.item()=0.48143434524536133

01101111	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01000001	[78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244]
10011111	[94, 95, 96, 97, 226, 227]
11111111	[98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224]
01011110	[225]

01101111	159	False
01000001	23	False
10011111	193	False
11111111	205	True
01011110	169	False

Number of codes used=5

Iteration=     30000 training nets give:
alice_loss.item()=17.868366241455078	bob_loss.item()=0.5485391616821289

01101111	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
01111010	[30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10000100	[135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240]

01101111	27	True
01111010	43	True
10000100	162	True

Number of codes used=3

Iteration=     40000 training nets give:
alice_loss.item()=3.8220303058624268	bob_loss.item()=0.40455299615859985

01110101	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01111010	[31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]
01101111	[54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105]
10000100	[106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208]
10110010	[209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221]

01110101	254	True
01111010	58	False
01101111	45	False
10000100	181	True
10110010	137	False

Number of codes used=5

Iteration=     50000 training nets give:
alice_loss.item()=0.4840390086174011	bob_loss.item()=0.27892279624938965

01110101	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10001110	[10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
10001111	[64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]
01011110	[80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
00001000	[100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122]
10000000	[123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
01000001	[144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203]

01110101	0	True
10001110	34	True
10001111	67	True
01011110	77	False
00001000	124	False
10000000	149	False
01000001	148	True

Number of codes used=7

Iteration=     60000 training nets give:
alice_loss.item()=0.4060567319393158	bob_loss.item()=0.2034197598695755

10001110	[0, 17, 19, 20, 23, 24, 25, 30, 31, 32, 33, 34, 35, 37, 38]
11110111	[1, 2, 3, 4, 5, 6, 18, 21, 22, 26, 27, 28, 29, 39, 40, 249, 252, 253, 254]
01101110	[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 36, 43, 49]
01101111	[41, 42, 44, 45, 46, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113]
01000001	[114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168]
10100100	[169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181]
00000100	[182, 183, 184, 185, 187, 188, 189, 190, 191, 194, 195, 197, 198, 199, 200]
00000001	[186, 196, 201, 205, 208, 209]
11100100	[192, 193, 202, 203, 204, 216, 217, 218]
11001110	[206, 207, 210, 211, 212, 213, 214, 215, 219]
11111011	[220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240]
11110011	[241, 242, 243, 244, 245, 246, 247, 248, 250, 251]
01111101	[255]

10001110	34	True
11110111	244	False
01101110	31	False
01101111	62	True
01000001	149	True
10100100	179	True
00000100	171	False
00000001	148	False
11100100	183	False
11001110	20	False
11111011	253	False
11110011	246	True
01111101	10	False

Number of codes used=13

Iteration=     70000 training nets give:
alice_loss.item()=0.17504271864891052	bob_loss.item()=0.06532920897006989

01110101	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11010011	[10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
10001110	[24, 25, 26, 27, 28, 29, 30, 31, 35, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52]
10001111	[32, 33, 34, 36, 37, 38, 39, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75]
01001011	[76, 77, 78, 79, 81, 82, 83, 84, 87, 90, 91, 92, 94, 95, 96, 97, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 113, 114, 115, 116, 117, 118]
01011110	[80, 85, 86, 88, 89, 93, 98, 103]
00100111	[110, 111, 112, 119, 123, 125, 128, 129, 130, 131, 133, 134, 135, 136, 138, 143, 144, 146]
01001001	[120, 121, 122, 124, 126, 127]
00000010	[132]
10000000	[137, 139, 140, 141, 142, 145, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156]
01000001	[157, 158, 159, 160, 161, 162, 163]
10000100	[164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217]

01110101	1	True
11010011	17	True
10001110	32	False
10001111	62	True
01001011	83	True
01011110	92	False
00100111	114	False
01001001	116	False
00000010	120	False
10000000	153	True
01000001	147	False
10000100	186	True

Number of codes used=12


End of hp run 4.  Result of run:
[(-0.8771119638063423, 70000), ('21-05-14_22:23:16BST_NLearn_model_4_Alice_iter70000', '21-05-14_22:23:16BST_NLearn_model_4_Bob_iter70000')]
(-0.8771119638063423, 70000)



Time taken over all 4 given sets of hyperparameters=8:00:08, averaging 2:00:02 per run


 ---- Table of results ----

 code  hp_run  result
   00       1  (-0.873, 70000)
   01       2  (-0.919, 70000)
   02       3  (-0.886, 70000)
   03       4  (-0.877, 70000)
 --------------------------

++++ Best result was (-0.919, 70000) on hp_run=2 with
hyperparameters = {
	'N_ITERATIONS': 70000,
	'RANDOM_SEEDS': (714844, 936892, 888616, 165835),
	'ALICE_NET': 'MaxNet("In", 3, 50, dropout=0.5)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 640000,
	'START_TRAINING': 20000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 2000,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 20000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 30000,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
	'n_rng': Generator(PCG64)
	'ne_rng': Generator(PCG64)
	't_rng': <torch._C.Generator object at 0x7f00d9a508f0>
	'te_rng': <torch._C.Generator object at 0x7f00d9a50910>
}


End closed log for run 21-05-14_22:23:16BST
