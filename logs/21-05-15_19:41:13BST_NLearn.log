The closed log for run 21-05-15_19:41:13BST

SMOOTHING_LENGTH = 2500
SAVE_PERIOD = 100000
CODE_BOOK_PERIOD = 2500
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # Used: "cuda"
MODEL_FOLDER = 'models'
CONFIGS_FOLDER = 'configs'
LOGS_FOLDER = 'logs'

hyperparameters = {
	'N_ITERATIONS': 17500,
	'RANDOM_SEEDS': [(714844, 936892, 888616, 165835)],
	'ALICE_NET': 'MaxNet("In", 3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 160000,
	'START_TRAINING': 5000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 500,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 5000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 7500,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}



>>>> hp_run=1 of 1
hyperparameters = {
	'N_ITERATIONS': 17500,
	'RANDOM_SEEDS': (714844, 936892, 888616, 165835),
	'ALICE_NET': 'MaxNet("In", 3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 160000,
	'START_TRAINING': 5000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 500,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 5000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 7500,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=      5000 training nets give:
alice_loss.item()=0.5360234975814819	bob_loss.item()=0.5451840162277222

01011100	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01111101	[25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
11111110	[36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
11000000	[60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131]
00101000	[132, 133, 134, 135, 136, 137]
01011001	[138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183]
00101010	[184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205]
01001110	[206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224]

01011100	116	False
01111101	129	False
11111110	119	False
11000000	146	False
00101000	173	False
01011001	139	True
00101010	170	False
01001110	123	False

Number of codes used=8

Iteration=      7500 training nets give:
alice_loss.item()=0.1816544234752655	bob_loss.item()=0.26822373270988464

00011110	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10110101	[38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85]
01001000	[86, 87, 88, 89, 90, 91]
11000000	[92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153]
00101010	[154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190]
01011101	[191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]

00011110	2	True
10110101	28	False
01001000	254	False
11000000	111	True
00101010	174	True
01011101	236	False

Number of codes used=6

Iteration=     10000 training nets give:
alice_loss.item()=0.1668827384710312	bob_loss.item()=0.19662104547023773

00011110	[0, 1, 2, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00011010	[3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
10110101	[13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]
10010101	[50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87]
10110100	[88, 89]
11000000	[90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136]
01011001	[137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158]
00101010	[159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196]
01011101	[197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245]

00011110	7	False
00011010	254	False
10110101	51	False
10010101	43	False
10110100	31	False
11000000	115	True
01011001	138	True
00101010	182	True
01011101	218	True

Number of codes used=9

Iteration=     12500 training nets give:
alice_loss.item()=0.16355036199092865	bob_loss.item()=0.1659068763256073

01011110	[0, 1, 2, 3, 4, 254, 255]
00110101	[5, 6, 7, 8, 9]
10000101	[10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]
10100101	[34, 35, 36, 37, 38]
11100101	[39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]
11110101	[65, 66, 67, 68, 69, 70, 71, 72, 73, 74]
10000000	[75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]
11000000	[101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132]
00001010	[133, 134, 135, 136, 137, 138, 139, 140, 141, 142]
01011001	[143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163]
00101010	[164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185]
01101010	[186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210]
01011101	[211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235]
00011100	[236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253]

01011110	248	False
00110101	35	False
10000101	55	False
10100101	51	False
11100101	65	False
11110101	57	False
10000000	95	True
11000000	122	True
00001010	174	False
01011001	141	False
00101010	187	False
01101010	190	True
01011101	215	True
00011100	253	True

Number of codes used=14

Iteration=     15000 training nets give:
alice_loss.item()=0.06887372583150864	bob_loss.item()=0.13795241713523865

00011100	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10110001	[17, 18, 19, 20, 21, 22, 23, 24]
10010101	[25, 26, 27, 28, 29, 30, 31, 32, 33]
00100101	[34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]
10110101	[46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58]
11110101	[59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]
11010000	[70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81]
10000000	[82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107]
11000000	[108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131]
01011001	[132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156]
00001010	[157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171]
00101010	[172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189]
01101010	[190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204]
01011101	[205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232]

00011100	250	True
10110001	42	False
10010101	39	False
00100101	38	True
10110101	56	True
11110101	60	True
11010000	91	False
10000000	99	True
11000000	119	True
01011001	144	True
00001010	173	False
00101010	185	True
01101010	191	True
01011101	214	True

Number of codes used=14

Iteration=     17500 training nets give:
alice_loss.item()=0.013265468180179596	bob_loss.item()=0.01345845591276884

00011100	[0, 1, 2, 254, 255]
00011011	[3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
10110111	[13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38]
00100101	[39, 40, 41, 42, 43, 44]
10100101	[45, 46, 47, 48, 49, 50, 51, 52, 53]
10110101	[54, 55, 56, 57, 58, 59]
11110101	[60, 61, 62, 63]
11000100	[64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84]
10000000	[85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110]
11000000	[111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130]
01011001	[131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156]
01001001	[157]
00001010	[158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177]
00101010	[178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188]
01101010	[189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203]
01011101	[204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225]
01010101	[226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238]
00111100	[239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249]
01011110	[250, 251, 252, 253]

00011100	248	False
00011011	254	False
10110111	30	True
00100101	34	False
10100101	45	True
10110101	56	True
11110101	62	True
11000100	75	True
10000000	96	True
11000000	117	True
01011001	145	True
01001001	142	False
00001010	175	True
00101010	186	True
01101010	197	True
01011101	218	True
01010101	227	True
00111100	237	False
01011110	254	False

Number of codes used=19


End of hp run 1.  Result of run:
[(-0.9754495073649964, 17500), ('21-05-15_19:41:13BST_NLearn_model_1_Alice_iter17500', '21-05-15_19:41:13BST_NLearn_model_1_Bob_iter17500')]
(-0.9754495073649964, 17500)



Time taken over all 1 given sets of hyperparameters=0:26:42, averaging 0:26:42 per run


 ---- Table of results ----

 code  hp_run  result
    0       1  (-0.975, 17500)
 --------------------------

++++ Best result was (-0.975, 17500) on hp_run=1 with
hyperparameters = {
	'N_ITERATIONS': 17500,
	'RANDOM_SEEDS': (714844, 936892, 888616, 165835),
	'ALICE_NET': 'MaxNet("In", 3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 160000,
	'START_TRAINING': 5000,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 500,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 5000,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 7500,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
	'n_rng': Generator(PCG64)
	'ne_rng': Generator(PCG64)
	't_rng': <torch._C.Generator object at 0x7f35ff99ccf0>
	'te_rng': <torch._C.Generator object at 0x7f35ff99c970>
}


End closed log for run 21-05-15_19:41:13BST
