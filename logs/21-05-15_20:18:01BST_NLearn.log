The closed log for run 21-05-15_20:18:01BST

SMOOTHING_LENGTH = 3571
SAVE_PERIOD = 100000
CODE_BOOK_PERIOD = 3571
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # Used: "cuda"
MODEL_FOLDER = 'models'
CONFIGS_FOLDER = 'configs'
LOGS_FOLDER = 'logs'

hyperparameters = {
	'N_ITERATIONS': 25000,
	'RANDOM_SEEDS': [(714844, 936892, 888616, 165835)],
	'ALICE_NET': 'MaxNet("In", 3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 228571,
	'START_TRAINING': 7142,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 714,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 7142,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 10714,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}



>>>> hp_run=1 of 1
hyperparameters = {
	'N_ITERATIONS': 25000,
	'RANDOM_SEEDS': (714844, 936892, 888616, 165835),
	'ALICE_NET': 'MaxNet("In", 3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 228571,
	'START_TRAINING': 7142,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 714,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 7142,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 10714,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=      7142 training nets give:
alice_loss.item()=0.6501375436782837	bob_loss.item()=0.5874632596969604

10001111	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00100000	[24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88]
00101110	[89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163]

10001111	179	True
00100000	192	False
00101110	223	False

Number of codes used=3

Iteration=     10713 training nets give:
alice_loss.item()=0.18949124217033386	bob_loss.item()=0.3643704652786255

10110010	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 255]
11100110	[82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145]
10001111	[146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228]
00011010	[229, 230, 231, 232, 233, 234, 235, 236]
10000101	[237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254]

10110010	46	True
11100110	121	True
10001111	166	True
00011010	12	False
10000101	203	False

Number of codes used=5

Iteration=     14284 training nets give:
alice_loss.item()=0.10888376832008362	bob_loss.item()=0.24428197741508484

10110011	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10111010	[26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]
10110010	[38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55]
11100010	[56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97]
01100110	[98, 99, 100, 134, 135, 136, 137, 138, 139]
11100110	[101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133]
10101111	[140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]
10001111	[160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180]
11001111	[181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197]
10001011	[198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241]

10110011	29	False
10111010	37	True
10110010	28	False
11100010	97	True
01100110	125	False
11100110	137	False
10101111	170	False
10001111	187	False
11001111	187	True
10001011	205	True

Number of codes used=10

Iteration=     17855 training nets give:
alice_loss.item()=0.15916095674037933	bob_loss.item()=0.1768643856048584

10000011	[0, 1, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00011010	[2, 3, 4, 5]
10000001	[6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
10111010	[32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 54, 55, 56, 57, 58]
10110010	[48, 49, 50, 51, 52, 53]
10100010	[59, 60, 61, 62, 63, 64, 65, 66, 67, 68]
10101010	[69, 70]
11100010	[71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
11100110	[115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132]
11101110	[133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166]
10001111	[167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181]
11001111	[182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192]
00101111	[193, 194, 195, 196, 197, 198, 199]
10001011	[200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218]
10001001	[219, 220, 221, 222, 223, 224, 225, 226, 227]
00011011	[228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242]

10000011	238	False
00011010	240	False
10000001	6	True
10111010	28	False
10110010	39	False
10100010	77	False
10101010	49	False
11100010	95	True
11100110	102	False
11101110	148	True
10001111	179	True
11001111	183	True
00101111	171	False
10001011	193	False
10001001	215	False
00011011	0	False

Number of codes used=16

Iteration=     21426 training nets give:
alice_loss.item()=0.024096064269542694	bob_loss.item()=0.06396939605474472

01011010	[0, 1, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00010011	[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
00010010	[13, 14, 15, 16, 17, 18]
10111110	[19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
10110000	[32, 33, 34, 35, 36, 37, 38, 39]
10111011	[40, 41, 42]
10101010	[43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62]
01100010	[63, 64, 65, 66, 67, 68, 69, 70, 71]
11100000	[72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82]
11100010	[83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109]
10100110	[110, 111, 112, 113, 114, 115, 116]
01100110	[117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135]
11110110	[136, 137]
11101110	[138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156]
10001110	[157, 158, 159, 160, 161, 162, 163]
10101101	[164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179]
01001111	[180]
11001101	[181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198]
10001001	[199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224]
10000011	[225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243]
00011010	[244, 245]

01011010	1	True
00010011	11	True
00010010	10	False
10111110	23	True
10110000	42	False
10111011	47	False
10101010	57	True
01100010	81	False
11100000	83	False
11100010	102	True
10100110	103	False
01100110	122	True
11110110	129	False
11101110	141	True
10001110	170	False
10101101	172	True
01001111	176	False
11001101	181	True
10001001	212	True
10000011	236	True
00011010	244	True

Number of codes used=21

Iteration=     24997 training nets give:
alice_loss.item()=0.0005699567263945937	bob_loss.item()=0.0016053584404289722

10000001	[0, 1, 255]
00010011	[2, 3, 4, 5, 6, 7, 8, 9, 10]
00010010	[11, 12, 13, 14, 15, 16]
10111110	[17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
10110000	[32, 33, 34, 35, 36, 37, 38, 39]
10111011	[40, 41, 42, 43, 44, 45, 46, 47, 48, 49]
10101010	[50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68]
01100010	[69, 70, 71, 72]
11100000	[73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90]
11100010	[91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106]
10100110	[107, 108, 109, 110, 111, 112, 113, 114, 115, 116]
01100110	[117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131]
11101110	[132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156]
10001110	[157, 158, 159, 160, 161]
01001111	[162, 163, 164, 165, 166, 176, 177, 178, 179, 180]
11001110	[167, 168, 169, 170, 171, 172, 173, 174, 175]
11001011	[181, 182, 183, 184, 185, 186, 187, 188]
00000111	[189, 190, 191, 192, 193, 194, 195]
00001011	[196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206]
10001001	[207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222]
10000011	[223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234]
00011010	[235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247]
10011010	[248, 249, 250, 251, 252, 253, 254]

10000001	11	False
00010011	7	True
00010010	13	True
10111110	24	True
10110000	43	False
10111011	48	True
10101010	59	True
01100010	82	False
11100000	79	True
11100010	101	True
10100110	112	True
01100110	125	True
11101110	143	True
10001110	169	False
01001111	176	True
11001110	170	True
11001011	182	True
00000111	184	False
00001011	194	False
10001001	211	True
10000011	235	False
00011010	247	True
10011010	255	False

Number of codes used=23


End of hp run 1.  Result of run:
[(-0.9818927458982242, 24997), ('21-05-15_20:18:01BST_NLearn_model_1_Alice_iter25000', '21-05-15_20:18:01BST_NLearn_model_1_Bob_iter25000')]
(-0.9818927458982242, 24997)



Time taken over all 1 given sets of hyperparameters=0:38:36, averaging 0:38:36 per run


 ---- Table of results ----

 code  hp_run  result
    0       1  (-0.982, 24997)
 --------------------------

++++ Best result was (-0.982, 24997) on hp_run=1 with
hyperparameters = {
	'N_ITERATIONS': 25000,
	'RANDOM_SEEDS': (714844, 936892, 888616, 165835),
	'ALICE_NET': 'MaxNet("In", 3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 228571,
	'START_TRAINING': 7142,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 714,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 7142,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 10714,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
	'n_rng': Generator(PCG64)
	'ne_rng': Generator(PCG64)
	't_rng': <torch._C.Generator object at 0x7fc7f3232cf0>
	'te_rng': <torch._C.Generator object at 0x7fc7f3232970>
}


End closed log for run 21-05-15_20:18:01BST
