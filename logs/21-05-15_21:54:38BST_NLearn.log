The closed log for run 21-05-15_21:54:38BST

SMOOTHING_LENGTH = 3571
SAVE_PERIOD = 100000
CODE_BOOK_PERIOD = 3571
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # Used: "cuda"
MODEL_FOLDER = 'models'
CONFIGS_FOLDER = 'configs'
LOGS_FOLDER = 'logs'

hyperparameters = {
	'N_ITERATIONS': 25000,
	'RANDOM_SEEDS': [(714844, 936892, 888616, 165835)],
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 228571,
	'START_TRAINING': 7142,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 714,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 7142,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 10714,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}



>>>> hp_run=1 of 1
hyperparameters = {
	'N_ITERATIONS': 25000,
	'RANDOM_SEEDS': (714844, 936892, 888616, 165835),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 228571,
	'START_TRAINING': 7142,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 714,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 7142,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 10714,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=      7142 training nets give:
alice_loss.item()=0.6462386846542358	bob_loss.item()=0.6002883911132812

10010110	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00110011	[17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]
00000011	[37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52]
01010010	[53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123]
00000111	[124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161]
11011101	[162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182]
10101110	[183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233]

10010110	225	False
00110011	201	False
00000011	13	False
01010010	243	False
00000111	248	False
11011101	31	False
10101110	0	False

Number of codes used=7

Iteration=     10713 training nets give:
alice_loss.item()=0.1497088223695755	bob_loss.item()=0.21794848144054413

10010110	[0, 1, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00000011	[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52]
01010000	[53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117]
11011100	[118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]

10010110	232	True
00000011	27	True
01010000	101	True
11011100	183	True

Number of codes used=4

Iteration=     14284 training nets give:
alice_loss.item()=0.20684321224689484	bob_loss.item()=0.1828310191631317

10010110	[0, 1, 2, 3, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10101110	[4, 5, 6]
00000011	[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
00100011	[60, 61, 62, 63, 64, 65]
01010000	[66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102]
01110000	[103, 104, 105, 106, 130, 131, 132, 133, 134, 135, 136, 137, 138]
01001010	[107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129]
11111100	[139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150]
01011100	[151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166]
11011100	[167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188]
11111101	[189, 190, 191, 192, 193]
10010010	[194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219]

10010110	232	True
10101110	207	False
00000011	23	True
00100011	33	False
01010000	110	False
01110000	99	False
01001010	136	False
11111100	184	False
01011100	169	False
11011100	175	True
11111101	188	False
10010010	245	False

Number of codes used=12

Iteration=     17855 training nets give:
alice_loss.item()=0.15769228339195251	bob_loss.item()=0.20147013664245605

10010111	[0, 1, 2, 3, 4, 5, 6, 7, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00000010	[8, 9, 10, 11]
00000011	[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]
00100011	[41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]
01110000	[65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96]
01010000	[97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112]
01010001	[113, 114, 115, 116]
01011110	[117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151]
01011100	[152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169]
11011100	[170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184]
10111101	[185]
11111101	[186, 187]
11010100	[188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201]
10011110	[202, 203, 204, 205, 206, 207, 208, 209, 210, 211]
11011111	[212, 213, 214, 215, 216, 217]
10011100	[218, 219, 220, 221, 222]
10010110	[223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242]

10010111	251	True
00000010	26	False
00000011	34	True
00100011	39	False
01110000	90	True
01010000	92	False
01010001	95	False
01011110	152	False
01011100	154	True
11011100	162	False
10111101	180	False
11111101	200	False
11010100	188	True
10011110	216	False
11011111	193	False
10011100	189	False
10010110	224	True

Number of codes used=17

Iteration=     21426 training nets give:
alice_loss.item()=0.07432589679956436	bob_loss.item()=0.16292433440685272

10010111	[0, 1, 2, 3, 4, 5, 6, 14, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10010010	[7, 8, 9, 10, 11, 12, 13]
00000010	[15]
00000011	[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
00100011	[32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]
01100011	[47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78]
01110000	[79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96]
01010000	[97, 98, 99]
01011001	[100, 101, 102, 103, 104, 105, 106, 107, 108, 109]
01011010	[110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120]
01001010	[121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140]
01001110	[141, 142, 143, 144, 145, 146, 147, 148, 149, 150]
01011110	[151]
01011100	[152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166]
11010101	[167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189]
11111101	[190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200]
11111111	[201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223]
10010110	[224, 225, 226, 227, 228, 229, 230, 231, 232, 233]
10110110	[234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245]

10010111	253	True
10010010	249	False
00000010	26	False
00000011	36	False
00100011	43	True
01100011	53	True
01110000	92	True
01010000	96	False
01011001	111	False
01011010	121	False
01001010	128	True
01001110	140	False
01011110	149	False
01011100	154	True
11010101	177	True
11111101	196	True
11111111	215	True
10010110	228	True
10110110	239	True

Number of codes used=19

Iteration=     24997 training nets give:
alice_loss.item()=0.0008746465318836272	bob_loss.item()=0.0033037252724170685

10010111	[0, 1, 2, 3, 4, 5, 6, 7, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00001011	[8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]
00010011	[23, 24, 25, 26, 27]
00000011	[28, 29, 30, 31, 32, 33, 34, 35, 36, 37]
00100011	[38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]
01100011	[49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73]
01110000	[74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92]
01010000	[93, 94, 95, 96, 97, 98, 99]
01010001	[100, 101]
01011001	[102, 103, 104, 105, 106, 107, 108, 109, 114, 115]
01011010	[110, 111, 112, 113, 116, 117, 118]
01001010	[119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136]
01001110	[137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148]
01011110	[149, 150, 151, 152]
01011100	[153, 154, 155, 156, 157, 158, 159, 160]
11011101	[161, 162, 163, 164, 165, 166, 167, 168, 169, 170]
11010101	[171, 172, 173, 174, 175, 176, 177, 178]
10111101	[179, 180, 181, 182, 183, 184, 185, 186, 187]
11111101	[188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203]
11111111	[204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223]
10010110	[224, 225, 226, 227, 228, 229, 230]
10110110	[231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245]

10010111	252	True
00001011	24	False
00010011	34	False
00000011	33	True
00100011	45	True
01100011	56	True
01110000	91	True
01010000	96	True
01010001	102	False
01011001	110	False
01011010	119	False
01001010	129	True
01001110	143	True
01011110	145	False
01011100	151	False
11011101	162	True
11010101	170	False
10111101	182	True
11111101	186	False
11111111	217	True
10010110	234	False
10110110	239	True

Number of codes used=22


End of hp run 1.  Result of run:
[(-0.9809977651026711, 24997), ('21-05-15_21:54:38BST_NLearn_model_1_Alice_iter25000', '21-05-15_21:54:38BST_NLearn_model_1_Bob_iter25000')]
(-0.9809977651026711, 24997)



Time taken over all 1 given sets of hyperparameters=0:26:52, averaging 0:26:52 per run


 ---- Table of results ----

 code  hp_run  result
    0       1  (-0.981, 24997)
 --------------------------

++++ Best result was (-0.981, 24997) on hp_run=1 with
hyperparameters = {
	'N_ITERATIONS': 25000,
	'RANDOM_SEEDS': (714844, 936892, 888616, 165835),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 228571,
	'START_TRAINING': 7142,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 714,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 7142,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 10714,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
	'n_rng': Generator(PCG64)
	'ne_rng': Generator(PCG64)
	't_rng': <torch._C.Generator object at 0x7f74a8ef2cf0>
	'te_rng': <torch._C.Generator object at 0x7f74a8ef2970>
}


End closed log for run 21-05-15_21:54:38BST
