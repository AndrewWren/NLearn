The closed log for run 21-05-15_22:28:32BST

SMOOTHING_LENGTH = 3571
SAVE_PERIOD = 100000
CODE_BOOK_PERIOD = 3571
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # Used: "cuda"
MODEL_FOLDER = 'models'
CONFIGS_FOLDER = 'configs'
LOGS_FOLDER = 'logs'

hyperparameters = {
	'N_ITERATIONS': 25000,
	'RANDOM_SEEDS': [(789873, 935177, 972236, 435766), (532334, 809631, 735618, 545983), (321406, 416695, 885201, 467036), (911011, 571019, 667157, 225093), (335581, 265392, 137411, 842014), (307035, 405050, 968633, 690674), (577683, 443890, 562139, 319257), (625084, 419126, 762692, 952720)],
	'ALICE_NET': ['MaxNet("In", 3, 50)', 'FFs(3, 50)'],
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 228571,
	'START_TRAINING': 7142,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 714,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 7142,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 10714,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}



>>>> hp_run=1 of 16
hyperparameters = {
	'N_ITERATIONS': 25000,
	'RANDOM_SEEDS': (789873, 935177, 972236, 435766),
	'ALICE_NET': 'MaxNet("In", 3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 228571,
	'START_TRAINING': 7142,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 714,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 7142,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 10714,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=      7142 training nets give:
alice_loss.item()=0.6180400252342224	bob_loss.item()=0.6650435924530029

10011110	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11111100	[76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134]
10100001	[135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165]
11110100	[166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237]
01000111	[238, 239]

10011110	41	True
11111100	61	False
10100001	10	False
11110100	98	False
01000111	19	False

Number of codes used=5

Iteration=     10713 training nets give:
alice_loss.item()=0.08481621742248535	bob_loss.item()=0.2413868010044098

01000111	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10011110	[24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]
11111100	[50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98]
00011001	[99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150]
00100011	[151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222]

01000111	11	True
10011110	44	True
11111100	76	True
00011001	98	False
00100011	192	True

Number of codes used=5

Iteration=     14284 training nets give:
alice_loss.item()=0.051274191588163376	bob_loss.item()=0.13358700275421143

01000111	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10011110	[30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
11111100	[52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76]
00111001	[77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109]
00011001	[110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
00100111	[144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157]
00100011	[158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217]
01000110	[218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238]

01000111	7	True
10011110	55	False
11111100	74	True
00111001	120	False
00011001	128	True
00100111	159	False
00100011	197	True
01000110	252	False

Number of codes used=8

Iteration=     17855 training nets give:
alice_loss.item()=0.12731926143169403	bob_loss.item()=0.16422821581363678

01000111	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 253, 254, 255]
00011110	[25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
11000111	[52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67]
11011110	[68, 69, 70, 71, 72]
11011100	[73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]
00011011	[90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111]
00111001	[112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130]
01011001	[131, 132, 133, 134, 135]
00100001	[136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150]
01100011	[151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177]
00100011	[178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219]
10100111	[220, 221]
01000110	[222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249]
01010111	[250, 251, 252]

01000111	1	True
00011110	42	True
11000111	20	False
11011110	52	False
11011100	72	False
00011011	101	True
00111001	119	True
01011001	121	False
00100001	147	True
01100011	173	True
00100011	204	True
10100111	182	False
01000110	247	True
01010111	4	False

Number of codes used=14

Iteration=     21426 training nets give:
alice_loss.item()=0.059120893478393555	bob_loss.item()=0.08403217792510986

01000110	[0, 1, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01000111	[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
00010111	[15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
00011110	[31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]
10011110	[51, 52, 53, 54, 55, 56, 57, 58, 59]
00111110	[60, 61]
01111100	[62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86]
00011011	[87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103]
10001001	[104, 105, 106, 107, 108]
10111000	[109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121]
00111001	[122, 123, 124]
00011001	[125, 126, 127, 128, 129, 130, 131, 132, 133, 134]
00010001	[135, 136, 137, 138, 139, 140, 141, 142]
00100001	[143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154]
00100111	[155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167]
01100011	[168, 169, 170, 171, 172, 173]
01110011	[174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185]
00100011	[186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218]
01100111	[219, 220, 221, 222, 223, 224, 225, 226, 227, 228]

01000110	242	True
01000111	252	False
00010111	253	False
00011110	44	True
10011110	46	False
00111110	48	False
01111100	72	True
00011011	94	True
10001001	112	False
10111000	113	True
00111001	117	False
00011001	127	True
00010001	129	False
00100001	154	True
00100111	167	True
01100011	181	False
01110011	184	True
00100011	182	False
01100111	242	False

Number of codes used=19

Iteration=     24997 training nets give:
alice_loss.item()=0.000940009718760848	bob_loss.item()=0.0035974844358861446

01000111	[0, 1, 2, 3, 4, 5, 6, 253, 254, 255]
00000111	[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
01010101	[21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]
00011110	[38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]
10011110	[49, 50, 51, 52, 53, 54, 55, 56, 57]
10011101	[58, 59]
01111100	[60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81]
11101100	[82, 83, 84, 85, 86, 87, 88, 89, 90, 91]
00011011	[92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103]
10001001	[104, 105, 106]
00111001	[107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135]
00011001	[136]
00010001	[137, 138, 139]
00100001	[140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156]
00100111	[157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168]
00110011	[169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179]
00000011	[180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196]
00100011	[197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220]
01100110	[221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252]

01000111	254	True
00000111	12	True
01010101	27	True
00011110	42	True
10011110	49	True
10011101	50	False
01111100	73	True
11101100	81	False
00011011	96	True
10001001	112	False
00111001	122	True
00011001	127	False
00010001	125	False
00100001	151	True
00100111	163	True
00110011	172	True
00000011	189	True
00100011	205	True
01100110	241	True

Number of codes used=19


End of hp run 1.  Result of run:
[(-0.9753357715004952, 24997), ('21-05-15_22:28:32BST_NLearn_model_1_Alice_iter25000', '21-05-15_22:28:32BST_NLearn_model_1_Bob_iter25000')]
(-0.9753357715004952, 24997)


>>>> hp_run=2 of 16, time elapsed 0:38:26 of estimated 10:14:52, 
implying ending at 08:43:24BST on Sunday 16 May 2021
hyperparameters = {
	'N_ITERATIONS': 25000,
	'RANDOM_SEEDS': (789873, 935177, 972236, 435766),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 228571,
	'START_TRAINING': 7142,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 714,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 7142,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 10714,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=      7142 training nets give:
alice_loss.item()=0.6751396656036377	bob_loss.item()=0.5119298696517944

00010100	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01101001	[72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137]
00010011	[138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230]

00010100	213	False
01101001	197	False
00010011	116	False

Number of codes used=3

Iteration=     10713 training nets give:
alice_loss.item()=0.21220740675926208	bob_loss.item()=0.29495489597320557

00010000	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11110001	[20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113]
00000001	[114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177]
01110110	[178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217]

00010000	18	True
11110001	45	True
00000001	145	True
01110110	204	True

Number of codes used=4

Iteration=     14284 training nets give:
alice_loss.item()=0.1823725700378418	bob_loss.item()=0.2900097966194153

00010010	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11110101	[26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]
11110001	[38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]
11110011	[65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97]
01000001	[98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123]
00000001	[124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171]
00000011	[172, 173, 174, 175, 176, 177, 178, 179]
01110110	[180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226]

00010010	253	True
11110101	48	False
11110001	61	True
11110011	78	True
01000001	127	False
00000001	121	False
00000011	135	False
01110110	209	True

Number of codes used=8

Iteration=     17855 training nets give:
alice_loss.item()=0.1314222514629364	bob_loss.item()=0.2686004042625427

00010100	[0, 1, 2, 3, 4, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00010000	[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
11010001	[20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38]
11110101	[39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]
11110001	[51, 52, 53, 54, 55, 56, 57, 58, 59]
11110000	[60, 61, 62, 63, 64, 65, 66, 67]
11110011	[68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81]
10110011	[82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
01110001	[96, 97, 98, 99, 100]
01100001	[101, 102, 103, 104]
01010001	[105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122]
01000001	[123]
00000001	[124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139]
01000101	[140, 141, 142, 143, 144, 145, 146, 147]
00010011	[148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164]
01111110	[165, 166]
01110010	[167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192]
01110110	[193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214]
00010110	[215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234]
00110110	[235, 236, 237, 238, 239, 240, 241, 242]

00010100	244	True
00010000	8	True
11010001	34	True
11110101	52	False
11110001	82	False
11110000	58	False
11110011	83	False
10110011	91	True
01110001	106	False
01100001	114	False
01010001	106	True
01000001	136	False
00000001	138	True
01000101	136	False
00010011	168	False
01111110	187	False
01110010	190	True
01110110	188	False
00010110	221	True
00110110	213	False

Number of codes used=20

Iteration=     21426 training nets give:
alice_loss.item()=0.028213102370500565	bob_loss.item()=0.0538434237241745

00010010	[0, 1, 241, 252, 253, 254, 255]
00010000	[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
10010000	[16, 17, 18, 19, 20, 21, 22, 23, 24]
11010001	[25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]
11110101	[43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58]
11110001	[59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]
11110011	[75, 76, 77, 78, 79, 80, 81, 82, 83, 84]
10110011	[85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98]
01010001	[99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111]
11100011	[112, 113, 114, 115, 116, 117]
11111011	[118, 119, 120, 121]
01000101	[122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149]
00010011	[150, 151, 152, 153, 154, 155, 156]
01100110	[157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175]
01110010	[176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195]
01110110	[196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207]
01010110	[208, 209, 210, 211, 212, 213]
00010110	[214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228]
00110100	[229, 230, 231, 232, 233, 234, 235, 236]
01110100	[237, 238, 239, 240]
00010100	[242, 243, 244, 245, 246, 247, 248, 249, 250, 251]

00010010	245	False
00010000	5	True
10010000	20	True
11010001	36	True
11110101	56	True
11110001	56	False
11110011	81	True
10110011	92	True
01010001	112	False
11100011	127	False
11111011	106	False
01000101	133	True
00010011	170	False
01100110	176	False
01110010	194	True
01110110	202	True
01010110	204	False
00010110	224	True
00110100	241	False
01110100	208	False
00010100	242	True

Number of codes used=21

Iteration=     24997 training nets give:
alice_loss.item()=0.00031696894438937306	bob_loss.item()=0.0021826601587235928

00010000	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 255]
10010000	[14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]
11010001	[29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]
11110101	[45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60]
11110000	[61, 62, 63, 64, 65, 66, 67, 68, 69]
11110001	[70]
11110011	[71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85]
10110011	[86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101]
01010001	[102, 103, 104, 105, 106, 107]
01100001	[108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124]
01000101	[125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137]
00000011	[138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153]
00010011	[154, 155, 156, 157, 158, 159, 160, 161]
01100110	[162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182]
01110010	[183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193]
01110110	[194, 195, 196, 197, 198, 199, 200]
01010110	[201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214]
00010110	[215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230]
00110100	[231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243]
00010100	[244, 245, 246, 247, 248, 249, 250]
00010010	[251, 252, 253, 254]

00010000	3	True
10010000	22	True
11010001	38	True
11110101	55	True
11110000	58	False
11110001	61	False
11110011	79	True
10110011	93	True
01010001	112	False
01100001	120	True
01000101	134	True
00000011	152	True
00010011	166	False
01100110	172	True
01110010	192	True
01110110	202	False
01010110	203	True
00010110	224	True
00110100	237	True
00010100	244	True
00010010	249	False

Number of codes used=21


End of hp run 2.  Result of run:
[(-0.9843443098054548, 24997), ('21-05-15_22:28:32BST_NLearn_model_2_Alice_iter25000', '21-05-15_22:28:32BST_NLearn_model_2_Bob_iter25000')]
(-0.9843443098054548, 24997)


>>>> hp_run=3 of 16, time elapsed 1:05:37 of estimated 8:44:59, 
implying ending at 07:13:31BST on Sunday 16 May 2021
hyperparameters = {
	'N_ITERATIONS': 25000,
	'RANDOM_SEEDS': (532334, 809631, 735618, 545983),
	'ALICE_NET': 'MaxNet("In", 3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 228571,
	'START_TRAINING': 7142,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 714,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 7142,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 10714,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=      7142 training nets give:
alice_loss.item()=0.608336865901947	bob_loss.item()=0.5932295918464661

10110011	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10011000	[79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175]
10011011	[176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246]
01001110	[188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227]

10110011	59	True
10011000	21	False
10011011	20	False
01001110	230	False

Number of codes used=4

Iteration=     10713 training nets give:
alice_loss.item()=0.1527871936559677	bob_loss.item()=0.22543883323669434

10011011	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]
10110011	[29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73]
00011011	[74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129]
00000101	[130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204]
01001110	[205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]

10011011	48	False
10110011	47	True
00011011	80	True
00000101	181	True
01001110	228	True

Number of codes used=5

Iteration=     14284 training nets give:
alice_loss.item()=0.051729172468185425	bob_loss.item()=0.15498556196689606

10010011	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
10011011	[26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
10110011	[48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66]
10111011	[67, 68, 69, 70, 71, 72, 73]
00011011	[74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98]
00011001	[99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147]
00000101	[148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193]
00000111	[194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207]
01001110	[208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]

10010011	43	False
10011011	51	False
10110011	37	False
10111011	44	False
00011011	79	True
00011001	110	True
00000101	178	True
00000111	192	False
01001110	232	True

Number of codes used=9

Iteration=     17855 training nets give:
alice_loss.item()=0.1553148627281189	bob_loss.item()=0.16484269499778748

01101110	[0, 1, 2, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10000011	[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]
10110011	[29, 30, 31, 32, 33, 34, 35]
10011011	[36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]
10011010	[54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75]
00011011	[76, 77, 78, 79, 80, 81, 82, 83, 84]
01011001	[85, 86, 87, 88, 89, 90, 91, 92, 93]
00011001	[94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144]
00000101	[145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183]
00000100	[184, 185, 186, 187, 188]
00001101	[189, 190]
00000111	[191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201]
00001111	[202, 203, 204, 205, 206, 207, 208]
10000101	[209, 210, 211, 212, 213, 214]
10000100	[215, 216, 217, 218, 219]

01101110	233	True
10000011	33	False
10110011	49	False
10011011	55	False
10011010	54	True
00011011	88	False
01011001	104	False
00011001	113	True
00000101	163	True
00000100	177	False
00001101	201	False
00000111	193	True
00001111	210	False
10000101	214	True
10000100	190	False

Number of codes used=15

Iteration=     21426 training nets give:
alice_loss.item()=0.05500517413020134	bob_loss.item()=0.03301919996738434

01001110	[0, 1, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10010111	[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]
10000011	[18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]
10001011	[47, 48, 49, 50, 51, 52, 53, 54, 55]
10011001	[56, 57, 58, 59, 60, 61, 62, 63, 64, 65]
00110010	[66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76]
00011011	[77, 78, 79, 80, 81, 82, 83, 84]
00111001	[85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98]
00011001	[99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137]
00000101	[138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174]
00001100	[175, 176, 177, 178, 179, 180, 181, 182]
00010101	[183, 184, 185, 186]
00000111	[187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198]
00000001	[199, 200, 201]
00001111	[202, 203, 204, 205, 206, 207, 208, 209]
01011110	[210, 211]
10000101	[212, 213, 214, 215, 216, 217, 218, 219, 220]
01101110	[221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235]

01001110	242	True
10010111	25	False
10000011	37	True
10001011	44	False
10011001	59	True
00110010	62	False
00011011	90	False
00111001	116	False
00011001	115	True
00000101	162	True
00001100	188	False
00010101	174	False
00000111	185	False
00000001	190	False
00001111	213	False
01011110	216	False
10000101	213	True
01101110	229	True

Number of codes used=18

Iteration=     24997 training nets give:
alice_loss.item()=0.002380328718572855	bob_loss.item()=0.01719607785344124

11101110	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 249, 250, 251, 252, 253, 254, 255]
10010111	[15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]
11011011	[38, 39, 40, 41, 42, 43, 44, 45]
10010011	[46, 47, 48, 49, 50, 51, 52, 53]
10011001	[54, 55, 56, 57, 58, 59, 60, 61]
01011011	[62, 63, 64, 65, 66, 67]
00011010	[68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81]
00011011	[82, 83, 84, 85, 86, 87, 88, 89, 90, 91]
00010001	[92, 93, 94, 95, 96, 97, 98, 99, 100, 101]
00011001	[102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136]
00111001	[137]
00000101	[138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173]
00001100	[174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186]
00000111	[187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198]
00000001	[199, 200, 201, 202, 203]
00001111	[204, 205, 206]
01011110	[207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221]
01101110	[222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234]
01001110	[235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248]

11101110	255	True
10010111	27	True
11011011	41	True
10010011	47	True
10011001	55	True
01011011	65	True
00011010	63	False
00011011	73	False
00010001	107	False
00011001	109	True
00111001	111	False
00000101	160	True
00001100	188	False
00000111	186	False
00000001	195	False
00001111	218	False
01011110	215	True
01101110	225	True
01001110	238	True

Number of codes used=19


End of hp run 3.  Result of run:
[(-0.968501990759508, 24997), ('21-05-15_22:28:32BST_NLearn_model_3_Alice_iter25000', '21-05-15_22:28:32BST_NLearn_model_3_Bob_iter25000')]
(-0.968501990759508, 24997)


>>>> hp_run=4 of 16, time elapsed 1:44:48 of estimated 9:18:58, 
implying ending at 07:47:31BST on Sunday 16 May 2021
hyperparameters = {
	'N_ITERATIONS': 25000,
	'RANDOM_SEEDS': (532334, 809631, 735618, 545983),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 228571,
	'START_TRAINING': 7142,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 714,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 7142,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 10714,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=      7142 training nets give:
alice_loss.item()=0.6934235095977783	bob_loss.item()=0.6342223882675171

01110100	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01000111	[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]
00101010	[101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190]
00101100	[191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237]

01110100	112	False
01000111	158	False
00101010	81	False
00101100	104	False

Number of codes used=4

Iteration=     10713 training nets give:
alice_loss.item()=0.18893708288669586	bob_loss.item()=0.2648620009422302

01100010	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00010010	[66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139]
01000111	[140, 141, 142, 143, 144, 145]
00111000	[146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191]
11111001	[192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231]

01100010	19	True
00010010	104	True
01000111	64	False
00111000	198	False
11111001	218	True

Number of codes used=5

Iteration=     14284 training nets give:
alice_loss.item()=0.14468079805374146	bob_loss.item()=0.21411770582199097

01100000	[0, 1, 2, 3, 4, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01100010	[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]
01000010	[49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62]
00010011	[63, 64, 65, 66, 67, 68, 69, 70, 71, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147]
10001001	[72, 73, 74, 75, 76, 77, 78, 79]
00010010	[80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117]
00111010	[148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 198, 199, 200, 201, 202, 203, 204, 224, 225, 226, 227, 228, 229, 230]
00111000	[177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197]
11111001	[205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223]

01100000	3	True
01100010	15	True
01000010	52	True
00010011	114	False
10001001	95	False
00010010	131	False
00111010	164	True
00111000	200	False
11111001	229	False

Number of codes used=9

Iteration=     17855 training nets give:
alice_loss.item()=0.3079245686531067	bob_loss.item()=0.2634457051753998

01100000	[0, 1, 2, 3, 253, 254, 255]
01100100	[4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
01100010	[14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]
11100010	[25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]
01010010	[37, 38, 39]
01000010	[40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68]
01000110	[69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87]
10001001	[88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113]
00010011	[114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138]
00111011	[139, 140, 141, 142, 143, 144, 145, 146, 147]
00111010	[148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177]
00011011	[178, 179, 180, 181, 182, 183]
00111110	[184]
00111000	[185, 186, 187, 188, 189, 190, 191, 192]
10111001	[193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210]
01111001	[211, 212, 213, 214, 215]
11111001	[216, 217, 218, 219, 220, 221, 222, 223]
01111000	[224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236]
11111000	[237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252]

01100000	19	False
01100100	3	False
01100010	12	False
11100010	22	False
01010010	52	False
01000010	44	True
01000110	69	True
10001001	100	True
00010011	124	True
00111011	154	False
00111010	166	True
00011011	129	False
00111110	186	False
00111000	207	False
10111001	209	True
01111001	220	False
11111001	223	True
01111000	235	True
11111000	243	True

Number of codes used=19

Iteration=     21426 training nets give:
alice_loss.item()=0.017733581364154816	bob_loss.item()=0.08316470682621002

01100100	[0, 1, 2, 3, 4, 5, 6, 7]
01100010	[8, 9, 10, 11, 12, 13, 14, 15, 16]
11100010	[17, 18, 19, 20, 21, 22, 23, 24, 25, 26]
01100011	[27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]
01000010	[40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52]
01000000	[53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]
01000110	[73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86]
10001001	[87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105]
00011011	[106, 107, 108, 109, 110, 111]
00010011	[112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124]
00010010	[125, 126, 127, 128, 129]
00110011	[130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148]
00111011	[149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]
00111010	[160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174]
00111110	[175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194]
00111001	[195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214]
11101000	[215, 216, 217, 218, 219, 220]
11111001	[221, 222, 223, 224, 225, 226, 227, 228, 229, 230]
01111000	[231, 232, 233, 234, 235, 236]
11111000	[237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249]
01110000	[250, 251, 252, 253, 254, 255]

01100100	6	True
01100010	20	False
11100010	25	True
01100011	39	True
01000010	49	True
01000000	46	False
01000110	72	False
10001001	102	True
00011011	138	False
00010011	125	False
00010010	110	False
00110011	132	True
00111011	163	False
00111010	166	True
00111110	181	True
00111001	207	True
11101000	254	False
11111001	224	True
01111000	237	False
11111000	245	True
01110000	7	False

Number of codes used=21

Iteration=     24997 training nets give:
alice_loss.item()=0.0009297793731093407	bob_loss.item()=0.0028955144807696342

01100100	[0, 1, 2, 3, 4, 5, 6, 7]
11100000	[8, 9, 10, 11, 12, 13, 14, 15, 16, 17]
11100010	[18, 19, 20, 21, 22, 23, 24, 25, 26]
01100011	[27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]
01000010	[41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52]
01000011	[53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]
01000110	[65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81]
10000001	[82, 83, 84, 85, 86]
00100010	[87, 88, 89, 90]
10001001	[91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110]
00010110	[111, 112, 113, 114, 115, 116, 117, 118]
00010011	[119, 120, 121, 122, 123, 124, 125, 126, 127, 128]
00110011	[129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144]
10111010	[145, 146, 147, 148, 149, 150]
00111011	[151, 152, 153, 154, 155, 156]
00111010	[157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174]
00111110	[175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194]
00111001	[195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210]
10111001	[211, 212, 213, 214]
11111011	[215, 216, 217, 218, 219, 220, 221]
11111001	[222, 223, 224, 225, 226, 227, 228, 229, 230, 231]
01111000	[232, 233, 234, 235, 236, 237]
11111000	[238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252]
01110000	[253, 254, 255]

01100100	4	True
11100000	12	True
11100010	25	True
01100011	36	True
01000010	52	True
01000011	63	True
01000110	73	True
10000001	80	False
00100010	76	False
10001001	101	True
00010110	110	False
00010011	124	True
00110011	133	True
10111010	158	False
00111011	164	False
00111010	167	True
00111110	183	True
00111001	205	True
10111001	207	False
11111011	224	False
11111001	231	True
01111000	240	False
11111000	246	True
01110000	0	False

Number of codes used=24


End of hp run 4.  Result of run:
[(-0.9847402990311803, 24997), ('21-05-15_22:28:32BST_NLearn_model_4_Alice_iter25000', '21-05-15_22:28:32BST_NLearn_model_4_Bob_iter25000')]
(-0.9847402990311803, 24997)


>>>> hp_run=5 of 16, time elapsed 2:11:56 of estimated 8:47:45, 
implying ending at 07:16:17BST on Sunday 16 May 2021
hyperparameters = {
	'N_ITERATIONS': 25000,
	'RANDOM_SEEDS': (321406, 416695, 885201, 467036),
	'ALICE_NET': 'MaxNet("In", 3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 228571,
	'START_TRAINING': 7142,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 714,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 7142,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 10714,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=      7142 training nets give:
alice_loss.item()=0.471466600894928	bob_loss.item()=0.5808262825012207

11010001	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01110000	[38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125]
10011101	[126, 127, 128, 129, 130, 131, 132, 133, 134, 135]
10000101	[136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187]
00010111	[188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241]

11010001	48	False
01110000	42	True
10011101	17	False
10000101	81	False
00010111	24	False

Number of codes used=5

Iteration=     10713 training nets give:
alice_loss.item()=0.09066297858953476	bob_loss.item()=0.21068492531776428

00010111	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11010001	[17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
01110000	[32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93]
10010100	[94, 95, 96, 97, 98, 99, 100]
10011100	[101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182]
10100011	[183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241]

00010111	227	False
11010001	29	True
01110000	67	True
10010100	143	False
10011100	143	True
10100011	192	True

Number of codes used=6

Iteration=     14284 training nets give:
alice_loss.item()=0.11901748180389404	bob_loss.item()=0.21987313032150269

10000011	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 254, 255]
11010001	[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
01110000	[48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75]
11110000	[76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101]
10011000	[102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126]
10111100	[127, 128, 129, 130, 131]
10001100	[132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149]
10011100	[150, 151, 152]
00011100	[153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180]
10100011	[181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208]
10110011	[209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223]
00100011	[224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253]

10000011	221	False
11010001	14	False
01110000	81	False
11110000	70	False
10011000	133	False
10111100	126	False
10001100	137	True
10011100	119	False
00011100	150	False
10100011	217	False
10110011	221	True
00100011	232	True

Number of codes used=12

Iteration=     17855 training nets give:
alice_loss.item()=0.07203635573387146	bob_loss.item()=0.08380541205406189

11000001	[0, 1, 251, 252, 253, 254, 255]
11000011	[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
11010001	[15, 16, 17, 18, 19]
01010001	[20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]
11110010	[43, 44, 45, 46, 47, 48]
01110001	[49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62]
01110100	[63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93]
01111000	[94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112]
00001100	[113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123]
10000100	[124, 125, 126, 127, 128, 129, 130]
10111100	[131, 132, 133]
10011100	[134, 135, 136]
10001100	[137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151]
10100001	[152, 153, 154, 155, 156, 157, 158, 159, 160]
10011110	[161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178]
10101011	[179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]
10100011	[200, 201, 202, 203]
10101111	[204, 205, 206]
10110011	[207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221]
00110011	[222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249]
00100001	[250]

11000001	16	False
11000011	246	False
11010001	30	False
01010001	34	True
11110010	71	False
01110001	62	True
01110100	69	True
01111000	94	True
00001100	131	False
10000100	121	False
10111100	143	False
10011100	139	False
10001100	141	True
10100001	189	False
10011110	156	False
10101011	189	True
10100011	204	False
10101111	183	False
10110011	219	True
00110011	234	True
00100001	236	False

Number of codes used=21

Iteration=     21426 training nets give:
alice_loss.item()=0.021538708359003067	bob_loss.item()=0.07205570489168167

11000001	[0, 1, 2, 3, 4, 5, 252, 253, 254, 255]
11011001	[6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]
11110101	[19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]
01100001	[37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54]
01110001	[55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]
01110000	[75, 76, 77, 78, 79, 80]
11010100	[81, 82, 83, 84, 85, 86, 87, 88, 89, 90]
01111010	[91, 92, 93, 94, 95, 96, 97]
01111000	[98]
11111000	[99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116]
10000100	[117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128]
10011100	[129, 130, 131, 132, 133, 134, 135, 136]
10001100	[137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147]
10111000	[148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167]
00011101	[168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188]
10101011	[189, 190, 191, 192, 193, 194, 195, 196]
10100111	[197, 198, 199, 200, 201, 202, 203, 204, 205]
10100011	[206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216]
10110011	[217, 218, 219, 220, 221, 222]
00100111	[223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236]
11100011	[237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251]

11000001	4	True
11011001	16	True
11110101	29	True
01100001	49	True
01110001	63	True
01110000	69	False
11010100	80	False
01111010	92	True
01111000	104	False
11111000	110	True
10000100	124	True
10011100	140	False
10001100	142	True
10111000	153	True
00011101	175	True
10101011	189	True
10100111	199	True
10100011	202	False
10110011	208	False
00100111	226	True
11100011	239	True

Number of codes used=21

Iteration=     24997 training nets give:
alice_loss.item()=0.0005138746928423643	bob_loss.item()=0.0019198181107640266

11011011	[0, 1, 2, 3, 4, 253, 254, 255]
11000001	[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
11110101	[20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]
01010001	[34, 35, 36, 37, 38, 39, 40, 41, 42, 43]
01100001	[44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57]
01110001	[58, 59, 60, 61, 62, 63, 64, 65, 66, 67]
01110000	[68, 69, 70, 71, 72, 73, 74, 75, 76]
01110100	[77, 78, 79]
11010100	[80, 81, 82, 83, 84, 85, 86, 87, 88]
01111010	[89, 90, 91, 92, 93, 94, 95]
01111000	[96, 97, 98, 99, 100, 101]
11111000	[102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
10000100	[115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131]
10011100	[132, 133, 134, 135, 136, 137, 138, 139, 140]
10111000	[141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160]
10011110	[161, 162]
00011101	[163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182]
10101011	[183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194]
10100111	[195, 196, 197, 198, 199, 200, 201, 202, 203]
10100011	[204, 205, 206, 207, 208, 209]
10000011	[210, 211, 212, 213, 214, 215, 216, 217, 218, 219]
10110011	[220, 221]
00000011	[222, 223, 224, 225]
00110011	[226, 227, 228, 229, 230, 231, 232]
00100111	[233, 234]
11100011	[235, 236, 237, 238, 239, 240, 241, 242]
10010001	[243, 244, 245, 246, 247, 248, 249, 250, 251, 252]

11011011	0	True
11000001	5	True
11110101	30	True
01010001	35	True
01100001	51	True
01110001	63	True
01110000	72	True
01110100	72	False
11010100	81	True
01111010	91	True
01111000	98	True
11111000	105	True
10000100	124	True
10011100	134	True
10111000	152	True
10011110	153	False
00011101	173	True
10101011	183	True
10100111	197	True
10100011	206	True
10000011	217	True
10110011	219	False
00000011	229	False
00110011	229	True
00100111	227	False
11100011	238	True
10010001	246	True

Number of codes used=27


End of hp run 5.  Result of run:
[(-0.9896711929277252, 24997), ('21-05-15_22:28:32BST_NLearn_model_5_Alice_iter25000', '21-05-15_22:28:32BST_NLearn_model_5_Bob_iter25000')]
(-0.9896711929277252, 24997)


>>>> hp_run=6 of 16, time elapsed 2:51:05 of estimated 9:07:27, 
implying ending at 07:36:00BST on Sunday 16 May 2021
hyperparameters = {
	'N_ITERATIONS': 25000,
	'RANDOM_SEEDS': (321406, 416695, 885201, 467036),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 228571,
	'START_TRAINING': 7142,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 714,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 7142,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 10714,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=      7142 training nets give:
alice_loss.item()=0.5550155639648438	bob_loss.item()=0.5000336170196533

11111110	[0, 1, 2, 3, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11000000	[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 116, 117, 118, 119, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230]
10100100	[20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]
01111011	[42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98]
10010101	[120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146]

11111110	207	False
11000000	106	False
10100100	162	False
01111011	216	False
10010101	139	True

Number of codes used=5

Iteration=     10713 training nets give:
alice_loss.item()=0.3128829002380371	bob_loss.item()=0.325662761926651

11111110	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01001011	[22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]
11101101	[50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122]
10010101	[123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181]
10010110	[182, 183, 184, 185, 186]

11111110	236	True
01001011	71	False
11101101	93	True
10010101	129	True
10010110	229	False

Number of codes used=5

Iteration=     14284 training nets give:
alice_loss.item()=0.14919590950012207	bob_loss.item()=0.20539698004722595

11011110	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 252, 253, 254, 255]
10111110	[31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]
01101101	[47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78]
10101101	[79, 80, 81, 82, 83]
11101101	[84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
11101001	[96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112]
11001101	[113, 114, 115]
10010101	[116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160]
11111101	[161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193]
11110110	[194, 195, 196, 197, 198, 199]
11111110	[200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251]

11011110	1	True
10111110	251	False
01101101	72	True
10101101	84	False
11101101	70	False
11101001	93	False
11001101	89	False
10010101	111	False
11111101	91	False
11110110	248	False
11111110	254	False

Number of codes used=11

Iteration=     17855 training nets give:
alice_loss.item()=0.19283321499824524	bob_loss.item()=0.23730503022670746

11110110	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 255]
11011010	[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]
01001010	[37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
01001011	[62, 63, 64, 65]
00111110	[66, 67, 68, 69, 70, 71, 72, 73, 74]
01001101	[75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88]
10010111	[89, 90, 91, 92, 93, 94, 95, 96, 97]
11010101	[98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110]
10010001	[111, 112, 113, 114, 115, 116]
10010101	[117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168]
10111010	[169, 170, 171, 172, 173, 174, 175]
11001110	[176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212]
11111110	[213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254]

11110110	234	False
11011010	0	False
01001010	42	True
01001011	49	False
00111110	246	False
01001101	78	True
10010111	119	False
11010101	107	True
10010001	119	False
10010101	124	True
10111010	4	False
11001110	227	False
11111110	240	True

Number of codes used=13

Iteration=     21426 training nets give:
alice_loss.item()=0.059304267168045044	bob_loss.item()=0.18747949600219727

11101110	[0, 1, 2, 3, 4, 253, 254, 255]
10111100	[5, 6, 7, 8, 9, 10, 11]
01001010	[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]
01101100	[45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55]
00001011	[56, 57, 58, 59]
11101111	[60, 61, 62, 63, 64, 65]
11101101	[66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77]
10101001	[78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]
11100101	[90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]
11010101	[101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111]
10110101	[112, 113]
10010001	[114]
10010101	[115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153]
10010010	[154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]
11001110	[210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233]
11111110	[234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252]

11101110	245	False
10111100	233	False
01001010	29	True
01101100	42	False
00001011	69	False
11101111	65	True
11101101	94	False
10101001	92	False
11100101	96	True
11010101	117	False
10110101	129	False
10010001	130	False
10010101	134	True
10010010	180	True
11001110	216	True
11111110	229	False

Number of codes used=16

Iteration=     24997 training nets give:
alice_loss.item()=0.008284803479909897	bob_loss.item()=0.03400367498397827

01011010	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 254, 255]
01001010	[20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]
01101100	[45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55]
11101111	[56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67]
11001011	[68, 69, 70, 71, 72, 73, 74, 75, 76]
01001101	[77, 78, 79]
11111001	[80, 81, 82, 83, 84, 85, 86, 87, 88]
10101101	[89, 90, 91, 92]
11100101	[93, 94, 95, 96, 97, 98, 99]
11010101	[100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113]
00010101	[114]
10010111	[115, 116, 117, 118, 119, 120]
10010101	[121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154]
10010010	[155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]
11001110	[200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228]
11111110	[229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241]
11101110	[242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252]
11111100	[253]

01011010	17	True
01001010	34	True
01101100	44	False
11101111	69	False
11001011	74	True
01001101	85	False
11111001	89	False
10101101	91	True
11100101	94	True
11010101	107	True
00010101	124	False
10010111	131	False
10010101	135	True
10010010	165	True
11001110	215	True
11111110	236	True
11101110	239	False
11111100	244	False

Number of codes used=18


End of hp run 6.  Result of run:
[(-0.9556915284576393, 24997), ('21-05-15_22:28:32BST_NLearn_model_6_Alice_iter25000', '21-05-15_22:28:32BST_NLearn_model_6_Bob_iter25000')]
(-0.9556915284576393, 24997)


>>>> hp_run=7 of 16, time elapsed 3:18:21 of estimated 8:48:56, 
implying ending at 07:17:29BST on Sunday 16 May 2021
hyperparameters = {
	'N_ITERATIONS': 25000,
	'RANDOM_SEEDS': (911011, 571019, 667157, 225093),
	'ALICE_NET': 'MaxNet("In", 3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 228571,
	'START_TRAINING': 7142,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 714,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 7142,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 10714,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=      7142 training nets give:
alice_loss.item()=0.6214711666107178	bob_loss.item()=0.706329345703125

01100011	[0, 1, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00010011	[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
01110010	[16, 17, 18, 19, 20]
11110111	[21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]
00100101	[46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115]
01111100	[116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179]
10010010	[180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223]

01100011	106	False
00010011	2	True
01110010	92	False
11110111	82	False
00100101	82	True
01111100	122	True
10010010	81	False

Number of codes used=7

Iteration=     10713 training nets give:
alice_loss.item()=0.18908096849918365	bob_loss.item()=0.20387056469917297

00010011	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11110111	[39, 40, 41, 42, 43]
00100101	[44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102]
01111100	[103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158]
00011010	[159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227]

00010011	242	True
11110111	13	False
00100101	78	True
01111100	130	True
00011010	207	True

Number of codes used=5

Iteration=     14284 training nets give:
alice_loss.item()=0.05605505406856537	bob_loss.item()=0.1668613851070404

00010011	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01010011	[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
00000101	[36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60]
01111101	[61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117]
01111100	[118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144]
00111100	[145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155]
00011000	[156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171]
01011010	[172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203]
00011010	[204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224]
00111010	[225, 226, 227, 228, 229, 230, 231, 232, 233]

00010011	239	True
01010011	4	False
00000101	62	False
01111101	124	False
01111100	142	True
00111100	128	False
00011000	186	False
01011010	189	True
00011010	189	False
00111010	199	False

Number of codes used=10

Iteration=     17855 training nets give:
alice_loss.item()=0.11684370040893555	bob_loss.item()=0.13912709057331085

01010011	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00000111	[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]
10110101	[45, 46, 47, 48, 49]
00110101	[50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
00100101	[62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85]
00101101	[86, 87, 88, 89, 90, 91, 92, 93, 94]
00111100	[95, 96, 97, 98, 99]
00111110	[100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
01111100	[128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144]
11011000	[145, 146, 147, 148]
01111000	[149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171]
00011000	[172, 173, 174, 175, 176, 177, 178, 179]
01011010	[180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194]
01011000	[195, 196, 197]
00011010	[198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208]
00010010	[209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220]
00001010	[221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231]
00010011	[232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246]

01010011	11	True
00000111	33	True
10110101	72	False
00110101	69	False
00100101	73	True
00101101	84	False
00111100	126	False
00111110	119	True
01111100	134	True
11011000	175	False
01111000	147	False
00011000	181	False
01011010	176	False
01011000	174	False
00011010	220	False
00010010	243	False
00001010	234	False
00010011	237	True

Number of codes used=18

Iteration=     21426 training nets give:
alice_loss.item()=0.023284640163183212	bob_loss.item()=0.029130835086107254

01010011	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 252, 253, 254, 255]
00000011	[11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]
00000111	[23, 24, 25, 26, 27, 28, 29, 30, 31, 32]
10110111	[33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]
11110110	[47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57]
00110101	[58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]
00100101	[70, 71, 72, 73, 74, 75, 76, 77, 78]
00101101	[79, 80, 81, 82, 83, 84, 85, 86]
10101101	[87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101]
01110100	[102]
00100100	[103, 104, 105, 106, 107]
00111110	[108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123]
01111101	[124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147]
01111000	[148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160]
01101101	[161, 162]
00011000	[163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187]
11011010	[188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202]
10011010	[203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221]
10010011	[222, 223, 224, 225, 226]
00010011	[227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239]
00011011	[240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251]

01010011	10	True
00000011	17	True
00000111	34	False
10110111	42	True
11110110	58	False
00110101	68	True
00100101	73	True
00101101	83	True
10101101	88	True
01110100	109	False
00100100	95	False
00111110	115	True
01111101	116	False
01111000	143	False
01101101	109	False
00011000	176	True
11011010	200	True
10011010	204	True
10010011	240	False
00010011	237	True
00011011	248	True

Number of codes used=21

Iteration=     24997 training nets give:
alice_loss.item()=0.007057092152535915	bob_loss.item()=0.007856292650103569

01010011	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
00000011	[14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
11110111	[24, 25, 26, 27, 28, 29, 30]
00000111	[31, 32, 33, 34, 35, 36, 37, 38, 39, 40]
10110111	[41, 42, 43, 44, 45, 46, 47, 48, 49, 50]
11110110	[51, 52, 53, 54, 55, 56, 57, 58, 59, 60]
00000101	[61, 62, 63, 64, 65, 66, 67, 68]
00100101	[69, 70, 71, 72, 73, 74, 75, 76]
00101101	[77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90]
10101101	[91, 92, 93, 94]
00111101	[95, 96]
00101100	[97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112]
00111110	[113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125]
01111100	[126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137]
00011100	[138]
01111000	[139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160]
00011000	[161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187]
11011010	[188, 189, 190, 191, 192, 193, 194, 195]
10011010	[196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217]
11011011	[218, 219, 220, 221, 222, 223, 224, 225, 226, 227]
00010011	[228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241]
00011011	[242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]

01010011	7	True
00000011	17	True
11110111	31	False
00000111	33	True
10110111	41	True
11110110	51	True
00000101	61	True
00100101	71	True
00101101	81	True
10101101	85	False
00111101	83	False
00101100	104	True
00111110	109	False
01111100	134	True
00011100	145	False
01111000	146	True
00011000	178	True
11011010	200	False
10011010	207	True
11011011	230	False
00010011	234	True
00011011	245	True

Number of codes used=22


End of hp run 7.  Result of run:
[(-0.9818689807085994, 24997), ('21-05-15_22:28:32BST_NLearn_model_7_Alice_iter25000', '21-05-15_22:28:32BST_NLearn_model_7_Bob_iter25000')]
(-0.9818689807085994, 24997)


>>>> hp_run=8 of 16, time elapsed 3:57:35 of estimated 9:03:03, 
implying ending at 07:31:36BST on Sunday 16 May 2021
hyperparameters = {
	'N_ITERATIONS': 25000,
	'RANDOM_SEEDS': (911011, 571019, 667157, 225093),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 228571,
	'START_TRAINING': 7142,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 714,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 7142,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 10714,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=      7142 training nets give:
alice_loss.item()=0.6521692872047424	bob_loss.item()=0.7044067978858948

11101000	[0, 1, 2, 3, 4, 5, 6, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01101010	[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]
00100101	[33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77]
11101110	[78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104]

11101000	181	True
01101010	215	False
00100101	205	False
11101110	217	False

Number of codes used=4

Iteration=     10713 training nets give:
alice_loss.item()=0.2052234411239624	bob_loss.item()=0.3421092629432678

01010100	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10000101	[40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 133, 134, 135, 136]
11101101	[70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132]
11101000	[137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204]

01010100	253	True
10000101	68	True
11101101	80	True
11101000	164	True

Number of codes used=4

Iteration=     14284 training nets give:
alice_loss.item()=0.10036467760801315	bob_loss.item()=0.17444759607315063

01010100	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01010101	[18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233]
01110100	[31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]
10000001	[50, 51, 52, 53, 54, 55]
10000101	[56, 57, 58, 59, 60, 61, 62, 63, 64]
10100101	[65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87]
11101101	[88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118]
11100000	[119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139]
11001000	[140, 141, 142, 143, 144, 145, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190]
11101000	[146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180]
11101001	[191, 192, 193, 194, 195]
01101000	[196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]

01010100	254	True
01010101	255	False
01110100	3	False
10000001	53	True
10000101	50	False
10100101	64	False
11101101	80	False
11100000	174	False
11001000	169	False
11101000	156	True
11101001	170	False
01101000	177	False

Number of codes used=12

Iteration=     17855 training nets give:
alice_loss.item()=0.1344520002603531	bob_loss.item()=0.207127183675766

01010100	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 253, 254, 255]
00000101	[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]
01000100	[27, 28, 29]
10000111	[30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]
10000101	[46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
11000001	[62, 63, 64]
10100101	[65, 66, 67, 68]
10101101	[69, 70, 71, 72, 73, 74, 75, 76, 77, 78]
11101100	[79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
10101000	[115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134]
11101000	[135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167]
11100000	[168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189]
11111000	[190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213]
01101010	[214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226]
11010110	[227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252]

01010100	251	False
00000101	42	False
01000100	5	False
10000111	46	False
10000101	57	True
11000001	67	False
10100101	68	True
10101101	85	False
11101100	100	True
10101000	138	False
11101000	151	True
11100000	162	False
11111000	177	False
01101010	190	False
11010110	242	True

Number of codes used=15

Iteration=     21426 training nets give:
alice_loss.item()=0.05496058613061905	bob_loss.item()=0.12666349112987518

01010111	[0, 1, 2, 3, 4, 5, 6, 7, 255]
01100100	[8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]
10000111	[27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43]
10000101	[44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54]
11000001	[55, 56, 57, 58, 59, 60, 61]
10100101	[62, 63, 64, 65, 66, 67, 68, 69]
10100100	[70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84]
10101100	[85, 86, 87, 88]
11101100	[89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117]
10101000	[118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
11101000	[144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156]
11101010	[157, 158, 159, 160]
11001000	[161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171]
01101000	[172, 173, 174, 175, 176, 177, 178, 179, 180, 181]
11111000	[182, 183, 184, 185, 186, 187, 188, 189]
01111000	[190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214]
01011000	[215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238]
11010110	[239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249]
01010100	[250, 251, 252, 253, 254]

01010111	255	True
01100100	10	True
10000111	41	True
10000101	58	False
11000001	67	False
10100101	70	False
10100100	71	True
10101100	95	False
11101100	102	True
10101000	139	True
11101000	152	True
11101010	162	False
11001000	164	True
01101000	184	False
11111000	168	False
01111000	198	True
01011000	228	True
11010110	249	True
01010100	251	True

Number of codes used=19

Iteration=     24997 training nets give:
alice_loss.item()=0.0005302335484884679	bob_loss.item()=0.00218211580067873

01010101	[0, 1, 2, 3, 253, 254, 255]
01100100	[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]
10000111	[28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]
10000101	[49, 50, 51, 52, 53, 54, 55]
10000001	[56, 57, 58, 59, 60, 61, 62, 63, 64, 65]
10100101	[66, 67, 68]
10100100	[69, 70, 71, 72, 73, 74, 75, 76, 77]
11001100	[78, 79, 80, 81, 82, 83, 84, 85]
10101100	[86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96]
11101100	[97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120]
10101000	[121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145]
11101000	[146, 147, 148, 149, 150, 151, 152, 153, 154, 155]
11101010	[156, 157, 158, 159, 160]
11001000	[161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174]
01101000	[175, 176, 177]
01101001	[178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190]
01111000	[191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212]
01011000	[213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238]
11000110	[239]
01011100	[240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250]
01010110	[251, 252]

01010101	2	True
01100100	13	True
10000111	42	True
10000101	57	False
10000001	61	True
10100101	75	False
10100100	73	True
11001100	86	False
10101100	90	True
11101100	102	True
10101000	139	True
11101000	156	False
11101010	162	False
11001000	165	True
01101000	185	False
01101001	189	True
01111000	198	True
01011000	228	True
11000110	246	False
01011100	248	True
01010110	253	False

Number of codes used=21


End of hp run 8.  Result of run:
[(-0.9803222702591821, 24997), ('21-05-15_22:28:32BST_NLearn_model_8_Alice_iter25000', '21-05-15_22:28:32BST_NLearn_model_8_Bob_iter25000')]
(-0.9803222702591821, 24997)


>>>> hp_run=9 of 16, time elapsed 4:24:50 of estimated 8:49:39, 
implying ending at 07:18:12BST on Sunday 16 May 2021
hyperparameters = {
	'N_ITERATIONS': 25000,
	'RANDOM_SEEDS': (335581, 265392, 137411, 842014),
	'ALICE_NET': 'MaxNet("In", 3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 228571,
	'START_TRAINING': 7142,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 714,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 7142,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 10714,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=      7142 training nets give:
alice_loss.item()=0.7400696873664856	bob_loss.item()=0.5266748070716858

11000100	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11000111	[27, 28, 29]
01110010	[30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
11111100	[52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101]
10111101	[102, 103, 104]
00100000	[105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128]
01011101	[129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166]
00001001	[167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194]

11000100	67	False
11000111	2	False
01110010	192	False
11111100	243	False
10111101	204	False
00100000	229	False
01011101	89	False
00001001	156	False

Number of codes used=8

Iteration=     10713 training nets give:
alice_loss.item()=0.16433942317962646	bob_loss.item()=0.3674028515815735

10100110	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01100000	[78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134]
00001001	[135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203]
00001101	[204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235]

10100110	34	True
01100000	111	True
00001001	153	True
00001101	184	False

Number of codes used=4

Iteration=     14284 training nets give:
alice_loss.item()=0.11870904266834259	bob_loss.item()=0.18829849362373352

11100110	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10100110	[24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
01100001	[48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67]
01110000	[68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93]
01100000	[94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132]
00001001	[133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169]
00001101	[170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207]
01001101	[208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232]

11100110	13	True
10100110	13	False
01100001	97	False
01110000	101	False
01100000	90	False
00001001	182	False
00001101	199	True
01001101	202	False

Number of codes used=8

Iteration=     17855 training nets give:
alice_loss.item()=0.14333027601242065	bob_loss.item()=0.17520444095134735

11100110	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11101110	[21, 22, 23]
10000100	[24, 25, 26, 27, 28, 29]
10100010	[30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66]
01111000	[67, 68]
01100100	[69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]
01110000	[90, 91, 92, 93, 94, 95, 96, 97, 120, 121, 122, 123, 124]
01100000	[98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]
00000001	[125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170]
01000101	[171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182]
00001101	[183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198]
01001101	[199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214]
01001111	[215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232]
01101101	[233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245]

11100110	1	True
11101110	234	False
10000100	15	False
10100010	40	True
01111000	107	False
01100100	85	True
01110000	106	False
01100000	111	True
00000001	155	True
01000101	200	False
00001101	203	False
01001101	209	True
01001111	223	True
01101101	200	False

Number of codes used=14

Iteration=     21426 training nets give:
alice_loss.item()=0.06022088974714279	bob_loss.item()=0.0827416181564331

11100110	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 251, 252, 253, 254, 255]
10100110	[10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]
10110111	[23, 24, 25, 26, 27, 28]
10100010	[29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]
01101100	[49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70]
01100100	[71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92]
01100000	[93, 94, 95]
01110000	[96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130]
00000001	[131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170]
00001001	[171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181]
01000101	[182, 183, 184]
00001101	[185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208]
01001101	[209, 210, 211, 212]
01001111	[213, 214, 215, 216, 217, 218, 219]
01001110	[220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241]
10001110	[242, 243, 244]
10001111	[245, 246, 247, 248, 249, 250]

11100110	11	False
10100110	18	True
10110111	17	False
10100010	47	True
01101100	57	True
01100100	72	True
01100000	110	False
01110000	103	True
00000001	160	True
00001001	167	False
01000101	205	False
00001101	202	True
01001101	213	False
01001111	224	False
01001110	235	True
10001110	235	False
10001111	234	False

Number of codes used=17

Iteration=     24997 training nets give:
alice_loss.item()=0.001579747418873012	bob_loss.item()=0.007684278301894665

10100111	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 251, 252, 253, 254, 255]
10100110	[11, 12, 13, 14, 15, 16]
10110111	[17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]
10100010	[35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]
01101100	[51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68]
01100100	[69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86]
01100001	[87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103]
01110000	[104, 105, 106]
01000000	[107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118]
01000001	[119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142]
00000001	[143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160]
00001001	[161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173]
00000011	[174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187]
00001101	[188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205]
01001101	[206, 207, 208, 209, 210, 211, 212, 213]
01001111	[214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227]
01001110	[228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240]
10001110	[241, 242, 243, 244, 245, 246, 247, 248, 249, 250]

10100111	5	True
10100110	21	False
10110111	22	True
10100010	46	True
01101100	57	True
01100100	74	True
01100001	93	True
01110000	100	False
01000000	107	True
01000001	122	True
00000001	163	False
00001001	169	True
00000011	169	False
00001101	201	True
01001101	217	False
01001111	225	True
01001110	238	True
10001110	243	True

Number of codes used=18


End of hp run 9.  Result of run:
[(-0.9724973283202389, 24997), ('21-05-15_22:28:32BST_NLearn_model_9_Alice_iter25000', '21-05-15_22:28:32BST_NLearn_model_9_Bob_iter25000')]
(-0.9724973283202389, 24997)


>>>> hp_run=10 of 16, time elapsed 5:03:51 of estimated 9:00:11, 
implying ending at 07:28:43BST on Sunday 16 May 2021
hyperparameters = {
	'N_ITERATIONS': 25000,
	'RANDOM_SEEDS': (335581, 265392, 137411, 842014),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 228571,
	'START_TRAINING': 7142,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 714,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 7142,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 10714,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=      7142 training nets give:
alice_loss.item()=0.6162468791007996	bob_loss.item()=0.5632638335227966

11100001	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10011010	[93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167]
11111110	[168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222]
11110110	[223, 224, 225, 226, 227, 228, 229, 230, 231]

11100001	64	True
10011010	78	False
11111110	91	False
11110110	97	False

Number of codes used=4

Iteration=     10713 training nets give:
alice_loss.item()=0.1955561637878418	bob_loss.item()=0.3381722569465637

11110111	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11100001	[19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75]
10011010	[76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117]
00011010	[118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187]

11110111	222	True
11100001	45	True
10011010	120	False
00011010	130	True

Number of codes used=4

Iteration=     14284 training nets give:
alice_loss.item()=0.12371499091386795	bob_loss.item()=0.2265554964542389

11100000	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 250, 251, 252, 253, 254, 255]
11100001	[40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62]
11101001	[63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82]
10011010	[83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101]
00011011	[102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125]
00011010	[126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151]
00111010	[152, 153, 154, 155, 156, 157, 158, 159, 160, 161]
00010010	[162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183]
11110101	[184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208]
11110111	[209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249]

11100000	32	True
11100001	17	False
11101001	39	False
10011010	109	False
00011011	129	False
00011010	141	True
00111010	135	False
00010010	134	False
11110101	216	False
11110111	209	True

Number of codes used=10

Iteration=     17855 training nets give:
alice_loss.item()=0.1760702133178711	bob_loss.item()=0.1984647810459137

11100010	[0, 1, 2, 3, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11100000	[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]
11100001	[27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]
01100000	[42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85]
10100000	[86, 87, 88, 89, 90]
10011000	[91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104]
10011010	[105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
00011011	[128, 129, 130]
00011010	[131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151]
01011110	[152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168]
11110110	[169, 170, 171]
11111111	[172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192]
10110111	[193]
11111101	[194, 195, 196, 197, 198, 199, 202]
11010111	[200, 201]
11110111	[203, 204, 205, 206, 207, 208, 209, 210, 211, 212]
11110101	[213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234]
11110011	[235, 236, 237, 238, 239, 240, 241, 242]

11100010	17	False
11100000	44	False
11100001	51	False
01100000	38	False
10100000	67	False
10011000	99	True
10011010	99	False
00011011	127	False
00011010	147	True
01011110	151	False
11110110	205	False
11111111	193	False
10110111	201	False
11111101	194	True
11010111	196	False
11110111	209	True
11110101	212	False
11110011	214	False

Number of codes used=18

Iteration=     21426 training nets give:
alice_loss.item()=0.08239784836769104	bob_loss.item()=0.08171363174915314

11000000	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11100000	[37, 38]
11101001	[39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]
10111010	[51, 52]
10100000	[53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76]
10011000	[77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101]
10011010	[102, 103, 104, 105, 106]
00011001	[107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123]
00011011	[124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135]
00011110	[136]
00011010	[137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151]
01011110	[152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169]
10111111	[170, 171, 172, 173, 174, 175, 176, 177, 178]
01110101	[179, 180, 181, 182]
11111111	[183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194]
10110101	[195, 196, 197, 198, 199, 200, 201, 202]
11110111	[203, 204, 205, 206, 207, 208, 209]
11110101	[210, 211, 212, 213, 214, 215, 216]
11110011	[217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242]

11000000	20	True
11100000	28	False
11101001	50	True
10111010	90	False
10100000	68	True
10011000	98	True
10011010	98	False
00011001	113	True
00011011	125	True
00011110	147	False
00011010	150	True
01011110	160	True
10111111	184	False
01110101	193	False
11111111	196	False
10110101	194	False
11110111	212	False
11110101	212	True
11110011	216	False

Number of codes used=19

Iteration=     24997 training nets give:
alice_loss.item()=0.002825514180585742	bob_loss.item()=0.02459346316754818

11000000	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11100000	[32, 33, 34, 35, 36, 37, 38, 39, 40]
11101001	[41, 42, 43]
11100001	[44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57]
10100000	[58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77]
10101000	[78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88]
10011000	[89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108]
00011001	[109, 110, 111, 112, 113, 114, 115, 116, 117]
00001011	[118, 119, 120, 121, 122]
00011011	[123, 124, 125, 126, 127, 128, 129]
00011000	[130, 131, 132, 133]
00010010	[134, 135, 136, 137, 138, 139, 140]
00010110	[141, 142]
00011010	[143, 144, 145, 146, 147, 148, 149, 150, 151]
01011110	[152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170]
10111111	[171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191]
11111101	[192, 193, 194, 195, 196]
11010111	[197, 198, 199]
10110111	[200, 201, 202, 203, 204]
11110100	[205, 206, 207, 208, 209, 210, 211]
11110101	[212, 213, 214, 215, 216]
11110011	[217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244]

11000000	16	True
11100000	22	False
11101001	48	False
11100001	46	True
10100000	72	True
10101000	76	False
10011000	101	True
00011001	120	False
00001011	118	True
00011011	128	True
00011000	133	True
00010010	140	True
00010110	147	False
00011010	151	True
01011110	160	True
10111111	178	True
11111101	186	False
11010111	198	True
10110111	200	True
11110100	210	True
11110101	200	False
11110011	218	True

Number of codes used=22


End of hp run 10.  Result of run:
[(-0.9628887424154849, 24997), ('21-05-15_22:28:32BST_NLearn_model_10_Alice_iter25000', '21-05-15_22:28:32BST_NLearn_model_10_Bob_iter25000')]
(-0.9628887424154849, 24997)


>>>> hp_run=11 of 16, time elapsed 5:30:58 of estimated 8:49:33, 
implying ending at 07:18:05BST on Sunday 16 May 2021
hyperparameters = {
	'N_ITERATIONS': 25000,
	'RANDOM_SEEDS': (307035, 405050, 968633, 690674),
	'ALICE_NET': 'MaxNet("In", 3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 228571,
	'START_TRAINING': 7142,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 714,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 7142,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 10714,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=      7142 training nets give:
alice_loss.item()=0.6279585361480713	bob_loss.item()=0.601618766784668

11010011	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 248, 249, 250, 251, 252, 253, 254, 255]
10001111	[40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82]
00111001	[83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130]
01010101	[131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247]

11010011	174	False
10001111	111	False
00111001	193	False
01010101	193	True

Number of codes used=4

Iteration=     10713 training nets give:
alice_loss.item()=0.10115937888622284	bob_loss.item()=0.18295645713806152

11010111	[0, 1, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11011000	[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68]
00001000	[69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126]
10100101	[127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154]
01010101	[155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226]

11010111	217	False
11011000	41	True
00001000	79	True
10100101	186	False
01010101	197	True

Number of codes used=5

Iteration=     14284 training nets give:
alice_loss.item()=0.08523249626159668	bob_loss.item()=0.23976001143455505

11011001	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 253, 254, 255]
11011000	[27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56]
00000000	[57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]
00001000	[73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94]
00101000	[95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135]
00001010	[136, 137, 138, 139, 140, 141, 142]
01000101	[143, 144, 145, 146]
10100101	[147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200]
01010100	[201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225]
01110101	[226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252]

11011001	36	False
11011000	34	True
00000000	85	False
00001000	88	True
00101000	101	True
00001010	91	False
01000101	205	False
10100101	177	True
01010100	189	False
01110101	195	False

Number of codes used=10

Iteration=     17855 training nets give:
alice_loss.item()=0.1953442543745041	bob_loss.item()=0.23006178438663483

11011011	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11011001	[21, 22, 47]
11011000	[23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]
11011100	[48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80]
00001000	[81, 82, 83]
10101000	[84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109]
00101000	[110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122]
00111000	[123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134]
00100101	[135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172]
10100101	[173, 174, 175, 176, 177, 178, 179, 180, 181]
01100101	[182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207]
11011111	[208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239]

11011011	16	True
11011001	32	False
11011000	20	False
11011100	57	True
00001000	90	False
10101000	103	True
00101000	106	False
00111000	97	False
00100101	152	True
10100101	163	False
01100101	161	False
11011111	218	True

Number of codes used=12

Iteration=     21426 training nets give:
alice_loss.item()=0.09019484370946884	bob_loss.item()=0.09359408915042877

11011110	[0, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11011011	[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]
11011000	[27, 28]
11001000	[29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]
11011100	[47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62]
01011010	[63]
01010000	[64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76]
10001001	[77, 78, 79, 80]
10001100	[81, 82]
00001000	[83, 84, 85, 86, 87]
00001101	[88, 89]
00001110	[90, 91, 92, 93, 94, 95, 96, 97, 98]
00101010	[99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112]
00100100	[113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141]
00100101	[142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167]
10100101	[168, 169, 170, 171, 172, 173, 174, 175, 176]
01110101	[177, 178, 179, 180, 181, 182, 183, 184]
01000111	[185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195]
01010111	[196, 197]
01011101	[198, 199, 200, 201]
01010101	[202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213]
11010111	[214]
11011111	[215, 216, 217, 218, 219, 220, 221, 222]

11011110	242	True
11011011	3	True
11011000	18	False
11001000	28	False
11011100	59	True
01011010	57	False
01010000	69	True
10001001	76	False
10001100	90	False
00001000	92	False
00001101	93	False
00001110	94	True
00101010	110	True
00100100	131	True
00100101	155	True
10100101	167	False
01110101	177	True
01000111	191	True
01010111	203	False
01011101	202	False
01010101	195	False
11010111	217	False
11011111	234	False

Number of codes used=23

Iteration=     24997 training nets give:
alice_loss.item()=0.0010579725494608283	bob_loss.item()=0.006352149415761232

11011110	[0, 1, 2, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11011011	[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]
11011000	[18, 19, 20, 21, 22, 23, 24, 25, 26]
11001000	[27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]
11011100	[43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
01010000	[62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]
10001001	[73, 74, 75, 76, 77]
00000000	[78, 79, 80, 81, 82, 83, 84, 85, 86, 87]
00001110	[88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
00101010	[100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]
00100100	[120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
00100101	[144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156]
10100101	[157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168]
10110101	[169, 170, 171, 172, 173, 174, 175, 176]
01110101	[177, 178, 179, 180, 181, 182, 183, 184]
01000111	[185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195]
01011101	[196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207]
11010111	[208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219]
11011111	[220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231]

11011110	244	True
11011011	9	True
11011000	28	False
11001000	26	False
11011100	59	True
01010000	67	True
10001001	79	False
00000000	82	True
00001110	94	True
00101010	109	True
00100100	131	True
00100101	151	True
10100101	168	True
10110101	169	True
01110101	179	True
01000111	194	True
01011101	202	True
11010111	218	True
11011111	222	True

Number of codes used=19


End of hp run 11.  Result of run:
[(-0.9824693963587671, 24997), ('21-05-15_22:28:32BST_NLearn_model_11_Alice_iter25000', '21-05-15_22:28:32BST_NLearn_model_11_Bob_iter25000')]
(-0.9824693963587671, 24997)


>>>> hp_run=12 of 16, time elapsed 6:09:44 of estimated 8:57:48, 
implying ending at 07:26:20BST on Sunday 16 May 2021
hyperparameters = {
	'N_ITERATIONS': 25000,
	'RANDOM_SEEDS': (307035, 405050, 968633, 690674),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 228571,
	'START_TRAINING': 7142,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 714,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 7142,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 10714,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=      7142 training nets give:
alice_loss.item()=0.5768299102783203	bob_loss.item()=0.624523937702179

01000001	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00111011	[29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
00011001	[52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156]
00110111	[157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169]
00001001	[170, 171, 172, 173, 174, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235]
10011011	[175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224]

01000001	230	False
00111011	131	False
00011001	134	True
00110111	113	False
00001001	74	False
10011011	121	False

Number of codes used=6

Iteration=     10713 training nets give:
alice_loss.item()=0.1832626610994339	bob_loss.item()=0.2191709578037262

01000001	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11101100	[24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38]
11100001	[39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56]
00110111	[57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88]
00011001	[89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181]

01000001	255	True
11101100	42	False
11100001	15	False
00110111	104	False
00011001	129	True

Number of codes used=5

Iteration=     14284 training nets give:
alice_loss.item()=0.2268589437007904	bob_loss.item()=0.26619982719421387

01000001	[0, 1, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01000011	[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
01001001	[16, 17, 18]
11101100	[19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75]
10011001	[76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 170, 171, 172, 173, 174, 175, 176]
00110111	[90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116]
00011001	[117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169]
00111111	[177, 178, 179, 180, 181, 182, 183, 184, 185, 186]
01000101	[187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197]

01000001	242	True
01000011	254	False
01001001	188	False
11101100	47	True
10011001	138	False
00110111	87	False
00011001	122	True
00111111	108	False
01000101	238	False

Number of codes used=9

Iteration=     17855 training nets give:
alice_loss.item()=0.13712334632873535	bob_loss.item()=0.19163104891777039

01000001	[0, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01000011	[1, 2, 3, 13, 14, 15]
11101110	[4, 5, 6, 7]
11000001	[8, 9, 10, 11, 12]
01100011	[16]
11001100	[17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
11101100	[31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54]
11101101	[55, 56, 57, 58, 59, 60, 61, 62, 63]
10101100	[64, 65, 66, 67, 68]
01011101	[69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]
00110101	[80, 81, 82, 83, 84]
00110111	[85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
00111001	[100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
00011001	[115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131]
00011011	[132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192]
01001011	[193, 194, 195, 196, 197, 198]
01000101	[199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241]

01000001	234	False
01000011	0	False
11101110	9	False
11000001	229	False
01100011	6	False
11001100	25	True
11101100	49	True
11101101	48	False
10101100	31	False
01011101	136	False
00110101	86	False
00110111	85	True
00111001	119	False
00011001	140	False
00011011	151	True
01001011	233	False
01000101	230	True

Number of codes used=17

Iteration=     21426 training nets give:
alice_loss.item()=0.04095642268657684	bob_loss.item()=0.09659511595964432

01000010	[0, 1, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01000011	[2, 3, 4, 5, 6, 7, 8]
01100011	[9]
11101000	[10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
11100100	[32, 33, 34, 35]
01101100	[36, 37, 38]
11101101	[39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65]
11111100	[66, 67, 68, 69, 70]
00110101	[71, 72, 73, 74, 75, 76, 77, 78, 79, 80]
00110111	[81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97]
00111111	[98, 99, 100, 101, 102, 103]
00111101	[104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116]
00111001	[117, 118, 119, 120, 121]
01011001	[122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136]
10011001	[137, 138, 139, 140, 141, 142, 143, 144, 145]
00011011	[146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191]
01000101	[192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235]
01000001	[236, 237, 238, 239, 240, 241, 242, 243, 244]

01000010	252	True
01000011	247	False
01100011	255	False
11101000	23	True
11100100	27	False
01101100	30	False
11101101	58	True
11111100	49	False
00110101	89	False
00110111	95	True
00111111	115	False
00111101	114	True
00111001	132	False
01011001	139	False
10011001	150	False
00011011	158	True
01000101	227	True
01000001	231	False

Number of codes used=18

Iteration=     24997 training nets give:
alice_loss.item()=0.05330300331115723	bob_loss.item()=0.03837057203054428

01000010	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11101110	[10, 11, 12]
11101000	[13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
01101100	[31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]
11101100	[43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]
11101101	[54, 55, 56, 57, 58, 59, 60]
10111100	[61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82]
00110101	[83, 84]
00110111	[85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96]
00111000	[97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107]
00111101	[108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126]
01011001	[127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146]
00011011	[147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164]
10011011	[165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]
01000101	[200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237]
01000001	[238, 239, 240, 241]

01000010	253	True
11101110	19	False
11101000	22	True
01101100	32	True
11101100	48	True
11101101	62	False
10111100	71	True
00110101	84	True
00110111	95	True
00111000	93	False
00111101	111	True
01011001	135	True
00011011	161	True
10011011	175	True
01000101	228	True
01000001	230	False

Number of codes used=16


End of hp run 12.  Result of run:
[(-0.9686548308012961, 24997), ('21-05-15_22:28:32BST_NLearn_model_12_Alice_iter25000', '21-05-15_22:28:32BST_NLearn_model_12_Bob_iter25000')]
(-0.9686548308012961, 24997)


>>>> hp_run=13 of 16, time elapsed 6:36:52 of estimated 8:49:09, 
implying ending at 07:17:41BST on Sunday 16 May 2021
hyperparameters = {
	'N_ITERATIONS': 25000,
	'RANDOM_SEEDS': (577683, 443890, 562139, 319257),
	'ALICE_NET': 'MaxNet("In", 3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 228571,
	'START_TRAINING': 7142,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 714,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 7142,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 10714,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=      7142 training nets give:
alice_loss.item()=0.6068688631057739	bob_loss.item()=0.6416081786155701

00000000	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01111000	[52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]
10010110	[73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
10010011	[84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122]
11111111	[123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197]

00000000	230	True
01111000	252	False
10010110	234	False
10010011	216	False
11111111	217	False

Number of codes used=5

Iteration=     10713 training nets give:
alice_loss.item()=0.06569705158472061	bob_loss.item()=0.12200791388750076

01111000	[0, 1, 2, 3, 4, 5, 6, 7, 8, 250, 251, 252, 253, 254, 255]
01011110	[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]
11011011	[75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154]
11010111	[155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166]
11111111	[167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203]
00000000	[204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249]

01111000	29	False
01011110	33	True
11011011	123	True
11010111	131	False
11111111	171	True
00000000	246	True

Number of codes used=6

Iteration=     14284 training nets give:
alice_loss.item()=0.18079225718975067	bob_loss.item()=0.11873947829008102

00000000	[0, 1, 2, 3, 4, 5, 6, 7, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01011110	[8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]
01011100	[41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]
01011011	[65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109]
11011010	[110, 111, 112]
11010111	[113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155]
11110111	[156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208]

00000000	244	True
01011110	32	True
01011100	46	True
01011011	84	True
11011010	93	False
11010111	143	True
11110111	182	True

Number of codes used=7

Iteration=     17855 training nets give:
alice_loss.item()=0.12923456728458405	bob_loss.item()=0.17401939630508423

00000100	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
00000010	[15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]
01011110	[27, 28, 29, 30, 31, 32]
01111100	[33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]
01011100	[49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]
01011011	[65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84]
11011010	[85, 86, 87, 88, 89, 90, 91]
11011001	[92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112]
11011011	[113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130]
11001011	[131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152]
01010111	[153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165]
11111111	[166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179]
11101111	[180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218]
00000001	[219, 220, 221, 222]
00000000	[223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]

00000100	255	False
00000010	244	False
01011110	29	True
01111100	39	True
01011100	49	True
01011011	70	True
11011010	86	True
11011001	106	True
11011011	114	True
11001011	128	False
01010111	155	True
11111111	168	True
11101111	184	True
00000001	239	False
00000000	236	True

Number of codes used=15

Iteration=     21426 training nets give:
alice_loss.item()=0.1404288411140442	bob_loss.item()=0.0822591558098793

00000100	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 250, 251, 252, 253, 254, 255]
01001110	[14, 15, 16, 17, 18, 19, 20, 21, 22]
01011110	[23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]
01111100	[38, 39, 40]
01011000	[41, 42, 43, 44, 45, 46, 47, 48, 49, 50]
01111110	[51, 52, 53, 54, 55, 56, 57, 58]
00011011	[59, 60, 61, 62, 63, 64, 65, 66]
01011011	[67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80]
11011010	[81, 82, 83, 84, 85, 86, 87, 88]
10011001	[89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104]
11011001	[105, 106, 107, 108, 109, 110]
11011011	[111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123]
10011011	[124, 125, 126, 127, 128, 129, 130, 131]
11010011	[132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144]
11010111	[145, 146, 147, 148, 149, 150]
11111110	[151, 152]
10010011	[153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169]
11111111	[170, 171, 172, 173]
11101111	[174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207]
10101111	[208]
00100001	[209, 210, 211, 212, 213, 214, 215]
00000001	[216, 217, 218, 219, 220, 221, 222, 223]
00000000	[224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249]

00000100	2	True
01001110	25	False
01011110	28	True
01111100	46	False
01011000	51	False
01111110	55	True
00011011	85	False
01011011	75	True
11011010	85	True
10011001	98	True
11011001	103	False
11011011	115	True
10011011	120	False
11010011	145	False
11010111	153	False
11111110	142	False
10010011	156	True
11111111	165	False
11101111	183	True
10101111	178	False
00100001	234	False
00000001	236	False
00000000	236	True

Number of codes used=23

Iteration=     24997 training nets give:
alice_loss.item()=0.0014554429799318314	bob_loss.item()=0.003919381648302078

00000100	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 247, 248, 249, 250, 251, 252, 253, 254, 255]
01001110	[13, 14, 15, 16, 17, 18, 19, 20]
01011110	[21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]
01111100	[38]
01011100	[39, 40, 41, 42, 43, 44, 45, 46, 47]
01011000	[48, 49, 50, 51, 52, 53, 54]
01011010	[55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66]
01011011	[67, 68, 69, 70, 71, 72, 73, 74, 75]
11011010	[76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91]
10011001	[92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106]
11011001	[107, 108, 109, 110, 111, 112]
10011011	[113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129]
11001011	[130, 131, 132, 133, 134, 135, 136]
11010011	[137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149]
11010111	[150, 151]
01010111	[152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165]
11111111	[166, 167, 168, 169, 170, 171, 172, 173, 174]
11101111	[175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210]
00100001	[211, 212, 213, 214, 215, 216]
00000000	[217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243]
00000001	[244, 245, 246]

00000100	2	True
01001110	26	False
01011110	27	True
01111100	46	False
01011100	54	False
01011000	53	True
01011010	61	True
01011011	80	False
11011010	82	True
10011001	98	True
11011001	102	False
10011011	123	True
11001011	131	True
11010011	147	True
11010111	153	False
01010111	157	True
11111111	167	True
11101111	188	True
00100001	233	False
00000000	238	True
00000001	238	False

Number of codes used=21


End of hp run 13.  Result of run:
[(-0.9724223064918305, 24997), ('21-05-15_22:28:32BST_NLearn_model_13_Alice_iter25000', '21-05-15_22:28:32BST_NLearn_model_13_Bob_iter25000')]
(-0.9724223064918305, 24997)


>>>> hp_run=14 of 16, time elapsed 7:15:34 of estimated 8:56:04, 
implying ending at 07:24:37BST on Sunday 16 May 2021
hyperparameters = {
	'N_ITERATIONS': 25000,
	'RANDOM_SEEDS': (577683, 443890, 562139, 319257),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 228571,
	'START_TRAINING': 7142,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 714,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 7142,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 10714,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=      7142 training nets give:
alice_loss.item()=0.5042502880096436	bob_loss.item()=0.5248094797134399

00111010	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00001111	[31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 70, 71, 72]
10110010	[65, 66, 67, 68, 69, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106]
11111001	[107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172]

00111010	202	True
00001111	3	False
10110010	188	False
11111001	124	True

Number of codes used=4

Iteration=     10713 training nets give:
alice_loss.item()=0.27636194229125977	bob_loss.item()=0.3636966645717621

00001111	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10010101	[43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135]
11111001	[136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176]
00111010	[177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233]

00001111	35	True
10010101	93	True
11111001	143	True
00111010	213	True

Number of codes used=4

Iteration=     14284 training nets give:
alice_loss.item()=0.06341689825057983	bob_loss.item()=0.11922547221183777

00001111	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00011111	[30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58]
10011101	[59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]
10010101	[73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]
10010001	[101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]
01111001	[120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134]
11111001	[135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171]
11111011	[172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182]
00111010	[183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240]

00001111	19	True
00011111	26	False
10011101	86	False
10010101	83	True
10010001	89	False
01111001	130	True
11111001	131	False
11111011	135	False
00111010	215	True

Number of codes used=9

Iteration=     17855 training nets give:
alice_loss.item()=0.09864107519388199	bob_loss.item()=0.12881970405578613

00001110	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 252, 253, 254, 255]
00001111	[24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38]
00001101	[39, 40]
00001100	[41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]
10000101	[54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78]
10010101	[79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96]
01110001	[97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121]
11111011	[122, 123]
11111001	[124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169]
01111101	[170, 171, 172, 173, 174, 175]
00111010	[176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228]
00111011	[229]
01111010	[230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247]
01001110	[248, 249, 250, 251]

00001110	10	True
00001111	13	False
00001101	40	True
00001100	35	False
10000101	68	True
10010101	73	False
01110001	135	False
11111011	148	False
11111001	146	True
01111101	135	False
00111010	206	True
00111011	212	False
01111010	220	False
01001110	1	False

Number of codes used=14

Iteration=     21426 training nets give:
alice_loss.item()=0.012585027143359184	bob_loss.item()=0.03576570004224777

01001110	[0, 1, 2, 3, 4, 5, 249, 250, 251, 252, 253, 254, 255]
00001110	[6, 7, 8, 9]
00101111	[10, 11, 12, 13]
00001011	[14]
00001111	[15, 16, 17, 18, 19, 20, 21, 22]
00011111	[23, 24, 25, 26, 27, 28, 29, 30, 31]
00000110	[32, 33, 34, 35, 36]
00001101	[37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
10010100	[52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67]
10000101	[68, 69, 70, 71, 72, 73, 74, 75]
00010101	[76, 77, 78, 79, 80, 81, 82, 83, 84, 85]
10010001	[86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103]
01101001	[104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123]
01111001	[124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136]
11111001	[137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165]
11110001	[166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176]
00111010	[177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217]
01111010	[218, 219, 220, 221, 222, 223, 224, 225, 226]
00111110	[227, 228, 229, 230, 231, 232]
00101010	[233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248]

01001110	3	True
00001110	8	True
00101111	13	True
00001011	11	False
00001111	18	True
00011111	23	True
00000110	20	False
00001101	44	True
10010100	62	True
10000101	67	False
00010101	81	True
10010001	91	True
01101001	120	True
01111001	133	True
11111001	147	True
11110001	149	False
00111010	210	True
01111010	221	True
00111110	225	False
00101010	237	True

Number of codes used=20

Iteration=     24997 training nets give:
alice_loss.item()=0.0012862477451562881	bob_loss.item()=0.014099766500294209

01001110	[0, 1, 2, 3, 4, 5, 6, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00001110	[7, 8, 9, 10, 11, 12, 13]
00001011	[14, 15]
00001111	[16, 17, 18, 19, 20, 21]
00011111	[22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]
00001101	[35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]
10010100	[51, 52, 53, 54, 55, 56, 58, 59, 60, 61]
11010100	[57, 62, 63, 64, 65, 66, 67]
10000101	[68, 69, 70, 71, 72, 73]
00010101	[74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
10010001	[84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107]
01101001	[108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130]
01111001	[131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141]
11110001	[142, 143, 144, 145, 146, 147, 148, 149, 150, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180]
11111001	[151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162]
00111010	[181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211]
00111110	[212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228]
01101010	[229, 230, 231, 232, 233]
00101010	[234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246]

01001110	4	True
00001110	9	True
00001011	8	False
00001111	21	True
00011111	23	True
00001101	40	True
10010100	64	False
11010100	66	True
10000101	71	True
00010101	84	False
10010001	95	True
01101001	119	True
01111001	133	True
11110001	151	False
11111001	147	False
00111010	209	True
00111110	227	True
01101010	233	True
00101010	235	True

Number of codes used=19


End of hp run 14.  Result of run:
[(-0.9666771656688649, 24997), ('21-05-15_22:28:32BST_NLearn_model_14_Alice_iter25000', '21-05-15_22:28:32BST_NLearn_model_14_Bob_iter25000')]
(-0.9666771656688649, 24997)


>>>> hp_run=15 of 16, time elapsed 7:42:15 of estimated 8:48:17, 
implying ending at 07:16:49BST on Sunday 16 May 2021
hyperparameters = {
	'N_ITERATIONS': 25000,
	'RANDOM_SEEDS': (625084, 419126, 762692, 952720),
	'ALICE_NET': 'MaxNet("In", 3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 228571,
	'START_TRAINING': 7142,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 714,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 7142,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 10714,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=      7142 training nets give:
alice_loss.item()=0.5800727009773254	bob_loss.item()=0.5829677581787109

00111001	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00101010	[15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138]
00011101	[139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]
01001111	[230, 231, 232, 233, 234]

00111001	33	False
00101010	30	True
00011101	27	False
01001111	176	False

Number of codes used=4

Iteration=     10713 training nets give:
alice_loss.item()=0.31639593839645386	bob_loss.item()=0.32004690170288086

00111001	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 248, 249, 250, 251, 252, 253, 254, 255]
00101010	[26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]
10001101	[65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142]
01010101	[143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247]

00111001	1	True
00101010	60	True
10001101	97	True
01010101	202	True

Number of codes used=4

Iteration=     14284 training nets give:
alice_loss.item()=0.17177903652191162	bob_loss.item()=0.2083350419998169

00111000	[0, 1, 2, 3, 25, 26, 27, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00111001	[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]
00101010	[28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
10101111	[64, 65, 66, 67, 68, 69, 70, 71, 72, 73]
10001101	[74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116]
10001100	[117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153]
01110101	[154, 155, 156, 157, 158, 159, 160, 161, 162]
01010111	[163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185]
01010101	[186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214]
01010001	[215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243]

00111000	19	False
00111001	1	False
00101010	69	False
10101111	67	True
10001101	80	True
10001100	116	False
01110101	210	False
01010111	205	False
01010101	211	True
01010001	221	True

Number of codes used=10

Iteration=     17855 training nets give:
alice_loss.item()=0.14241868257522583	bob_loss.item()=0.12975303828716278

00111001	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 251, 252, 253, 254, 255]
00101001	[14, 15, 248, 249, 250]
00111000	[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]
00101011	[34, 35, 36]
00101000	[37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
10001111	[60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]
10001101	[101]
10001100	[102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134]
10000100	[135, 136, 137, 138]
00001100	[139, 140]
10011100	[141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151]
11001101	[152, 153, 154, 155]
01011100	[156]
01000100	[157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194]
01000101	[195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213]
01010001	[214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232]
00110001	[233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247]

00111001	5	True
00101001	20	False
00111000	17	True
00101011	34	True
00101000	45	True
10001111	85	True
10001101	91	False
10001100	110	True
10000100	106	False
00001100	117	False
10011100	109	False
11001101	111	False
01011100	186	False
01000100	180	True
01000101	205	True
01010001	220	True
00110001	252	False

Number of codes used=17

Iteration=     21426 training nets give:
alice_loss.item()=0.04722057282924652	bob_loss.item()=0.11478955298662186

00111001	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 253, 254, 255]
00111000	[13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
00101011	[24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]
00101000	[37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54]
00101111	[55, 56, 57, 58, 59, 60]
10101101	[61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73]
10100111	[74, 75, 76]
10001010	[77, 78, 79]
10001101	[80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96]
10011100	[97, 98, 99, 100]
10001100	[101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135]
11000100	[136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173]
01000100	[174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191]
01000101	[192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206]
01010101	[207, 208, 209, 210, 211, 212, 213, 214, 215]
01010001	[216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227]
00010001	[228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243]
00111011	[244, 245, 246, 247, 248, 249, 250]
00111101	[251, 252]

00111001	6	True
00111000	18	True
00101011	38	False
00101000	43	True
00101111	51	False
10101101	77	False
10100111	70	False
10001010	79	True
10001101	100	False
10011100	120	False
10001100	109	True
11000100	164	True
01000100	177	True
01000101	205	True
01010101	210	True
01010001	220	True
00010001	226	False
00111011	1	False
00111101	1	False

Number of codes used=19

Iteration=     24997 training nets give:
alice_loss.item()=0.0026019569486379623	bob_loss.item()=0.007120484486222267

00110001	[0, 1, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
00111001	[2, 3, 4, 5, 6, 7, 8, 9, 10]
00111000	[11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]
00101011	[27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38]
00101000	[39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
00101111	[52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62]
10101111	[63, 64, 65, 66, 67, 68, 69, 70, 71]
10001010	[72, 73, 74, 75, 76, 77, 78]
00001101	[79, 80, 81, 82, 83, 84, 85, 86, 87]
10001111	[88, 89]
10001101	[90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
10001100	[100, 101, 102, 103, 104, 105, 106, 107]
00001100	[108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142]
11000100	[143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177]
01000100	[178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191]
01000101	[192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203]
11010101	[204, 205, 206, 207, 208]
01010101	[209, 210, 211, 212, 213, 214, 215]
01011001	[216]
01010001	[217, 218, 219, 220, 221, 222]
00010001	[223, 224, 225]
01111001	[226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240]

00110001	247	True
00111001	4	True
00111000	19	True
00101011	34	True
00101000	39	True
00101111	52	True
10101111	69	True
10001010	85	False
00001101	79	True
10001111	89	True
10001101	90	True
10001100	116	False
00001100	125	True
11000100	168	True
01000100	177	False
01000101	203	True
11010101	205	True
01010101	210	True
01011001	218	False
01010001	218	True
00010001	225	True
01111001	233	True

Number of codes used=22


End of hp run 15.  Result of run:
[(-0.9761624702117081, 24997), ('21-05-15_22:28:32BST_NLearn_model_15_Alice_iter25000', '21-05-15_22:28:32BST_NLearn_model_15_Bob_iter25000')]
(-0.9761624702117081, 24997)


>>>> hp_run=16 of 16, time elapsed 8:20:27 of estimated 8:53:48, 
implying ending at 07:22:21BST on Sunday 16 May 2021
hyperparameters = {
	'N_ITERATIONS': 25000,
	'RANDOM_SEEDS': (625084, 419126, 762692, 952720),
	'ALICE_NET': 'FFs(3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 228571,
	'START_TRAINING': 7142,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 714,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 7142,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 10714,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
}

tuple_specs.random_reward_sd()=0.7071067811865476

Iteration=      7142 training nets give:
alice_loss.item()=0.5642938613891602	bob_loss.item()=0.5758819580078125

01101001	[0, 1, 2, 3, 4, 5, 250, 251, 252, 253, 254, 255]
10110010	[6, 7, 8, 9, 10, 11, 12, 13, 14]
01111101	[15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]
10001001	[54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168]
11010111	[169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219]
10010000	[220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242]
11100000	[243, 244]
01001100	[245, 246, 247, 248, 249]

01101001	144	False
10110010	111	False
01111101	208	False
10001001	179	False
11010111	198	True
10010000	162	False
11100000	138	False
01001100	171	False

Number of codes used=8

Iteration=     10713 training nets give:
alice_loss.item()=0.3070623576641083	bob_loss.item()=0.3853071331977844

11100100	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 247, 248, 249, 250, 251, 252, 253, 254, 255]
10101101	[46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108]
10100101	[109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147]
10001001	[148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169]
11010111	[170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246]

11100100	28	True
10101101	58	True
10100101	89	False
10001001	131	False
11010111	195	True

Number of codes used=5

Iteration=     14284 training nets give:
alice_loss.item()=0.20731936395168304	bob_loss.item()=0.2109905481338501

11110100	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11100100	[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55]
01100100	[56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]
10101101	[70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87]
10100101	[88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120]
10100001	[121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137]
11010011	[138, 139, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178]
10001001	[140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150]
11010111	[179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215]
11010110	[216, 217, 218, 219, 220, 221]
11010101	[222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243]

11110100	26	False
11100100	37	True
01100100	42	False
10101101	66	False
10100101	114	True
10100001	107	False
11010011	179	False
10001001	156	False
11010111	179	True
11010110	189	False
11010101	188	False

Number of codes used=11

Iteration=     17855 training nets give:
alice_loss.item()=0.217685729265213	bob_loss.item()=0.254789263010025

01100110	[0, 1, 2, 3, 4, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
11100110	[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]
11100101	[19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]
11100100	[38, 39, 40, 41, 42, 43, 44]
01100100	[45, 46, 47, 48, 49, 50]
10101001	[51, 52, 53, 54, 66, 67, 68, 69]
10101101	[55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65]
10111101	[70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91]
10100101	[92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112]
10100001	[113, 114]
10000101	[115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131]
10001001	[132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148]
10011001	[149, 150, 151]
10001011	[152, 153, 154, 155, 156, 157, 158, 159, 160]
10100011	[161, 162]
10010111	[163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174]
01010111	[175, 176, 177]
10011011	[178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230]
11110110	[231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245]

01100110	13	False
11100110	10	True
11100101	31	True
11100100	39	True
01100100	42	False
10101001	67	True
10101101	71	False
10111101	74	True
10100101	119	False
10100001	118	False
10000101	126	True
10001001	146	True
10011001	155	False
10001011	164	False
10100011	127	False
10010111	167	True
01010111	185	False
10011011	168	False
11110110	242	True

Number of codes used=19

Iteration=     21426 training nets give:
alice_loss.item()=0.03817470371723175	bob_loss.item()=0.10706405341625214

11000101	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 255]
11100110	[15, 16, 17, 18, 19, 20]
11100000	[21, 22, 23, 24, 25, 26, 27, 28, 29]
11100101	[30, 31, 32, 33, 34, 35, 36]
11100100	[37, 38, 39, 40, 41, 42, 43, 44, 45]
01100100	[46, 47, 48, 49]
11101100	[50, 51, 52, 53, 54, 55, 56]
00101101	[57, 58, 59, 60, 61]
10101001	[62, 63, 64, 65, 66, 67, 68, 69, 70]
10111101	[71, 72, 73, 74, 75, 76]
10100100	[77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
00100001	[96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107]
10100101	[108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120]
10100111	[121]
10000001	[122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132]
00001001	[133, 134, 135, 136, 137, 138, 139, 140, 141, 142]
10001001	[143, 144, 145, 146, 147, 148, 149, 150]
10011001	[151, 152, 153, 154, 155, 156, 157, 158, 159]
10001011	[160, 161, 162]
10010111	[163, 164, 165, 166, 167, 168, 169, 170, 171]
11001011	[172, 173, 174, 175, 176]
11010111	[177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190]
11010010	[191, 192, 193, 194]
11010110	[195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]
11110111	[210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]
11110110	[230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254]

11000101	2	True
11100110	3	False
11100000	24	True
11100101	29	False
11100100	35	False
01100100	33	False
11101100	44	False
00101101	69	False
10101001	68	True
10111101	81	False
10100100	88	True
00100001	104	True
10100101	111	True
10100111	113	False
10000001	128	True
00001001	132	False
10001001	141	False
10011001	146	False
10001011	164	False
10010111	167	True
11001011	170	False
11010111	190	True
11010010	198	False
11010110	207	True
11110111	226	True
11110110	238	True

Number of codes used=26

Iteration=     24997 training nets give:
alice_loss.item()=0.050061821937561035	bob_loss.item()=0.04993892461061478

11000101	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 251, 252, 253, 254, 255]
11100110	[13, 14]
11100010	[15]
11100000	[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]
11100101	[28, 29, 30, 31, 32, 33, 34, 35, 36]
11100100	[37, 38, 39, 40, 41, 42, 43]
01100100	[44]
11101100	[45, 46, 47, 48, 49, 50, 51, 52, 53]
11101001	[54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]
10101001	[65, 66, 67, 68, 69, 70, 71]
10111101	[72, 73, 74, 75, 76, 77, 78, 79, 80]
10100100	[81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94]
00100001	[95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108]
10100101	[109, 110, 111, 112, 113, 114, 115, 116, 117]
10100111	[118, 119, 120, 121]
10000001	[122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132]
00001001	[133, 134, 135, 136, 137, 138, 139]
10001001	[140, 141, 142, 143, 144, 145, 146, 147, 148]
10101011	[149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]
10001011	[160, 161, 162, 163, 164, 165, 166, 167]
10010111	[168, 169, 170, 171, 172, 173, 174]
10010011	[175, 176, 177, 178, 179, 180, 181]
11001011	[182]
11010111	[183, 184, 185]
11010010	[186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198]
11010110	[199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217]
11110111	[218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232]
11110110	[233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250]

11000101	2	True
11100110	4	False
11100010	5	False
11100000	23	True
11100101	32	True
11100100	37	True
01100100	37	False
11101100	46	True
11101001	61	True
10101001	72	False
10111101	79	True
10100100	88	True
00100001	102	True
10100101	106	False
10100111	117	False
10000001	129	True
00001001	131	False
10001001	142	True
10101011	149	True
10001011	162	True
10010111	168	True
10010011	175	True
11001011	169	False
11010111	190	False
11010010	197	True
11010110	208	True
11110111	225	True
11110110	237	True

Number of codes used=28


End of hp run 16.  Result of run:
[(-0.9873814744460769, 24997), ('21-05-15_22:28:32BST_NLearn_model_16_Alice_iter25000', '21-05-15_22:28:32BST_NLearn_model_16_Bob_iter25000')]
(-0.9873814744460769, 24997)



Time taken over all 16 given sets of hyperparameters=8:47:30, averaging 0:32:58 per run


 ---- Table of results ----

 code  hp_run  result
   00       1  (-0.975, 24997)
   01       2  (-0.984, 24997)
   10       3  (-0.969, 24997)
   11       4  (-0.985, 24997)
   20       5  (-0.990, 24997)
   21       6  (-0.956, 24997)
   30       7  (-0.982, 24997)
   31       8  (-0.980, 24997)
   40       9  (-0.972, 24997)
   41      10  (-0.963, 24997)
   50      11  (-0.982, 24997)
   51      12  (-0.969, 24997)
   60      13  (-0.972, 24997)
   61      14  (-0.967, 24997)
   70      15  (-0.976, 24997)
   71      16  (-0.987, 24997)
 --------------------------

++++ Best result was (-0.990, 24997) on hp_run=5 with
hyperparameters = {
	'N_ITERATIONS': 25000,
	'RANDOM_SEEDS': (321406, 416695, 885201, 467036),
	'ALICE_NET': 'MaxNet("In", 3, 50)',
	'BOB_NET': 'FFs(3, 50)',
	'BATCHSIZE': 32,
	'GAMESIZE': 32,
	'BUFFER_CAPACITY': 228571,
	'START_TRAINING': 7142,
	'N_SELECT': 256,
	'EPSILON_ONE_END': 714,
	'EPSILON_MIN': 0.0,
	'EPSILON_MIN_POINT': 7142,
	'ALICE_PLAY': 'QPerCode',
	'ALICE_TRAIN': 'QPerCode',
	'BOB_PLAY': 'CircularVocab',
	'BOB_TRAIN': 'CircularVocab',
	'ALICE_OPTIMIZER': 'SGD(lr=0.01)',
	'BOB_OPTIMIZER': 'SGD(lr=0.01)',
	'ALICE_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'BOB_LOSS_FUNCTION': 'Huber(beta=0.1)',
	'ALICE_PROXIMITY_BONUS': 100000000,
	'ALICE_PROXIMITY_SLOPE_LENGTH': 10000,
	'ALICE_LAST_TRAINING': 10000000,
	'NOISE_START': 10714,
	'NOISE': 0.1,
	'ALICE_DOUBLE': None,
	'N_CODE': 8,
	'N_NUMBERS': 256
	'n_rng': Generator(PCG64)
	'ne_rng': Generator(PCG64)
	't_rng': <torch._C.Generator object at 0x7f88654e1a50>
	'te_rng': <torch._C.Generator object at 0x7f88654e1a70>
}


End closed log for run 21-05-15_22:28:32BST
